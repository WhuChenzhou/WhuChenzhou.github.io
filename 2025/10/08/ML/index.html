<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="西瓜书中涉及的机器学习算法, Chenzhou">
    <meta name="description" content="这是笔者在准备nju推免机试时准备的自用资料，但是估计从今年（2025）过后不会再有这类型的机器学习算法题目了，可以作为学习西瓜书的一些参考（应该有不少谬误及typo）

常见损失函数1. Mean Squared Error (MSE)均">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>西瓜书中涉及的机器学习算法 | Chenzhou</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Chenzhou</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Chenzhou</div>
        <div class="logo-desc">
            
            Though we drift apart for a while, we are destined to meet again.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">西瓜书中涉及的机器学习算法</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Python/">
                                <span class="chip bg-color">Python</span>
                            </a>
                        
                            <a href="/tags/%E7%AE%97%E6%B3%95/">
                                <span class="chip bg-color">算法</span>
                            </a>
                        
                            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">机器学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Python/" class="post-category">
                                Python
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>这是笔者在准备nju推免机试时准备的自用资料，但是估计从今年（2025）过后不会再有这类型的机器学习算法题目了，可以作为学习西瓜书的一些参考（应该有不少谬误及typo）</p>
<hr>
<h2 id="常见损失函数"><a href="#常见损失函数" class="headerlink" title="常见损失函数"></a>常见损失函数</h2><h3 id="1-Mean-Squared-Error-MSE"><a href="#1-Mean-Squared-Error-MSE" class="headerlink" title="1. Mean Squared Error (MSE)"></a>1. Mean Squared Error (MSE)</h3><p>均方误差 (MSE) 用于回归任务，衡量预测值与真实值之间的平均平方差。</p>
<h4 id="数学公式："><a href="#数学公式：" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (y_{\text{true}}^i - y_{\text{pred}}^i)^2</script><ul>
<li>$y_{\text{true}}^i $是第 ( i ) 个样本的真实值。</li>
<li>$ y_{\text{pred}}^i$ 是第 ( i ) 个样本的预测值。</li>
<li>( N ) 是样本总数。</li>
</ul>
<h3 id="2-Mean-Absolute-Error-MAE"><a href="#2-Mean-Absolute-Error-MAE" class="headerlink" title="2. Mean Absolute Error (MAE)"></a>2. Mean Absolute Error (MAE)</h3><p>平均绝对误差 (MAE) 用于回归任务，衡量预测值与真实值之间的平均绝对差。</p>
<h4 id="数学公式：-1"><a href="#数学公式：-1" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{MAE}} = \frac{1}{N} \sum_{i=1}^{N} |y_{\text{true}}^i - y_{\text{pred}}^i|</script><ul>
<li>$ y_{\text{true}}^i $ 是第 ( i ) 个样本的真实值。</li>
<li>$ y_{\text{pred}}^i $ 是第 ( i ) 个样本的预测值。</li>
</ul>
<hr>
<h3 id="3-Binary-Cross-Entropy"><a href="#3-Binary-Cross-Entropy" class="headerlink" title="3. Binary Cross-Entropy"></a>3. Binary Cross-Entropy</h3><p>二分类交叉熵用于二分类任务，衡量模型预测的概率与真实标签之间的差异。</p>
<h4 id="数学公式：-2"><a href="#数学公式：-2" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{binary}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_{\text{true}}^i \log(y_{\text{pred}}^i) + (1 - y_{\text{true}}^i) \log(1 - y_{\text{pred}}^i) \right]</script><ul>
<li>( $y_{\text{true}}^i \in \{0, 1\}$ ) 是第 ( i ) 个样本的真实标签。</li>
<li>( $y_{\text{pred}}^i $) 是第 ( i ) 个样本的预测概率。</li>
</ul>
<hr>
<h3 id="4-Categorical-Cross-Entropy"><a href="#4-Categorical-Cross-Entropy" class="headerlink" title="4. Categorical Cross-Entropy"></a>4. Categorical Cross-Entropy</h3><p>多分类交叉熵用于多分类任务，衡量模型预测的概率分布与真实标签的分布之间的差异。</p>
<h4 id="数学公式：-3"><a href="#数学公式：-3" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{categorical}} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{\text{true},c}^i \log(y_{\text{pred},c}^i)</script><ul>
<li>( $y_{\text{true},c}^i \in \{0, 1\}$ ) 是第 ( i ) 个样本的真实标签的 one-hot 编码。</li>
<li>( $y_{\text{pred},c}^i $) 是第 ( i ) 个样本属于类别 ( c ) 的预测概率。</li>
<li>( C ) 是类别的总数。</li>
</ul>
<hr>
<h3 id="5-Hinge-Loss-SVM-Loss"><a href="#5-Hinge-Loss-SVM-Loss" class="headerlink" title="5. Hinge Loss (SVM Loss)"></a>5. Hinge Loss (SVM Loss)</h3><p>铰链损失（SVM损失）通常用于支持向量机（SVM），它最大化分类边界。</p>
<h4 id="数学公式：-4"><a href="#数学公式：-4" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{hinge}} = \frac{1}{N} \sum_{i=1}^{N} \max(0, 1 - y_{\text{true}}^i y_{\text{pred}}^i)</script><ul>
<li>( $y_{\text{true}}^i$ ) 是第 ( i ) 个样本的真实标签（取值为 ±1)）。</li>
<li>( $y_{\text{pred}}^i $) 是第 ( i ) 个样本的预测值。</li>
</ul>
<hr>
<h3 id="6-Huber-Loss"><a href="#6-Huber-Loss" class="headerlink" title="6. Huber Loss"></a>6. Huber Loss</h3><p>Huber 损失对回归任务中的异常值进行了处理，当误差较小时使用平方误差，误差较大时使用线性误差。</p>
<h4 id="数学公式：-5"><a href="#数学公式：-5" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{huber}} = \frac{1}{N} \sum_{i=1}^{N} \left[ \begin{array}{ll}
0.5 \cdot \left| y_{\text{true}}^i - y_{\text{pred}}^i \right|^2 & \text{if } \left| y_{\text{true}}^i - y_{\text{pred}}^i \right| \leq \delta \\
\delta \cdot \left( \left| y_{\text{true}}^i - y_{\text{pred}}^i \right| - 0.5 \cdot \delta \right) & \text{if } \left| y_{\text{true}}^i - y_{\text{pred}}^i \right| > \delta
\end{array} \right.</script><ul>
<li>( \delta ) 是一个超参数，控制“平滑”与“惩罚”之间的平衡。</li>
<li>( y_{\text{true}}^i ) 是第 ( i ) 个样本的真实值。</li>
<li>( y_{\text{pred}}^i ) 是第 ( i ) 个样本的预测值。</li>
</ul>
<hr>
<h3 id="7-L2-Regularization-Loss"><a href="#7-L2-Regularization-Loss" class="headerlink" title="7. L2 Regularization Loss"></a>7. L2 Regularization Loss</h3><p>L2 正则化损失常用于防止过拟合，鼓励模型参数的平方和较小。</p>
<h4 id="数学公式：-6"><a href="#数学公式：-6" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{L2}} = \lambda_{\text{reg}} \sum_{j=1}^{M} w_j^2</script><ul>
<li>( w_j ) 是模型参数的第 ( j ) 个权重。</li>
<li>( \lambda_{\text{reg}} ) 是正则化系数。</li>
<li>( M ) 是参数的数量。</li>
<li><ul>
<li>惩罚的是参数的<strong>平方</strong>，所以对大的参数惩罚非常重（比如 w=10，惩罚是100；w=2，惩罚是4）。</li>
<li>它会让所有参数都<strong>向0收缩</strong>，但通常不会让任何参数<strong>精确为0</strong>。</li>
<li>结果是：<strong>所有特征都保留，但权重变小了</strong>，模型更平滑。</li>
<li>处理特征相关性强</li>
</ul>
</li>
</ul>
<hr>
<h3 id="8-L1-Regularization-Loss"><a href="#8-L1-Regularization-Loss" class="headerlink" title="8. L1 Regularization Loss"></a>8. L1 Regularization Loss</h3><p>L1 正则化损失常用于促进稀疏性，鼓励模型参数为零。</p>
<h4 id="数学公式：-7"><a href="#数学公式：-7" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
L_{\text{L1}} = \lambda_{\text{reg}} \sum_{j=1}^{M} |w_j|</script><ul>
<li>( w_j ) 是模型参数的第 ( j ) 个权重。</li>
<li>( \lambda_{\text{reg}} ) 是正则化系数。</li>
<li>( M ) 是参数的数量。</li>
<li><ul>
<li>惩罚的是参数的<strong>绝对值</strong>，对大参数和小参数的惩罚是线性的。</li>
<li>由于其几何特性（L1的等高线是菱形），优化过程中更容易“撞到角点”，而角点对应某些参数为0。</li>
<li>结果是：<strong>会把不重要的特征的权重直接压缩为0</strong>，实现<strong>特征选择</strong></li>
<li>促进参数权重的稀疏性</li>
</ul>
</li>
</ul>
<hr>
<h3 id="9-Kullback-Leibler-Divergence-KL-Divergence"><a href="#9-Kullback-Leibler-Divergence-KL-Divergence" class="headerlink" title="9. Kullback-Leibler Divergence (KL Divergence)"></a>9. Kullback-Leibler Divergence (KL Divergence)</h3><p>KL 散度衡量两个概率分布之间的差异，常用于概率模型和生成模型。</p>
<h4 id="数学公式：-8"><a href="#数学公式：-8" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
D_{\text{KL}}(p || q) = \sum_{i=1}^{N} p_i \log \left( \frac{p_i}{q_i} \right)</script><ul>
<li>( p_i ) 是真实分布的第 ( i ) 个概率值。</li>
<li>( q_i ) 是预测分布的第 ( i ) 个概率值。</li>
</ul>
<hr>
<h3 id="10-Softmax-Function"><a href="#10-Softmax-Function" class="headerlink" title="10. Softmax Function"></a>10. Softmax Function</h3><p>Softmax 函数将一个向量转换为概率分布，广泛应用于多分类问题。</p>
<h4 id="数学公式：-9"><a href="#数学公式：-9" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}}</script><ul>
<li>( x_i ) 是输入向量 ( x ) 中的第 ( i ) 个元素。</li>
<li>( C ) 是类别的总数。</li>
</ul>
<hr>
<h3 id="11-Softmax-Loss"><a href="#11-Softmax-Loss" class="headerlink" title="11. Softmax Loss"></a>11. Softmax Loss</h3><p>Softmax 损失（与交叉熵损失类似）常用于多分类问题中，结合 Softmax 函数计算损失。</p>
<h3 id="数学公式：-10"><a href="#数学公式：-10" class="headerlink" title="数学公式："></a>数学公式：</h3><script type="math/tex; mode=display">
L_{\text{softmax}} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{\text{true},c}^i \log(y_{\text{pred},c}^i)</script><ul>
<li>( $y_{\text{true},c}^i \in \{0, 1\} $) 是第 ( i ) 个样本的真实标签的 one-hot 编码。</li>
<li>( $y_{\text{pred},c}^i $) 是第 ( i ) 个样本属于类别 ( c ) 的预测概率。</li>
</ul>
<hr>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
import numpy as np

# 1. Mean Squared Error (MSE)
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# 2. Mean Absolute Error (MAE)
def mean_absolute_error(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# 3. Binary Cross-Entropy
def binary_crossentropy(y_true, y_pred):
    epsilon &#x3D; 1e-15  # Avoid log(0)
    y_pred &#x3D; np.clip(y_pred, epsilon, 1 - epsilon)  # Prevent y_pred from being 0 or 1
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))


&#x2F;&#x2F;把 y_pred 的所有值限制在 [epsilon, 1 - epsilon] 范围内

# 4. Categorical Cross-Entropy
def categorical_crossentropy(y_true, y_pred):
    #numpy.clip(a, a_min, a_max, out&#x3D;None)
    epsilon &#x3D; 1e-15  # 避免对数计算时出现log(0)
    y_pred &#x3D; np.clip(y_pred, epsilon, 1 - epsilon)  # 防止y_pred为0或1
    #y_true 必须是 one-hot 编码（独热编码） 的形式
    return -np.mean(np.sum(y_true * np.log(y_pred), axis&#x3D;1))
    #axis&#x3D;1出来时竖着，&#x3D;0出来时横着

# 5. Hinge Loss (SVM Loss)
def hinge_loss(y_true, y_pred):
    return np.mean(np.maximum(0, 1 - y_true * y_pred))

# 6. Huber Loss
def huber_loss(y_true, y_pred, delta&#x3D;1.0):
    error &#x3D; np.abs(y_true - y_pred)
    is_small_error &#x3D; error &lt;&#x3D; delta
    small_error_loss &#x3D; 0.5 * error**2
    large_error_loss &#x3D; delta * (error - 0.5 * delta)
    return np.mean(np.where(is_small_error, small_error_loss, large_error_loss))

# 7. L2 Regularization Loss
def l2_regularization_loss(weights, lambda_reg&#x3D;0.01):
    return lambda_reg * np.sum(weights**2)

# 8. L1 Regularization Loss
def l1_regularization_loss(weights, lambda_reg&#x3D;0.01):
    return lambda_reg * np.sum(np.abs(weights))

# 9. Kullback-Leibler Divergence (KL Divergence)
def kl_divergence(p, q):
    epsilon &#x3D; 1e-15  # Prevent log(0)
    p &#x3D; np.clip(p, epsilon, 1 - epsilon)
    q &#x3D; np.clip(q, epsilon, 1 - epsilon)
    return np.sum(p * np.log(p &#x2F; q))

# 10. Softmax Function
def softmax(x):
    exp_x &#x3D; np.exp(x - np.max(x, axis&#x3D;-1, keepdims&#x3D;True))  # Prevent overflow
    return exp_x &#x2F; np.sum(exp_x, axis&#x3D;-1, keepdims&#x3D;True)

# 11. Softmax Loss
def softmax_loss(y_true, y_pred):
    epsilon &#x3D; 1e-15
    y_pred &#x3D; np.clip(y_pred, epsilon, 1 - epsilon)  # Prevent log(0)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis&#x3D;1))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><p>如果数据中存在缺失值，可以选择填充它们，通常用均值或中位数来填充数值型数据。如果是类别数据，也可以选择填充众数。</p>
<p><code>mean_value = np.nanmean(column)#忽略nan值</code></p>
<p><code>column[np.isnan(column)] = mean_value # 填充nan缺失值为mean_value</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def fill_missing_values(data):
    &#39;&#39;&#39;
    使用每列的均值填充缺失值
    :param data: 数据集，类型为ndarray
    :return: 填充后的数据集
    &#39;&#39;&#39;
    # 遍历每一列，填充缺失值
    for i in range(data.shape[1]):
        column &#x3D; data[:, i]
        # 计算列的均值（忽略nan）
        mean_value &#x3D; np.nanmean(column)
        # 填充缺失值
        column[np.isnan(column)] &#x3D; mean_value
    return data
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p>对特征进行标准化，使其均值为0，方差为1，避免特征尺度对模型产生不良影响。标准化可以使用 <code>(x - mean) / std</code> 的方式来实现。</p>
<p><code>means=np.mean(data,axis=0) #取每一个特征的均值</code></p>
<p><code>std=np.std(data,axis=0) #取每一个特征的标准差</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def standardize_data(data):
    &#39;&#39;&#39;
    标准化数据：每个特征减去均值，再除以标准差
    :param data: 数据集，类型为ndarray
    :return: 标准化后的数据集
    &#39;&#39;&#39;
    # 计算每列的均值和标准差
    means &#x3D; np.mean(data, axis&#x3D;0)
    stds &#x3D; np.std(data, axis&#x3D;0)

    # 标准化每一列
    standardized_data &#x3D; (data - means) &#x2F; stds
    return standardized_data
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p><img src="/images/ml/72967e50-080e-477c-91cc-73731e5a22db.png" alt="72967e50-080e-477c-91cc-73731e5a22db"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">min_vals &#x3D; np.min(data, axis&#x3D;0)
max_vals &#x3D; np.max(data, axis&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def normalize_data(data):
    &#39;&#39;&#39;
    归一化数据：将每个特征缩放到[0, 1]范围内
    :param data: 数据集，类型为ndarray
    :return: 归一化后的数据集
    &#39;&#39;&#39;
    # 计算每列的最小值和最大值
    min_vals &#x3D; np.min(data, axis&#x3D;0)
    max_vals &#x3D; np.max(data, axis&#x3D;0)

    # 归一化每一列
    normalized_data &#x3D; (data - min_vals) &#x2F; (max_vals - min_vals)
    return normalized_data
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Z-score"><a href="#Z-score" class="headerlink" title="Z-score"></a>Z-score</h3><p>可以使用 Z-Score 方法来去除异常值。通常认为 Z-Score 大于 3 或小于 -3 的样本是异常值。</p>
<p><code>filtered_data = data[np.all(np.abs(z_scores) &lt; threshold, axis=1)]</code></p>
<ul>
<li><code>axis=1</code>：沿着列方向判断（即：对每个样本的所有特征判断）</li>
<li><code>np.all</code>：要求<strong>所有特征都满足条件</strong>（即：所有特征的 |Z| &lt; threshold）</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def remove_outliers(data, threshold&#x3D;3):
    &#39;&#39;&#39;
    使用 Z-Score 去除异常值
    :param data: 数据集，类型为ndarray
    :param threshold: Z-Score 阈值，默认是3
    :return: 去除异常值后的数据集
    &#39;&#39;&#39;
    # 计算每列的均值和标准差
    means &#x3D; np.mean(data, axis&#x3D;0)
    stds &#x3D; np.std(data, axis&#x3D;0)

    # 计算 Z-Score
    z_scores &#x3D; (data - means) &#x2F; stds

    # 过滤 Z-Score 小于阈值的数据
    filtered_data &#x3D; data[np.all(np.abs(z_scores) &lt; threshold, axis&#x3D;1)]

    return filtered_data
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np
def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -&gt; np.ndarray:
    # Your code here, make sure to round
    m,n&#x3D;X.shape
    theta&#x3D;np.zeros((n,1))
    y &#x3D; y.reshape(-1, 1)  # (m,) -&gt; (m, 1)
    for i in range(iterations):
        # 计算预测误差
        error &#x3D; X @ theta - y  # (m, 1)
        # 计算梯度: (1&#x2F;m) * X^T @ error
        gradient &#x3D; (1&#x2F;m) * X.T @ error
        # 更新参数
        theta -&#x3D; alpha * gradient
    return np.round(theta,4)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Python 尝试广播 <code>(3,1)</code> 和 <code>(3,)</code>，但<strong>广播规则不匹配</strong>，<code>y</code> reshape 成 <code>(m, 1)</code></p>
<h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p><img title="" src="/images/ml/de3a4ee6-0c36-421d-b365-d688dd1c4542.png" alt="de3a4ee6-0c36-421d-b365-d688dd1c4542" style="zoom:100%;"><img title="" src="/images/ml/94d0039d-c97e-4a61-ad89-375994782858.png" alt="94d0039d-c97e-4a61-ad89-375994782858" style="zoom:100%;"></p>
<p><img title="" src="/images/ml/6caef8a4-87bd-4b74-ae1c-97b14174b7f9.png" alt="6caef8a4-87bd-4b74-ae1c-97b14174b7f9" style="zoom:100%;"><img title="" src="/images/ml/93d50e4f-e0a3-4658-af4d-49e041d061d2.png" alt="93d50e4f-e0a3-4658-af4d-49e041d061d2" style="zoom:100%;"></p>
<p>MSE(mean squard error) MAE(mean absolute error) RMSE(root mse) R^2(R-squard)</p>
<p><code>np.mean((y_predict - y_test) ** 2) #mean自带了求和功能</code></p>
<p><code>X = np.hstack((np.ones((train_data.shape[0], 1)), train_data))#hstack添加一个偏置项b，每个样本的bias特征值为1</code></p>
<p><code>np.linalg.inv(X.T.dot(X)).dot(X.T).dot(train_label)</code></p>
<p><code>numpy.var(array, axis) #var为方差variance，std为标准差</code></p>
<p><code>numpy.vstack(([1,2,3],[4,5,6])) 行顺序堆叠</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

# MSE
def mse_score(y_predict, y_test):
    return np.mean((y_predict - y_test) ** 2)

# R2
def r2_score(y_predict, y_test):
    &#39;&#39;&#39;
    input: y_predict (ndarray): 预测值
           y_test (ndarray): 真实值
    output: r2 (float): r2值
    &#39;&#39;&#39;
    t1 &#x3D; np.sum((y_predict - y_test) ** 2)
    t2 &#x3D; np.sum((y_test - np.mean(y_test)) ** 2)
    return 1 - t1 &#x2F; t2

class LinearRegression:
    def __init__(self):
        &#39;&#39;&#39;初始化线性回归模型&#39;&#39;&#39;
        self.theta &#x3D; None

    def fit_normal(self, train_data, train_label):
        &#39;&#39;&#39;
        input: train_data (ndarray): 训练样本
               train_label (ndarray): 训练标签
        &#39;&#39;&#39;
        X &#x3D; np.hstack((np.ones((train_data.shape[0], 1)), train_data))
        #创建一个列向量，全为1，长度等于样本数
        self.theta &#x3D; np.linalg.inv(X.T.dot(X)).dot(X.T).dot(train_label)
        return self.theta

    def predict(self, test_data):
        &#39;&#39;&#39;
        input: test_data (ndarray): 测试样本
        &#39;&#39;&#39;
        test_data &#x3D; np.hstack((np.ones((test_data.shape[0], 1)), test_data))
        return test_data.dot(self.theta)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="逻辑回归Logistic-Regression"><a href="#逻辑回归Logistic-Regression" class="headerlink" title="逻辑回归Logistic Regression"></a>逻辑回归Logistic Regression</h3><p>逻辑回归是通过回归的思想来解决二分类问题的算法，用一个<strong>S型的函数（Sigmoid函数）</strong>，把线性回归的结果压缩到 [0,1] 区间，解释为“属于某个类别的概率”。它内部还是在做<strong>线性回归</strong> wTx+b，只是最后加了一个非线性变换（Sigmoid），用来做<strong>分类概率输出</strong>。可以使用np.exp(-z)</p>
<p><img title="" src="/images/ml/c08e2d31-b7a3-464d-9912-a0008883dcf1.png" alt="c08e2d31-b7a3-464d-9912-a0008883dcf1" style="zoom:100%;"></p>
<p>交叉熵损失yi代表真实类别，log(yi_hat)代表预测类别</p>
<p><img title="" src="/images/ml/22893227-1066-4b56-b28f-bf929f91158e.png" alt="22893227-1066-4b56-b28f-bf929f91158e" style="zoom:100%;"><img title="" src="/images/ml/ec35d332-8f41-40cc-b8b6-cc53fecdc323.png" alt="ec35d332-8f41-40cc-b8b6-cc53fecdc323" style="zoom:100%;"></p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>沿着“当前点的负梯度方向”走一小步，就能慢慢靠近最小值，在多维空间中，<strong>梯度向量</strong>是函数增长最快的方向。</p>
<p>大学习率收敛快，迅速接近最优解，但是容易震荡无法收敛，小学习率稳定收敛到精确解（可能局部），但是慢</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>问题</strong></th>
<th><strong>过拟合</strong></th>
<th><strong>欠拟合</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>表现</strong></td>
<td>训练误差低，测试误差高</td>
<td>训练误差和测试误差都高，模型过于简单或不充分学习数据特征</td>
</tr>
<tr>
<td><strong>学习率过大</strong></td>
<td>可能导致模型无法稳定收敛，训练过程震荡，导致最终过拟合训练数据</td>
<td>过大的学习率可能跳过局部最优，导致无法捕捉到数据中的复杂模式，出现欠拟合</td>
</tr>
<tr>
<td><strong>学习率过小</strong></td>
<td>可能导致训练过程过于缓慢，使得模型误学习训练数据中的噪声，过拟合</td>
<td>过小的学习率可能导致参数更新不足，学习不到数据的复杂模式，造成欠拟合</td>
</tr>
<tr>
<td><strong>解决方法</strong></td>
<td>增加正则化，减少模型复杂度，更多数据，适当调整学习率</td>
<td>增加模型复杂度，训练更多轮次，使用合适的学习率</td>
</tr>
</tbody>
</table>
</div>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -*- coding: utf-8 -*-

import numpy as np
import warnings
warnings.filterwarnings(&quot;ignore&quot;)

def sigmoid(x):
    &#39;&#39;&#39;
    sigmoid函数
    :param x: 转换前的输入
    :return: 转换后的概率
    &#39;&#39;&#39;
    return 1&#x2F;(1+np.exp(-x))


def fit(x,y,eta&#x3D;1e-3,n_iters&#x3D;10000):
    &#39;&#39;&#39;
    训练逻辑回归模型
    :param x: 训练集特征数据，类型为ndarray
    :param y: 训练集标签，类型为ndarray
    :param eta: 学习率，类型为float
    :param n_iters: 训练轮数，类型为int
    :return: 模型参数，类型为ndarray
    &#39;&#39;&#39;
    #   请在此添加实现代码   #
    #********** Begin *********#
    m,n&#x3D;x.shape
    theta&#x3D;np.zeros(n)
    for i in range(n_iters):
        yy&#x3D;x.dot(theta)
        z&#x3D;sigmoid(yy)
        grad&#x3D;x.T.dot(z-y)
        theta-&#x3D;eta*grad
    return theta
    #********** End **********#

from sklearn.linear_model import LogisticRegression

def digit_predict(train_image, train_label, test_image):
    &#39;&#39;&#39;
    实现功能：训练模型并输出预测结果
    :param train_sample: 包含多条训练样本的样本集，类型为ndarray,shape为[-1, 8, 8]
    :param train_label: 包含多条训练样本标签的标签集，类型为ndarray
    :param test_sample: 包含多条测试样本的测试集，类型为ndarry
    :return: test_sample对应的预测标签
    &#39;&#39;&#39;

    #************* Begin ************#
    train&#x3D;train_image.reshape(len(train_image),-1)
#-1 表示自动计算维度，将除样本数量以外的维度全部拉平成一维。
    test&#x3D;test_image.reshape(len(test_image),-1)
    model&#x3D;LogisticRegression(max_iter&#x3D;1000,C&#x3D;12)
# C：正则化系数的倒数，默认为 1.0 ，越小代表正则化越强；
    model.fit(train,train_label)
    result&#x3D;model.predict(test)
    return result
    #************* End **************#

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>它是一个<strong>二分类线性模型</strong>，核心思想是：找到一个超平面，将两类数据线性分开。</p>
<ul>
<li><p><strong>适用于线性可分数据集</strong>：即存在一个超平面能将两类样本完全分开。</p>
</li>
<li><p><strong>不能处理非线性问题</strong>：如异或问题（XOR）感知机无法解决。</p>
</li>
</ul>
<p><img title="" src="/images/ml/f572fd80-7786-4793-85d6-0f4738df2710.png" alt="f572fd80-7786-4793-85d6-0f4738df2710" style="zoom:100%;"><img title="" src="/images/ml/42e50b31-90f0-4b65-a426-5b8b2a1fa845.png" alt="42e50b31-90f0-4b65-a426-5b8b2a1fa845" style="zoom:100%;"></p>
<p><code>predict = np.where(output &gt; 0, 1, -1)</code></p>
<p><code>predict=np.sign(output)</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8
import numpy as np
#构建感知机算法
class Perceptron(object):
    def __init__(self, learning_rate &#x3D; 0.01, max_iter &#x3D; 200):
        self.lr&#x3D;learning_rate
        self.max_iter&#x3D;max_iter
    def fit(self, data, label):
        &#39;&#39;&#39;
        input:data(ndarray):训练数据特征
              label(ndarray):训练数据标签
        output:w(ndarray):训练好的权重
               b(ndarry):训练好的偏置
        &#39;&#39;&#39;
        #编写感知机训练方法，w为权重，b为偏置

        #********* Begin *********#
        self.w&#x3D;np.random.randn(data.shape[1])
        self.b&#x3D;0
        for i in range(self.max_iter):
            for j in range(len(data)):
                output&#x3D;self.w.dot(data[j].T)+self.b
                predict&#x3D;1 if output&gt;0 else -1
                if(label[j]*output&lt;&#x3D;0):
                    self.w+&#x3D;self.lr*label[j]*data[j]
                    self.b+&#x3D;self.lr*label[j]


        #********* End *********#
    def predict(self, data):
        &#39;&#39;&#39;
        input:data(ndarray):测试数据特征
        output:predict(ndarray):预测标签
        &#39;&#39;&#39;
        #********* Begin *********#
        output &#x3D; np.dot(data, self.w) + self.b
        predict &#x3D; np.where(output &gt; 0, 1, -1)
        predict&#x3D;np.sign(output)
        #********* End *********#
        return predict

#encoding&#x3D;utf8
import os
import pandas as pd
from sklearn.linear_model.perceptron import Perceptron

if os.path.exists(&#39;.&#x2F;step2&#x2F;result.csv&#39;):
    os.remove(&#39;.&#x2F;step2&#x2F;result.csv&#39;)

#********* Begin *********#
train_data&#x3D;pd.read_csv(&#39;.&#x2F;step2&#x2F;train_data.csv&#39;)
train_label&#x3D;pd.read_csv(&#39;.&#x2F;step2&#x2F;train_label.csv&#39;)
train_label&#x3D;train_label[&#39;target&#39;]

test_data&#x3D;pd.read_csv(&#39;.&#x2F;step2&#x2F;test_data.csv&#39;)

model&#x3D;Perceptron(eta0&#x3D;0.1,max_iter&#x3D;500)
#eta0：学习率大小，默认为 1.0 ；
#max_iter：最大训练轮数。
model.fit(train_data,train_label)
prediction&#x3D;model.predict(test_data)
res&#x3D;pd.DataFrame(prediction,columns&#x3D;[&#39;result&#39;])
res.to_csv(&#39;.&#x2F;step2&#x2F;result.csv&#39;,index&#x3D;False)
#********* End *********#


<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="LDA（线性判别分析）"><a href="#LDA（线性判别分析）" class="headerlink" title="LDA（线性判别分析）"></a>LDA（线性判别分析）</h3><p>LDA 是一种 <strong>有监督降维</strong> 技术，其目标是将数据投影到一个低维空间中，使得不同类别之间尽可能可分。目标是最大化类间散度（类之间的距离），最小化类内散度（每类自己的分散程度）。</p>
<p><img title="" src="/images/ml/4d717c12-bca5-47eb-ab20-f99094171823.png" alt="4d717c12-bca5-47eb-ab20-f99094171823" style="zoom:100%;"><img title="" src="/images/ml/b53022c6-9481-4c55-95cb-702d108573d5.png" alt="b53022c6-9481-4c55-95cb-702d108573d5" style="zoom:100%;"></p>
<p><img title="" src="/images/ml/4d5a5350-0b9e-4559-b764-ce155badb894.png" alt="4d5a5350-0b9e-4559-b764-ce155badb894" style="zoom:100%;"><img title="" src="/images/ml/3e366804-e71c-43ad-8782-1b6cc0153e69.png" alt="3e366804-e71c-43ad-8782-1b6cc0153e69" style="zoom:100%;"><img title="" src="/images/ml/470f2a07-a3aa-4edf-8c7c-8dc4c1a54a8d.png" alt="470f2a07-a3aa-4edf-8c7c-8dc4c1a54a8d" style="zoom:100%;"><img title="" src="/images/ml/bfa7df51-524a-441f-96ef-9e0129454a26.png" alt="bfa7df51-524a-441f-96ef-9e0129454a26" style="zoom:100%;"></p>
<p><code>x0m = np.mean(x0, axis=0)  # 类别 0 的均值向量,求样本的每个特征均值要加axis=0</code></p>
<p><code>sigma0 = np.cov(x0, rowvar=False)#默认 np.cov 会按行计算特征（即样本是列），但我们通常每一行是一个样本，所以需要加</code>.T<code>或使用rowvar=False</code></p>
<p><code>mean_diff = (x0m - x1m).reshape(-1, 1)  # 列向量化, shape=(n,) 变成 shape=(n, 1)，列向量</code></p>
<p><code>return np.linalg.inv(sw).dot(x0m - x1m)</code></p>
<p><code>x_new = model.fit_transform(x, y)</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def lda(X, y):
    &#39;&#39;&#39;
    线性判别分析（Linear Discriminant Analysis）

    参数:
        X(ndarray): 样本特征矩阵，形状为 (n_samples, n_features)
        y(ndarray): 样本标签向量，取值为 0 或 1（仅支持二分类）

    返回:
        w(ndarray): 最优投影方向向量，用于将高维数据投影到一维，实现类别分离
    &#39;&#39;&#39;

    # 将数据按类别分开（这里是二分类，分成类 0 和类 1）
    x0 &#x3D; X[y &#x3D;&#x3D; 0]  # 类别 0 的所有样本
    x1 &#x3D; X[y &#x3D;&#x3D; 1]  # 类别 1 的所有样本

    # 分别计算每一类的均值向量（每个特征的均值）
    x0m &#x3D; np.mean(x0, axis&#x3D;0)  # 类别 0 的均值向量
    x1m &#x3D; np.mean(x1, axis&#x3D;0)  # 类别 1 的均值向量

    # 分别计算每一类的类内协方差矩阵（每一类内部的样本方差+协方差）
    # rowvar&#x3D;False 表示按列计算协方差（列是特征，行是样本）

    sigma0 &#x3D; np.cov(x0, rowvar&#x3D;False)
    sigma1 &#x3D; np.cov(x1, rowvar&#x3D;False)

    # 总类内散度矩阵 Sw（两个类别的协方差矩阵之和）
    sw &#x3D; sigma0 + sigma1

    # 类间散度矩阵 Sb &#x3D; (μ1 - μ0)(μ1 - μ0)^T，衡量类别中心的分离程度
    mean_diff &#x3D; (x0m - x1m).reshape(-1, 1)  # 列向量化
    sb &#x3D; mean_diff @ mean_diff.T  # 外积构成类间散度矩阵

    # 计算广义特征值问题 Sw^-1 * Sb 的特征值与特征向量
    eigvals, eigvecs &#x3D; np.linalg.eig(np.linalg.inv(sw).dot(sb))

    # 将特征值按从大到小排序，找出最大特征值对应的方向
    idx &#x3D; np.argsort(eigvals)[::-1]  # 逆序排列索引
    w &#x3D; eigvecs[:, idx[0]]  # 最大特征值对应的特征向量（最佳投影方向）

    return w  # 返回用于降维的最优方向向量

#encoding&#x3D;utf8 
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

def lda(x,y):
    &#39;&#39;&#39;
    input:x(ndarray):待处理数据
          y(ndarray):待处理数据标签
    output:x_new(ndarray):降维后数据
    &#39;&#39;&#39;
    model &#x3D; LinearDiscriminantAnalysis(n_components&#x3D;2) 
    x_new &#x3D; model.fit_transform(x, y)
    return x_new<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="多分类策略"><a href="#多分类策略" class="headerlink" title="多分类策略"></a>多分类策略</h3><h4 id="OvO"><a href="#OvO" class="headerlink" title="OvO"></a>OvO</h4><p>将 K 类的多分类问题拆分为 $\frac{K(K-1)}{2}$ 个二分类问题，每个分类器只区分两个类别。</p>
<p>对于任意两个类别 $c_i$ 与 $c_j$（$i &lt; j$）：</p>
<ol>
<li><p>从数据集中仅选出这两个类别的样本。</p>
</li>
<li><p>用二分类模型训练出分类器 $f_{i,j}$，判断样本属于 $c_i$ 还是 $c_j$。</p>
</li>
<li><p>最终得到 $\frac{K(K-1)}{2}$ 个分类器。</p>
</li>
<li><ul>
<li><p>对测试样本 $x$，输入所有分类器 $f_{i,j}$。</p>
</li>
<li><p>每个分类器对样本进行投票（即判断更像哪个类别）。</p>
</li>
<li><p>汇总投票，得票最多的类别为最终预测类别。</p>
</li>
</ul>
</li>
</ol>
<p><code>self.classes_ = np.unique(y)</code></p>
<p><code>self.models[(cls1, cls2)] = model</code> map</p>
<p><code>votes = [defaultdict(int) for _ in range(len(X))]</code></p>
<p><code>for (cls1, cls2), model in self.models.items()</code></p>
<p><code>`enumerate(preds)</code> 的作用是：<strong>在遍历预测结果 <code>preds</code> 的同时，提供每个预测结果的索引</strong>。它返回的是 <code>(索引, 值)</code> 对。`</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from collections import defaultdict
from sklearn.linear_model import LogisticRegression

class OvOClassifier:
    def __init__(self, base_model&#x3D;None):
        self.base_model &#x3D; base_model if base_model else LogisticRegression()
        self.models &#x3D; &#123;&#125;
        self.classes_ &#x3D; None

    def fit(self, X, y):
        self.classes_ &#x3D; np.unique(y)
        self.models &#x3D; &#123;&#125;

        for i in range(len(self.classes_)):
            for j in range(i + 1, len(self.classes_)):
                cls1, cls2 &#x3D; self.classes_[i], self.classes_[j]
# 找到所有属于 cls1 或 cls2 的样本
X_pair &#x3D; []
y_binary &#x3D; []
for xi, yi in zip(X, y):
    if yi &#x3D;&#x3D; cls1 or yi &#x3D;&#x3D; cls2:
        X_pair.append(xi)
        y_binary.append(1 if yi &#x3D;&#x3D; cls1 else 0)

X_pair &#x3D; np.array(X_pair)
y_binary &#x3D; np.array(


                model &#x3D; LogisticRegression()
                model.fit(X_pair, y_binary)
                self.models[(cls1, cls2)] &#x3D; model
#是一个 字典，key 是一个类别对的 元组，value 是对应的 训练好的模型对象

    def predict(self, X):
        votes &#x3D; [defaultdict(int) for _ in range(len(X))]
#创建了一个列表 votes，这个列表有 len(X) 个元素，每个元素是一个 defaultdict(int)

        for (cls1, cls2), model in self.models.items():
            preds &#x3D; model.predict(X)
            for i, pred in enumerate(preds):
                voted_class &#x3D; cls1 if pred &#x3D;&#x3D; 1 else cls2
                votes[i][voted_class] +&#x3D; 1

        final_preds &#x3D; []
        final_preds &#x3D; [max(vote, key&#x3D;vote.get) for vote in votes]

#vote 是一个 dict（通常是像 &#123;&#39;A&#39;: 2, &#39;B&#39;: 3&#125; 这样的结构），键是类别，值是票数；

#vote.get 用作 key 函数，表示对每个键使用 vote.get(键) 作为排序依据；

#max(...) 找出拥有最多票数的类别。

        return np.array(final_preds)

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="OvR"><a href="#OvR" class="headerlink" title="OvR"></a>OvR</h4><p>将一个 K 类的多分类问题拆解为 K 个二分类问题。每个分类器的任务是识别某一个类别（“正类”）与其余所有类别（“负类”）之间的区别。</p>
<p>假设标签集合为 $C = {c_1, c_2, …, c_K}$</p>
<p>对于每一个类别 $c_i$：</p>
<ol>
<li><p>创建一个新的二分类训练集：</p>
<ul>
<li><p>样本属于 $c_i$ 的，标签为 1（正类）</p>
</li>
<li><p>样本不属于 $c_i$ 的，标签为 0（负类）</p>
</li>
</ul>
</li>
<li><p>用一个二分类器（如 Logistic Regression、SVM）训练该数据子集。</p>
</li>
<li><p>共训练 K 个分类器，分别记为 $f_1, f_2, …, f_K$。</p>
</li>
<li><ul>
<li><p>对于一个测试样本 $x$，将其输入所有分类器 $f_1(x), …, f_K(x)$。</p>
</li>
<li><p>每个分类器输出一个置信度/得分。</p>
</li>
<li><p>选择得分最高的那个分类器所代表的类别作为预测结果。</p>
</li>
</ul>
</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>特性</th>
<th>OvR (One-vs-Rest)</th>
<th>OvO (One-vs-One)</th>
</tr>
</thead>
<tbody>
<tr>
<td>分类器数量</td>
<td>K</td>
<td>K(K-1)/2</td>
</tr>
<tr>
<td>每个分类器训练样本</td>
<td>全部样本（正类 vs 其他）</td>
<td>两类样本</td>
</tr>
<tr>
<td>预测机制</td>
<td>最大置信度</td>
<td>多数投票</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>较低</td>
<td>较高</td>
</tr>
<tr>
<td>优点</td>
<td>简洁、高效</td>
<td>更适合类别数少、边界复杂的任务</td>
</tr>
<tr>
<td>缺点</td>
<td>易受类别不均衡影响</td>
<td>分类器数量多、计算成本高</td>
</tr>
</tbody>
</table>
</div>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
import numpy as np

# 逻辑回归类
class tiny_logistic_regression(object):
    def __init__(self):
        # W 是系数，b 是截距
        self.coef_ &#x3D; None
        self.intercept_ &#x3D; None
        self._theta &#x3D; None  # 存储所有的 W 和 b
        self.label_map &#x3D; &#123;&#125;  # 0&#x2F;1 到标签的映射

    def _sigmoid(self, x):
        return 1. &#x2F; (1. + np.exp(-x))  # Sigmoid 激活函数

    # 训练模型
    def fit(self, train_datas, train_labels, learning_rate&#x3D;1e-4, n_iters&#x3D;1e3):
        # 计算损失函数
        def J(theta, X_b, y):
            y_hat &#x3D; self._sigmoid(X_b.dot(theta))
            try:
                return -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) &#x2F; len(y)
            except:
                return float(&#39;inf&#39;)  # 避免计算出错

        # 损失函数对 theta 的梯度
        def dJ(theta, X_b, y):
            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) &#x2F; len(y)

        # 批量梯度下降
        def gradient_descent(X_b, y, initial_theta, learning_rate, n_iters&#x3D;1e2, epsilon&#x3D;1e-6):
            theta &#x3D; initial_theta
            cur_iter &#x3D; 0
            while cur_iter &lt; n_iters:
                gradient &#x3D; dJ(theta, X_b, y)
                last_theta &#x3D; theta
                theta &#x3D; theta - learning_rate * gradient  # 更新 theta
                # 判断是否满足收敛条件
                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
                    break
                cur_iter +&#x3D; 1
            return theta

        # 将训练数据加上截距项（x0 &#x3D; 1）
        X_b &#x3D; np.hstack([np.ones((len(train_datas), 1)), train_datas])
        initial_theta &#x3D; np.zeros(X_b.shape[1])  # 初始化theta为零
        self._theta &#x3D; gradient_descent(X_b, train_labels, initial_theta, learning_rate, n_iters)
        self.intercept_ &#x3D; self._theta[0]
        self.coef_ &#x3D; self._theta[1:]
        return self

    # 预测样本属于正类的概率
    def predict_proba(self, X):
        X_b &#x3D; np.hstack([np.ones((len(X), 1)), X])
        return self._sigmoid(X_b.dot(self._theta))

    # 预测：如果属于正类的概率 &gt;&#x3D; 0.5，则预测为1，否则为0
    def predict(self, X):
        proba &#x3D; self.predict_proba(X)
        return np.array(proba &gt;&#x3D; 0.5, dtype&#x3D;&#39;int&#39;)

# One-vs-Rest (OvR) 分类器
class OvR(object):
    def __init__(self):
        self.models &#x3D; []  # 用于保存每个类别的二分类器
        self.real_label &#x3D; []  # 用于保存正类标签

    def fit(self, train_datas, train_labels):
        &#39;&#39;&#39;
        OvR的训练阶段，将模型保存到self.models中
        :param train_datas: 训练集数据，类型为ndarray
        :param train_labels: 训练集标签，类型为ndarray，shape为(-1,)
        :return: None
        &#39;&#39;&#39;
        self.classes &#x3D; np.unique(train_labels)  # 获取所有类别标签

        for cls in self.classes:
            # 将当前类别作为正类，其他类别作为负类
            y_binary &#x3D; (train_labels &#x3D;&#x3D; cls).astype(int)
            model &#x3D; tiny_logistic_regression()
            model.fit(train_datas, y_binary)  # 使用二分类器训练模型
            self.models.append(model)  # 将训练好的模型加入模型列表
            self.real_label.append(cls)  # 保存当前类别的标签

    def predict(self, test_datas):
        &#39;&#39;&#39;
        OvR的预测阶段
        :param test_datas: 测试集数据，类型为ndarray
        :return: 预测结果，类型为ndarray
        &#39;&#39;&#39;
        # 存储每个样本对每个类别的概率
        all_probs &#x3D; np.zeros((len(test_datas), len(self.classes)))

        for i, model in enumerate(self.models):
            # 获取每个模型对每个测试样本属于该类别的概率
            probs &#x3D; model.predict_proba(test_datas)
            all_probs[:, i] &#x3D; probs  # 存储该类别的概率

        # 每个样本选择概率最大的类别
        predicted_class_indices &#x3D; np.argmax(all_probs, axis&#x3D;1)
        return self.real_label[predicted_class_indices]  # 返回每个样本的最终预测类别
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>np.log2,Counter,len,np.unique</p>
<p>for label,count in labelcount.items():</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np
from collections import Counter

def calcInfoGain(feature, label, index):
    &#39;&#39;&#39;
    计算信息增益
    :param feature: 测试用例中的特征矩阵，类型为ndarray
    :param label: 测试用例中的标签，类型为ndarray
    :param index: 特征列的索引，指示使用第几个特征来计算信息增益。
    :return: 信息增益，类型float
    &#39;&#39;&#39;

    # 计算原始熵
    all_count &#x3D; len(label)

    # 获取标签的频率
    label_count &#x3D; Counter(label)

    # 计算标签的熵
    all_entropy &#x3D; 0
    for label_value, count in label_count.items():
        p &#x3D; count &#x2F; all_count
        all_entropy -&#x3D; p * np.log2(p)

    # 计算按特征划分后的熵
    feature_values &#x3D; np.unique(feature[:, index])  # 获取该特征的所有可能值
# feature[:, index] 表示选择 feature 矩阵的所有行（: 表示所有行），但是只选择第 index 列（即该列所有样本的特征值）。
    weighted_entropy &#x3D; 0

    for value in feature_values:
        # 根据特征值划分数据
        subset_label &#x3D; label[feature[:, index] &#x3D;&#x3D; value]
        subset_count &#x3D; len(subset_label)

        # 计算子集的熵
        subset_entropy &#x3D; 0
        subset_label_count &#x3D; Counter(subset_label)
        for subset_label_value, count in subset_label_count.items():
            p &#x3D; count &#x2F; subset_count
            subset_entropy -&#x3D; p * np.log2(p)

        # 加权平均子集的熵
        weighted_entropy +&#x3D; (subset_count &#x2F; all_count) * subset_entropy

    # 信息增益 &#x3D; 总熵 - 加权熵
    info_gain &#x3D; all_entropy - weighted_entropy
    return info_gain return info_gain<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">def calcInfoGainRatio(feature, label, index):

    #********* Begin *********#

    all_count&#x3D;len(label)

    #D

    needE&#x3D;0

    all_features &#x3D; np.unique(feature[:, index])  # 获取该特征列的所有唯一值

    for f in all_features:

        count&#x3D;0

        for j in range(len(label)):

            if feature[j][index]&#x3D;&#x3D;f:

                count+&#x3D;1

        #D1

        needE-&#x3D;(count&#x2F;all_count)*np.log2(count&#x2F;all_count)

    gain&#x3D;calcInfoGain(feature,label,index)

    return gain&#x2F;needE

    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Gini系数"><a href="#Gini系数" class="headerlink" title="Gini系数"></a>Gini系数</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">def calcGini(feature, label, index):
    &#39;&#39;&#39;
    &#39;&#39;&#39;
    #********* Begin *********#
    #先计算特征个数，在用特征里面的小个数计算
    allcount&#x3D;len(label)
    feats&#x3D;np.unique(feature[:,index])
    Gini&#x3D;0
    for val in feats:
        sub_count&#x3D;0
        sublabels&#x3D;[]
        subG&#x3D;1
        for j in range(allcount):
            if feature[j][index]&#x3D;&#x3D;val:
                sub_count+&#x3D;1
                sublabels.append(label[j])

        label_count&#x3D;Counter(sublabels)
        for la,co in label_count.items():
            subG-&#x3D;(co&#x2F;sub_count)**2
        Gini+&#x3D;subG*(sub_count&#x2F;allcount)
    return Gini

    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>鸢尾花识别</p>
<p>pd.read_csv,to_csv,DataFrame,</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
#********* Begin *********#

model&#x3D;DecisionTreeClassifier()
train_label &#x3D; pd.read_csv(&#39;.&#x2F;step7&#x2F;train_label.csv&#39;).values
train_data &#x3D; pd.read_csv(&#39;.&#x2F;step7&#x2F;train_data.csv&#39;).values
test_data &#x3D; pd.read_csv(&#39;.&#x2F;step7&#x2F;test_data.csv&#39;).values


model.fit(train_data,train_label)
result&#x3D;model.predict(test_data)
resultd&#x3D;pd.DataFrame(result,columns&#x3D;[&#39;prediction&#39;])
resultd.to_csv(&#39;.&#x2F;step7&#x2F;predict.csv&#39;,index&#x3D;False)
#********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h2><p>全概率公式</p>
<p><img src="/images/ml/9bbceb6a-0ae0-449f-9af5-cb33b8ef2ffd.png" title="" alt="9bbceb6a-0ae0-449f-9af5-cb33b8ef2ffd" style="zoom:100%;"></p>
<p>贝叶斯公式（条件概率乘法定理/全概率公式）</p>
<p><img src="/images/ml/cb2d0212-cba3-430c-8d92-d2eb0ca1ffe9.png" title="" alt="cb2d0212-cba3-430c-8d92-d2eb0ca1ffe9" style="zoom:100%;"><img src="/images/ml/b772390c-44da-4753-ba81-a7004d95f0a1.png" title="" alt="b772390c-44da-4753-ba81-a7004d95f0a1" style="zoom:100%;"></p>
<h3 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h3><p><code>label_val, label_count = np.unique(label, return_counts=True)</code></p>
<p><code>self.label_prob = &#123;label_val[i]: label_count[i] / all_count for i in range(len(label_val))&#125;</code></p>
<p><code>feat_lines = feature[label == val]</code></p>
<p><code>for a, b in zip(feat_val, feat_count):</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

class NaiveBayesClassifier(object):
    def __init__(self):
        self.label_prob &#x3D; &#123;&#125;  # 每种类别在数据中出现的概率
        self.condition_prob &#x3D; &#123;&#125;  # 条件概率 P(x|y)

    def fit(self, feature, label):
        &#39;&#39;&#39;
        对模型进行训练，计算各种概率
        :param feature: 训练数据集的特征，ndarray
        :param label: 训练数据集的标签，ndarray
        :return: 无返回
        &#39;&#39;&#39;
        # 计算每个标签的概率 P(y)
        label_val, label_count &#x3D; np.unique(label, return_counts&#x3D;True)
        all_count &#x3D; len(label)
        self.label_prob &#x3D; &#123;label_val[i]: label_count[i] &#x2F; all_count for i in range(len(label_val))&#125;

        # 计算每个标签下特征的条件概率 P(x|y)
        feat_num &#x3D; feature.shape[1]  # 特征数量
        self.condition_prob &#x3D; &#123;&#125;

        for val in label_val:
            feat_lines &#x3D; feature[label &#x3D;&#x3D; val]  # 当前标签下的特征数据
            feature_prob &#x3D; &#123;&#125;

            for i in range(feat_num):
                feat_val, feat_count &#x3D; np.unique(feat_lines[:, i], return_counts&#x3D;True)  # 当前特征列的取值及其频数
                feature_prob[i] &#x3D; &#123;&#125;
                sub_count &#x3D; len(feat_lines)
                for a, b in zip(feat_val, feat_count):
                    feature_prob[i][a] &#x3D; b &#x2F; sub_count  # 计算条件概率

            self.condition_prob[val] &#x3D; feature_prob  # 保存当前标签的条件概率

    def predict(self, feature):
        &#39;&#39;&#39;
        对数据进行预测，返回预测结果
        :param feature: 测试数据集所有特征组成的ndarray
        :return: 预测的标签
        &#39;&#39;&#39;
        predictions &#x3D; []

        for sample in feature:
            label_post &#x3D; &#123;&#125;

            # 计算每个标签的后验概率 P(y|x)
            for label_val, prob in self.label_prob.items():
                probability &#x3D; np.log2(prob)  # P(y)

                # 计算当前标签下的条件概率 P(x|y)
                for idx, val in enumerate(sample):
                    probability +&#x3D; np.log2(self.condition_prob[label_val][idx].get(val, 1e-10))

                label_post[label_val] &#x3D; probability

            # 选择后验概率最大的标签作为预测结果
            predict &#x3D; max(label_post, key&#x3D;label_post.get)
            predictions.append(predict)

        return np.array(predictions)  # 返回 numpy 数组
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Laplace平滑"><a href="#Laplace平滑" class="headerlink" title="Laplace平滑"></a>Laplace平滑</h3><p>假设N表示训练数据集总共有多少种类别，Ni表示训练数据集中第i列总共有多少种取值。则训练过程中在算类别的概率时分子加1，分母加N，算条件概率时分子加1，分母加Ni</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

class NaiveBayesClassifier(object):
    def __init__(self):
        &#39;&#39;&#39;
        &#39;&#39;&#39;
        self.label_prob &#x3D; &#123;&#125;  # 存储每个类别的先验概率
        self.condition_prob &#x3D; &#123;&#125;  # 存储每个类别下每个特征的条件概率

    def fit(self, feature, label):
        &#39;&#39;&#39;
        对模型进行训练，计算每种类别的先验概率和条件概率
        :param feature: 训练数据集所有特征组成的 ndarray
        :param label: 训练数据集所有标签组成的 ndarray
        :return: 无返回值，通过更新 self.label_prob 和 self.condition_prob 来保存训练结果
        &#39;&#39;&#39;
        #********* Begin *********#
        row_num &#x3D; len(feature)  # 样本的数量
        col_num &#x3D; len(feature[0])  # 特征的数量
        unique_label_count &#x3D; len(set(label))  # 唯一类别的数量

        # 计算每个类别的出现次数
        for c in label:
            if c in self.label_prob:
                self.label_prob[c] +&#x3D; 1
            else:
                self.label_prob[c] &#x3D; 1

        # 计算每个类别的先验概率，并进行拉普拉斯平滑
        for key in self.label_prob.keys():
            self.label_prob[key] +&#x3D; 1  # 拉普拉斯平滑
            self.label_prob[key] &#x2F;&#x3D; (unique_label_count + row_num)  # 计算先验概率

            # 构建条件概率字典，初始化每个类别下每个特征值的概率
            self.condition_prob[key] &#x3D; &#123;&#125;
            for i in range(col_num):
                self.condition_prob[key][i] &#x3D; &#123;&#125;
                for k in np.unique(feature[:, i], axis&#x3D;0):  # 遍历每个特征的不同取值
                    self.condition_prob[key][i][k] &#x3D; 1  # 初始概率设为 1（拉普拉斯平滑）

        # 统计训练集中每个类别下特征值的频次
        for i in range(len(feature)):
            for j in range(len(feature[i])):
                # 更新每个特征值的出现频次
                self.condition_prob[label[i]][j][feature[i][j]] +&#x3D; 1

        # 计算每个类别下每个特征值的条件概率，进行拉普拉斯平滑
        for label_key in self.condition_prob.keys():
            for k in self.condition_prob[label_key].keys():
                total &#x3D; len(self.condition_prob[label_key][k].keys())  # 特征的种类数
                for v in self.condition_prob[label_key][k].values():
                    total +&#x3D; v  # 总的计数值（包括平滑项）
                for kk in self.condition_prob[label_key][k].keys():
                    # 计算条件概率
                    self.condition_prob[label_key][k][kk] &#x2F;&#x3D; total
        #********* End *********#

    def predict(self, feature):
        &#39;&#39;&#39;
        对数据进行预测，返回预测结果
        :param feature: 测试数据集所有特征组成的 ndarray
        :return: 预测结果的数组，包含每个测试样本的预测类别
        &#39;&#39;&#39;
        result &#x3D; []  # 存储预测结果
        # 对每条测试数据进行预测
        for i, f in enumerate(feature):
            prob &#x3D; np.zeros(len(self.label_prob.keys()))  # 存储每个类别的概率，初始化为零
            ii &#x3D; 0  # 用于索引类别
            for label, label_prob in self.label_prob.items():
                prob[ii] &#x3D; label_prob  # 初始化为先验概率
                for j in range(len(f)):  # 遍历当前样本的所有特征
                    # 计算该特征值在该类别下的条件概率
                    prob[ii] *&#x3D; self.condition_prob[label][j][f[j]]
                ii +&#x3D; 1  # 处理下一个类别
            # 选择具有最大概率的类别作为预测结果
            result.append(list(self.label_prob.keys())[np.argmax(prob)])
        return np.array(result)  # 返回预测结果

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="新闻文本分类"><a href="#新闻文本分类" class="headerlink" title="新闻文本分类"></a>新闻文本分类</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
import numpy as np

def news_predict(train_sample, train_label, test_sample):
    &#39;&#39;&#39;
    训练模型并进行预测，返回预测结果
    :param train_sample: 原始训练集中的新闻文本，类型为 ndarray
    :param train_label: 训练集中新闻文本对应的主题标签，类型为 ndarray
    :param test_sample: 原始测试集中的新闻文本，类型为 ndarray
    :return: 预测结果，类型为 ndarray
    &#39;&#39;&#39;
    #********* Begin *********#
    # 实例化CountVectorizer，转换文本为词频矩阵
    vec &#x3D; CountVectorizer()
    X_train &#x3D; vec.fit_transform(train_sample)  # 先fit拟合训练集，再对训练集进行词频向量化
    X_test &#x3D; vec.transform(test_sample)  # 对测试集进行词频向量化

    # 实例化TfidfTransformer，转换为TF-IDF矩阵
    tfidf &#x3D; TfidfTransformer()
    X_train &#x3D; tfidf.fit_transform(X_train)  # 将训练集词频矩阵转换为TF-IDF矩阵
    X_test &#x3D; tfidf.transform(X_test)  # 将测试集词频矩阵转换为TF-IDF矩阵

    # 实例化MultinomialNB模型，设置平滑参数alpha&#x3D;0.8
    model &#x3D; MultinomialNB(alpha&#x3D;0.8)
    model.fit(X_train, train_label)  # 使用训练集训练模型

    # 预测测试集的标签
    result &#x3D; model.predict(X_test)

    # 返回预测结果
    return np.array(result)
    #********* End *********#

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h3><p><code>计算训练数据的均值和标准差
    mean = np.mean(train_data, axis=0)
    std = np.std(train_data,axis=0)</code></p>
<p><code>train_data = (train_data - mean) / std</code></p>
<p><code>model = LinearSVC(C=10, max_iter=10000)  # 增大C的值并增加最大迭代次数</code></p>
<p><strong><code>C</code>（正则化参数）</strong>: 控制模型的复杂度，较大的 <code>C</code> 值会让模型更加“严格”地拟合训练数据，但可能导致过拟合。较小的 <code>C</code> 值则允许更多的错误，从而使得模型更为平滑，防止过拟合</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8
from sklearn.svm import LinearSVC
import pandas as pd
import numpy as np

def linearsvc_predict(train_data,train_label,test_data):
    &#39;&#39;&#39;
    input:train_data(ndarray):训练数据
          train_label(ndarray):训练标签
    output:predict(ndarray):测试集预测标签
    &#39;&#39;&#39;
    # ********* Begin *********#

    # 1. 数据标准化（手动进行标准化：均值0，方差1）
    # 计算训练数据的均值和标准差
    mean &#x3D; np.mean(train_data, axis&#x3D;0)
    std &#x3D; np.std(train_data, axis&#x3D;0)

    # 对训练数据和测试数据进行标准化
    train_data &#x3D; (train_data - mean) &#x2F; std
    test_data &#x3D; (test_data - mean) &#x2F; std

    # 2. 创建并训练模型
    model &#x3D; LinearSVC(C&#x3D;10, max_iter&#x3D;10000)  # 增大C的值并增加最大迭代次数
    model.fit(train_data, train_label)

    # 3. 预测测试数据
    predict &#x3D; model.predict(test_data)

    # ********* End *********# 
    return predict<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a>非线性SVM</h3><p><img title="" src="/images/ml/526644dc-2c68-464b-9590-50f1c27f5fcc.png" alt="526644dc-2c68-464b-9590-50f1c27f5fcc" style="zoom:100%;"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8
from sklearn.svm import SVC

def svc_predict(train_data,train_label,test_data,kernel):
    &#39;&#39;&#39;
    input:train_data(ndarray):训练数据
          train_label(ndarray):训练标签
          kernel(str):使用核函数类型:
              &#39;linear&#39;:线性核函数
              &#39;poly&#39;:多项式核函数
              &#39;rbf&#39;:径像核函数&#x2F;高斯核
    output:predict(ndarray):测试集预测标签
    &#39;&#39;&#39;
    #********* Begin *********# 
    model&#x3D;SVC(kernel&#x3D;kernel)
    model.fit(train_data,train_label)
    predict&#x3D;model.predict(test_data)
    #********* End *********# 
    return predict<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h3><p>提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。</p>
<p>历史上，<strong>Kearns 和 Valiant 首先提出了强可学习和弱可学习的概念</strong>指出：在 PAC 学习的框架中，一个概念，</p>
<p>如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；</p>
<p>一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。</p>
<p>非常有趣的是 Schapire 后来证明强可学习与弱可学习是等价的，也就是说，在 PAC 学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p>
<p>这样一来，问题便成为，在<strong>学习中，如果已经发现了弱学习算法，那么能否将它提升为强学习算法。大家知道，发现弱学习算法通常要比发现强学习算法容易得多</strong>。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。</p>
<p>与 bagging 不同， boosting 采用的是一个串行训练的方法。首先，它训练出一个弱分类器，然后在此基础上，再训练出一个稍好点的弱分类器，以此类推，不断的训练出多个弱分类器，最终再将这些分类器相结合，这就是 boosting 的基本思想</p>
<p>可以看出，<strong>子模型之间存在强依赖关系，必须串行生成。</strong> boosting 是利用不同模型的相加，构成一个更好的模型，求取模型一般都采用序列化方法，后面的模型依据前面的模型</p>
<p>Adaboost算法原理<br>对提升方法来说，有两个问题需要回答：<strong>一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器</strong></p>
<p>关于第 1 个问题，<strong>AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值</strong></p>
<p>这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。</p>
<p>至于第 2 个问题，即弱分类器的组合，<strong>AdaBoost采取加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</strong></p>
<h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>与 Boosting 这种串行集成学习算法不同， Bagging 是并行式集成学习方法。</p>
<p>如果使用 Bagging 解决分类问题，就是将多个分类器的结果整合起来进行投票，选取票数最高的结果作为最终结果。如果使用 Bagging 解决回归问题，就将多个回归器的结果加起来然后求平均，将平均值作为最终结果。</p>
<p>n足够大时，考虑用正态分布来拟合二项分布</p>
<p><img src="/images/ml/02a997d7-32a2-4a6f-a846-782d1ecd53a2.png" title="" alt="02a997d7-32a2-4a6f-a846-782d1ecd53a2" style="zoom:100%;"></p>
<p> Bagging 在训练时的特点就是随机有放回采样和并行。</p>
<p><strong>随机有放回采样:</strong> 假设训练数据集有 m 条样本数据，每次从这 m 条数据中随机取一条数据放入采样集，然后将其返回，让下一次采样有机会仍然能被采样。<strong>然后重复 m 次，就能得到拥有 m 条数据的采样集</strong>，该采样集作为 Bagging 的众多分类器中的一个作为训练数据集。假设有 T 个分类器（随便什么分类器），那么就重复 T 此随机有放回采样，<strong>构建出 T 个采样集分别作为 T 个分类器的训练数据集</strong>。</p>
<p><strong>并行：</strong> 假设有 10 个分类器，在 Boosting 中，1 号分类器训练完成之后才能开始 2 号分类器的训练，而<strong>在 Bagging 中，分类器可以同时进行训练，当所有分类器训练完成之后，整个 Bagging 的训练过程就结束了</strong>。</p>
<p><code>samples = np.random.choice(n, n, replace=True)</code>从0到n-1选取n个有放回采样，</p>
<p><code>prediction=[model.predict(feature) for model in self.models]</code></p>
<p><code>votes=[prediction[j][i] for j in range(self.n_model)]</code></p>
<p><code>max(set(votes),key=votes.count)</code>以votes.count为排序依据，对set(votes)进行排序并取最大值</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np
from sklearn.tree import DecisionTreeClassifier

class BaggingClassifier():
    def __init__(self, n_model&#x3D;10):
        &#39;&#39;&#39;
        初始化函数，设置模型参数
        :param n_model: 分类器数量，默认为10个决策树
        &#39;&#39;&#39;
        # 分类器的数量，默认为10
        self.n_model &#x3D; n_model
        # 用于保存训练后的模型，存储每个决策树
        self.models &#x3D; []

    def fit(self, feature, label):
        # 获取样本数量
        n &#x3D; len(label)

        # 训练 n_model 个决策树
        for i in range(self.n_model):
            # 有放回地随机采样样本索引
            samples &#x3D; np.random.choice(n, n, replace&#x3D;True)

            # 生成样本的特征和标签子集
            sample_f &#x3D; feature[samples]
            sample_l &#x3D; label[samples]

            # 使用决策树模型进行训练，最大深度为3
            model &#x3D; DecisionTreeClassifier(max_depth&#x3D;3)
            model.fit(sample_f, sample_l)

            # 将训练好的模型添加到模型列表中
            self.models.append(model)

    def predict(self, feature):
        &#39;&#39;&#39;
        对测试集数据进行预测，采用投票机制
        :param feature: 测试集数据，类型为ndarray，形状为 (n_samples, n_features)
        :return: 预测结果，类型为ndarray，形状为 (n_samples,)
        &#39;&#39;&#39;
        # 存储每个模型的预测结果

        prediction&#x3D;[]
        for model in self.models:
            prediction.append(model.predict(feature))
        # prediction&#x3D;[model.predict(feature) for model in self.models]
        final_prediction&#x3D;[]
        for i in range(len(feature)):
            votes&#x3D;[prediction[j][i] for j in range(self.n_model)]
            l&#x3D;max(set(votes),key&#x3D;votes.count)
            final_prediction.append(l)

        return np.array(final_prediction)

        #************* End **************#
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林是 Bagging 的一种扩展变体，随机森林的训练过程相对与 Bagging 的训练过程的改变有：</p>
<p><strong>基学习器：</strong> Bagging 的基学习器可以是任意学习器，而<strong>随机森林则是以决策树作为基学习器</strong>。<br><strong>随机属性选择：</strong> 假设原始训练数据集有 10 个特征，从这 10 个特征中<strong>随机选取 k 个特征构成训练数据子集</strong>，然后将这个子集作为训练集扔给决策树去训练。其中 k 的取值一般为 <strong>log2(特征数量)</strong> 。<br>这样的改动通常会使得随机森林具有更加强的泛化性，因为每一棵决策树的训练数据集是随机的，而且训练数据集中的特征也是随机抽取的。如果每一棵决策树模型的差异比较大，那么就很容易<strong>能够解决决策树容易过拟合的问题</strong>。</p>
<p>随机森林的预测流程与 Bagging 的预测流程基本一致，<strong>如果是回归，就将结果基学习器的预测结果全部加起来算平均</strong>；如果是<strong>分类，就投票，票数最多的结果作为最终结果。但需要注意的是，在预测时所用到的特征必须与训练模型时所用到的特征保持一致</strong>。</p>
<p><code>s = np.random.choice(l, self.feature_nums, replace=False)</code>随机选择不重复的特征</p>
<p><code>sub_sample = sample_f[:, s]</code>在s列的所有行数据</p>
<p><code>votes.append(model.predict([sub_f])[0])  # 获取预测结果</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np
from collections import Counter
from sklearn.tree import DecisionTreeClassifier

class RandomForestClassifier():
    def __init__(self, n_model&#x3D;10):
        &#39;&#39;&#39;
        初始化函数
        &#39;&#39;&#39;
        self.n_model &#x3D; n_model
        self.feature_nums &#x3D; None
        self.models &#x3D; []
        self.col_indexs &#x3D; []

    def fit(self, feature, label):
        &#39;&#39;&#39;
        训练模型
        :param feature: 训练集数据，类型为ndarray
        :param label: 训练集标签，类型为ndarray
        :return: None
        &#39;&#39;&#39;
        n, l &#x3D; feature.shape
        if self.feature_nums is None:
            self.feature_nums &#x3D; int(np.log2(l))  # 取整以确保feature_nums是整数

        for i in range(self.n_model):
            # 有放回采样
            sample &#x3D; np.random.choice(n, n, replace&#x3D;True)
            sample_f &#x3D; feature[sample]
            sample_l &#x3D; label[sample]

            # 随机选择特征列
            s &#x3D; np.random.choice(l, self.feature_nums, replace&#x3D;False)
            self.col_indexs.append(s)

            sub_sample &#x3D; sample_f[:, s]
            model &#x3D; DecisionTreeClassifier(max_depth&#x3D;3)
            model.fit(sub_sample, sample_l)
            self.models.append(model)

    def predict(self, feature):
        &#39;&#39;&#39;
        :param feature: 测试集数据，类型为ndarray
        :return: 预测结果，类型为ndarray
        &#39;&#39;&#39;
        predictions &#x3D; []   
        for j in range(feature.shape[0]):  # 遍历每个测试样本
            votes &#x3D; []
            for i, model in enumerate(self.models):  # 遍历每棵决策树
                col &#x3D; self.col_indexs[i]  # 获取模型训练时使用的特征列
                sub_f &#x3D; feature[j, col]  # 获取测试样本的特征
                votes.append(model.predict([sub_f])[0])  # 获取预测结果

            # 投票机制，选择出现次数最多的结果
            predictions.append(max(set(votes), key&#x3D;votes.count))

        return np.array(predictions)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="层次聚类距离计算："><a href="#层次聚类距离计算：" class="headerlink" title="层次聚类距离计算："></a>层次聚类距离计算：</h3><p><code>dist = np.linalg.norm(i - j) #直接计算i，j的欧氏距离</code></p>
<p><code>return np.min(dist) #np.argmin(dist)返回索引</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def calc_min_dist(cluster1, cluster2):
    &#39;&#39;&#39;
    计算簇间最小距离
    :param cluster1:簇1中的样本数据，类型为ndarray
    :param cluster2:簇2中的样本数据，类型为ndarray
    :return:簇1与簇2之间的最小距离
    &#39;&#39;&#39;
    min_dist &#x3D; float(&#39;inf&#39;)  # 初始化最小距离为正无穷大
    for i in cluster1:
        for j in cluster2:
            dist &#x3D; np.linalg.norm(i - j)  # 计算欧几里得距离
            if dist &lt; min_dist:
                min_dist &#x3D; dist
    return min_dist


def calc_max_dist(cluster1, cluster2):
    &#39;&#39;&#39;
    计算簇间最大距离
    :param cluster1:簇1中的样本数据，类型为ndarray
    :param cluster2:簇2中的样本数据，类型为ndarray
    :return:簇1与簇2之间的最大距离
    &#39;&#39;&#39;
    max_dist &#x3D; -float(&#39;inf&#39;)  # 初始化最大距离为负无穷大
    for i in cluster1:
        for j in cluster2:
            dist &#x3D; np.linalg.norm(i - j)  # 计算欧几里得距离
            if dist &gt; max_dist:
                max_dist &#x3D; dist
    return max_dist


def calc_avg_dist(cluster1, cluster2):
    &#39;&#39;&#39;
    计算簇间平均距离
    :param cluster1:簇1中的样本数据，类型为ndarray
    :param cluster2:簇2中的样本数据，类型为ndarray
    :return:簇1与簇2之间的平均距离
    &#39;&#39;&#39;
    total_dist &#x3D; 0
    count1 &#x3D; 0
    count2 &#x3D; 0
    for i in cluster1:
        for j in cluster2:
            total_dist +&#x3D; np.linalg.norm(i - j)  # 计算欧几里得距离

    return total_dist &#x2F; (len(cluster1)*len(cluster2))  # 返回平均距离<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><h4 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h4><p>曼哈顿距离即每个维度距离求和，欧氏距离即空间直线距离</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def distance(x,y,p&#x3D;2):
    &#39;&#39;&#39;
    input:x(ndarray):第一个样本的坐标
          y(ndarray):第二个样本的坐标
          p(int):等于1时为曼哈顿距离，等于2时为欧氏距离
    output:distance(float):x到y的距离      
    &#39;&#39;&#39; 
    if(p&#x3D;&#x3D;1):
          return sum(np.abs(x-y))
    else:
          return np.sqrt(sum((x-y)**2))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="质心计算"><a href="#质心计算" class="headerlink" title="质心计算"></a>质心计算</h4><p><code>dist=np.power(np.sum(np.abs(x-y)**p),1/p)</code></p>
<p><code>Cmass=np.mean(data,axis=0);</code></p>
<p><code>dist_list=sorted(dist)</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8
import numpy as np
#计算样本间距离
def distance(x, y, p&#x3D;2):

    #********* Begin *********#
    # 计算x和y之间差的绝对值的p次方  
    dist&#x3D;np.power(np.sum(np.abs(x-y)**p),1&#x2F;p)
    # 计算最终的距离值 
    #********* End *********#
    return dist

#计算质心
def cal_Cmass(data):
    &#39;&#39;&#39;
    input:data(ndarray):数据样本
    output:mass(ndarray):数据样本质心
    &#39;&#39;&#39;
    #********* Begin *********#
    # 计算数据样本的质心（即每个维度的平均值）  
    Cmass&#x3D;np.mean(data,axis&#x3D;0)
    #********* End *********#
    return Cmass

#计算每个样本到质心的距离，并按照从小到大的顺序排列
def sorted_list(data,Cmass):
    &#39;&#39;&#39;
    input:data(ndarray):数据样本
          Cmass(ndarray):数据样本质心
    output:dis_list(list):排好序的样本到质心距离
    &#39;&#39;&#39;
    #********* Begin *********#
    # 初始化一个空列表，用于存储距离  
    dist&#x3D;[]
    # 遍历数据样本中的每个样本 
    for sample in data:
        dis&#x3D;distance(Cmass,sample)
        dist.append(dis)
    # 计算当前样本到质心的距离，并添加到列表中  
    # 对距离列表进行排序  
    dist_list&#x3D;sorted(dist)
    return dist_list
    #********* End *********#


<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="k-means-1"><a href="#k-means-1" class="headerlink" title="k-means"></a>k-means</h4><p><code>model = KMeans(n_clusters=k, init=&#39;k-means++&#39;, max_iter=500, tol=1e-4, random_state=42)</code></p>
<p>K-means 是一种 <strong>无监督学习</strong> 方法，它确实没有使用真实标签（ground truth labels），但仍然会为每个样本分配一个 <strong>聚类标签（cluster index）</strong>，这个标签是算法自动生成的，表示样本属于哪个簇（cluster）</p>
<ol>
<li><p><strong>初始化聚类中心</strong></p>
<ul>
<li>随机选择 <code>k</code> 个样本作为初始聚类中心 <code>centroids</code>。</li>
</ul>
</li>
<li><p><strong>迭代优化</strong>（直到收敛或达到最大迭代次数）：</p>
<ul>
<li><p><strong>步骤 1：分配样本到最近的簇</strong></p>
<ul>
<li><p>对每个样本 <code>x ∈ X</code>，计算它与所有 <code>centroids</code> 的距离，并分配到最近的簇。</p>
</li>
<li><p>代码实现：<code>create_clusters(centroids, X)</code>。</p>
</li>
</ul>
</li>
<li><p><strong>步骤 2：更新聚类中心</strong></p>
<ul>
<li><p>对每个簇，计算其所有样本的均值，作为新的聚类中心。</p>
</li>
<li><p>代码实现：<code>update_centroids(clusters)</code>。</p>
</li>
</ul>
</li>
<li><p><strong>步骤 3：检查收敛</strong></p>
<ul>
<li><p>如果新旧聚类中心的距离变化 <code>&lt;= ε</code>（<code>varepsilon</code>），则停止迭代。</p>
</li>
<li><p>否则，继续迭代。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>返回聚类标签</strong></p>
<ul>
<li>最终，为每个样本分配其最近的聚类中心的索引，作为聚类标签 <code>labels</code>。</li>
</ul>
</li>
</ol>
<p><code>labels = np.zeros(X.shape[0], dtype=int)</code></p>
<p><code>return np.argmin(distance)</code></p>
<p><code>clusters=[[] for _ in range(self.k)]</code></p>
<p><code>new_centroids[i] = np.mean(clusters[i], axis=0)</code></p>
<p><code>centroid_shift = np.linalg.norm(new_centroids - centroids)</code></p>
<p><code>#NumPy 提供的一个线性代数函数，用来计算矩阵的“范数”，在这里是计算 向量的欧几里得距离</code></p>
<p>sample=np.random.choice(len(X),self.k,replace=False)</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8
import numpy as np

# 计算一个样本与数据集中所有样本的欧氏距离的平方
def euclidean_distance(one_sample, X):
    return np.sum((X - one_sample) ** 2, axis&#x3D;1)


class Kmeans():
    &quot;&quot;&quot;Kmeans聚类算法.
    Parameters:
    -----------
    k: int
        聚类的数目.
    max_iterations: int
        最大迭代次数. 
    varepsilon: float
        判断是否收敛, 如果上一次的所有k个聚类中心与本次的所有k个聚类中心的差都小于varepsilon, 
        则说明算法已经收敛
    &quot;&quot;&quot;
    def __init__(self, k&#x3D;2, max_iterations&#x3D;500, varepsilon&#x3D;0.0001):
        self.k &#x3D; k
        self.max_iterations &#x3D; max_iterations
        self.varepsilon &#x3D; varepsilon
        np.random.seed(1)
    #********* Begin *********#
    # 从所有样本中随机选取self.k样本作为初始的聚类中心
    def init_random_centroids(self, X):
        sample&#x3D;np.random.choice(len(X),self.k,replace&#x3D;False)
        return X[sample]

    # 返回距离该样本最近的一个中心索引[0, self.k)
    def _closest_centroid(self, sample, centroids):
        distance&#x3D;euclidean_distance(sample,centroids)
        return np.argmin(distance)


    # 将所有样本进行归类，归类规则就是将该样本归类到与其最近的中心
    def create_clusters(self, centroids, X):
        clusters&#x3D;[[] for _ in range(self.k)]
        for sample in X:
            closest_idx&#x3D;self._closest_centroid(sample,centroids)
            clusters[closest_idx].append(sample)
        return clusters


    # 对中心进行更新
    def update_centroids(self, clusters, X):
        new_centroids &#x3D; np.zeros((self.k, len(clusters[0][0])))
        for i in range(self.k):
            # 对每个簇内样本求均值
            new_centroids[i] &#x3D; np.mean(clusters[i], axis&#x3D;0)
        return new_centroids


    # 对整个数据集X进行Kmeans聚类，返回其聚类的标签
    def predict(self, X):
        # 从所有样本中随机选取self.k样本作为初始的聚类中心
        centroids&#x3D;self.init_random_centroids(X)
        # 迭代，直到算法收敛(上一次的聚类中心和这一次的聚类中心几乎重合)或者达到最大迭代次数   
        for i in range(self.max_iterations):
            clusters&#x3D;self.create_clusters(centroids,X)
            new_centroids&#x3D;self.update_centroids(clusters,X)

            centroid_shift &#x3D; np.linalg.norm(new_centroids - centroids)
            if centroid_shift &lt;&#x3D; self.varepsilon:
                break
            # 更新聚类中心
            centroids &#x3D; new_centroids
            # 如果聚类中心几乎没有变化，说明算法已经收敛，退出迭代
        labels &#x3D; np.zeros(X.shape[0], dtype&#x3D;int)
        for i, sample in enumerate(X):
            labels[i] &#x3D; self._closest_centroid(sample, centroids)
        return labels

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><ul>
<li><p><strong>簇（Cluster）</strong> = <strong>高密度区域</strong>（数据点密集）。</p>
</li>
<li><p><strong>噪声（Noise）</strong> = <strong>低密度区域</strong>（数据点稀疏）。</p>
</li>
<li><p><strong><code>eps</code> (ε)</strong>：邻域半径，用于判断两个点是否“相邻”。</p>
</li>
<li><p><strong><code>min_samples</code></strong>：形成一个簇所需的最小样本数（核心点的邻域至少要有这么多点）。</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>定义</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>核心点（Core Point）</strong></td>
<td>在 <code>eps</code> 邻域内至少有 <code>min_samples</code> 个点（包括自己）</td>
<td>若 <code>min_samples=5</code>，某点周围有 ≥5 个点（含自己），则它是核心点</td>
</tr>
<tr>
<td><strong>边界点（Border Point）</strong></td>
<td>在某个核心点的 <code>eps</code> 邻域内，但自身不满足核心点条件</td>
<td>周围点 &lt; <code>min_samples</code>，但属于某个核心点的邻域</td>
</tr>
<tr>
<td><strong>噪声点（Noise Point）</strong></td>
<td>既不是核心点，也不是边界点</td>
<td>离群点，不属于任何簇</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>如果点 <code>q</code> 在点 <code>p</code> 的 <code>eps</code> 邻域内，且 <code>p</code> 是核心点，则称 <code>q</code> 从 <code>p</code> <strong>密度直达</strong>。</p>
</li>
<li><p>如果存在一个核心点序列 <code>p1, p2, ..., pn</code>，使得 <code>pi+1</code> 从 <code>pi</code> 密度直达，则称 <code>p1</code> 和 <code>pn</code> <strong>密度相连</strong>。</p>
</li>
</ul>
<ol>
<li><p><strong>随机选择一个未访问的点 <code>p</code></strong>。</p>
</li>
<li><p><strong>检查 <code>p</code> 的 <code>eps</code> 邻域</strong>：</p>
<ul>
<li><p>如果 <code>p</code> 是核心点（邻域内点数 ≥ <code>min_samples</code>）：</p>
<ul>
<li><p>创建一个新簇。</p>
</li>
<li><p>递归地找出所有从 <code>p</code> <strong>密度可达</strong>的点，加入该簇。</p>
</li>
</ul>
</li>
<li><p>如果 <code>p</code> 是噪声点，标记为噪声。</p>
</li>
</ul>
</li>
<li><p><strong>重复上述过程，直到所有点被访问</strong>。</p>
</li>
</ol>
<ul>
<li><p><strong>不需要预先指定簇数量（k）</strong>（比 K-means 更灵活）。</p>
</li>
<li><p><strong>能发现任意形状的簇</strong>（K-means 只能发现球形簇）。</p>
</li>
<li><p><strong>能识别噪声点</strong>（适合处理离群值）。</p>
</li>
<li><p><strong>对参数 <code>eps</code> 和 <code>min_samples</code> 敏感</strong>，需要调参。</p>
<p>| 特性           | DBSCAN                | K-means |<br>| —————— | ——————————- | ———- |<br>| <strong>簇形状</strong>      | 任意形状                  | 球形      |<br>| <strong>噪声处理</strong>     | 能识别噪声                 | 不能      |<br>| <strong>需指定簇数（k）</strong> | 不需要                   | 需要      |<br>| <strong>参数依赖</strong>     | <code>eps</code> 和 <code>min_samples</code> | <code>k</code>     |<br>| <strong>适合场景</strong>     | 非凸簇、噪声多               | 凸簇、数据均匀 |</p>
<p><code>neighbors += new_neighbors  # 将新邻域的点加入待处理列表</code><br><code>fit_predict()</code> 是一个结合了训练和预测的简化语法，通常在 <strong>聚类</strong> 或 <strong>无监督学习</strong> 中使用，它的作用是：首先对数据进行训练（拟合模型），然后直接对数据进行预测，返回聚类标签或者分类标签。这种方法可以简化代码并提高效率，尤其是在进行无监督学习时</p>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8 
import numpy as np
import random

# 寻找eps邻域内的点
def findNeighbor(j, X, eps):
    &quot;&quot;&quot;
    计算样本X[j]的eps邻域，返回所有在该邻域内的点的索引

    input:
        j (int): 当前点的索引
        X (ndarray): 样本数据
        eps (float): 邻域半径
    output:
        N (list): 邻域内的点的索引
    &quot;&quot;&quot;
    N &#x3D; []  # 存放邻域内的点
    for p in range(X.shape[0]):  # 遍历所有点
        # 计算欧式距离
        temp &#x3D; np.sqrt(np.sum(np.square(X[j] - X[p])))  
        if temp &lt;&#x3D; eps:  # 如果点p在eps半径范围内
            N.append(p)  # 将该点加入邻域列表
    return N

def expandCluster(X, labels, point, neighbors, cluster_id, eps, min_Pts):
    &quot;&quot;&quot;
    扩展簇，将密度可达的点加入到当前簇

    input:
        X (ndarray): 样本数据
        labels (list): 当前的聚类标签
        point (int): 当前扩展的核心点的索引
        neighbors (list): 核心点的邻域
        cluster_id (int): 当前簇的ID
        eps (float): 邻域半径
        min_Pts (int): 最小邻域内点数
    &quot;&quot;&quot;
    labels[point] &#x3D; cluster_id  # 将当前点标记为当前簇的成员
    i &#x3D; 0
    while i &lt; len(neighbors):  # 遍历邻域中的每一个点
        neighbor_point &#x3D; neighbors[i]
        # 如果邻居点是噪声（未分类），将其标记为当前簇的一部分
        if labels[neighbor_point] &#x3D;&#x3D; -1:  
            labels[neighbor_point] &#x3D; cluster_id
        # 如果邻居点没有被标记，且它是一个核心点，则继续扩展簇
        elif labels[neighbor_point] &#x3D;&#x3D; 0:
            labels[neighbor_point] &#x3D; cluster_id
            new_neighbors &#x3D; findNeighbor(neighbor_point, X, eps)
            if len(new_neighbors) &gt;&#x3D; min_Pts:  # 如果该点是核心点
                neighbors +&#x3D; new_neighbors  # 将新邻域的点加入待处理列表
        i +&#x3D; 1

# DBSCAN算法
def dbscan(X, eps, min_Pts):
    &#39;&#39;&#39;
    输入:
        X (ndarray): 样本数据
        eps (float): eps邻域半径
        min_Pts (int): eps邻域内最少点个数
    输出:
        labels (list): 聚类结果，每个点的标签（-1表示噪声，其他数字表示簇编号）
    &#39;&#39;&#39;
    # 样本数量
    nums &#x3D; len(X)
    labels &#x3D; np.zeros(nums)  # 初始化所有点的标签为0（表示未处理）
    cluster_id &#x3D; 0  # 聚类编号初始化为0

    # 遍历每个样本点
    for i in range(nums):
        if labels[i] !&#x3D; 0:  # 如果该点已经被处理过，跳过
            continue
        else:
            # 获取点i的邻域
            neighbors &#x3D; findNeighbor(i, X, eps)

            if len(neighbors) &lt; min_Pts:  # 如果邻域内的点数小于min_Pts，标记为噪声
                labels[i] &#x3D; -1
            else:  # 如果是核心点，则扩展簇
                cluster_id +&#x3D; 1  # 创建一个新的簇
                labels[i] &#x3D; cluster_id  # 将当前点标记为该簇
                expandCluster(X, labels, i, neighbors, cluster_id, eps, min_Pts)

    return labels  # 返回聚类结果

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="AGNES"><a href="#AGNES" class="headerlink" title="AGNES"></a>AGNES</h3><p>自底向上的不断合并簇直到簇数量满足要求</p>
<p><code>clusters[cova] += clusters[covb]</code> 合并簇</p>
<p><code>del clusters[covb]  # 删除第二个簇</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def AGNES(feature, k):
    &#39;&#39;&#39;
    AGNES聚类并返回聚类结果，量化距离时请使用簇间最大欧氏距离
    假设数据集为&#96;[1, 2], [10, 11], [1, 3]]，那么聚类结果可能为&#96;[[1, 2], [1, 3]], [[10, 11]]]
    :param feature:数据集，类型为ndarray
    :param k:表示想要将数据聚成&#96;k&#96;类，类型为&#96;int&#96;
    :return:聚类结果，类型为list
    &#39;&#39;&#39;
    #********* Begin *********#
    clusters&#x3D;[[feature[i]] for i in range(len(feature))]
    cluster_nums&#x3D;len(clusters)
    while cluster_nums &gt; k:
        dist_min &#x3D; 1e9
        cova, covb &#x3D; -1, -1

        # 寻找距离最小的簇对
        for idx1 in range(cluster_nums):
            for idx2 in range(idx1 + 1, cluster_nums):
                # 计算两个簇之间的最小欧氏距离
                dist &#x3D; []
                for i in clusters[idx1]:
                    for j in clusters[idx2]:
                        dist.append(np.linalg.norm(i - j))
                curr_max &#x3D; np.min(dist)  # 选择最小欧氏距离

                if curr_min &lt; dist_min:
                    dist_min &#x3D; curr_max
                    cova, covb &#x3D; idx1, idx2

        # 合并最小距离的两个簇
        clusters[cova] +&#x3D; clusters[covb]
        del clusters[covb]  # 删除第二个簇

        # 更新簇的数量
        cluster_nums -&#x3D; 1
    #********* End *********#
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>单次迭代</p>
<p><code>np.sum(np.abs(np.array(new_thetas)-np.array(thetas))</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def em_single(init_values, observations):
    pa, pb &#x3D; init_values
    a_heads &#x3D; 0
    a_tails &#x3D; 0
    b_heads &#x3D; 0
    b_tails &#x3D; 0

    for trial in observations:
        num_heads &#x3D; sum(trial)
        num_tails &#x3D; len(trial) - num_heads

        log_ka &#x3D; np.sum([np.log(pa if coin &#x3D;&#x3D; 1 else 1 - pa) for coin in trial])
        log_kb &#x3D; np.sum([np.log(pb if coin &#x3D;&#x3D; 1 else 1 - pb) for coin in trial])

        # Convert log-probabilities to probabilities safely
        max_log &#x3D; max(log_ka, log_kb)
        exp_ka &#x3D; np.exp(log_ka - max_log)
        exp_kb &#x3D; np.exp(log_kb - max_log)
        total &#x3D; exp_ka + exp_kb
        softa &#x3D; exp_ka &#x2F; total
        softb &#x3D; exp_kb &#x2F; total

        a_heads +&#x3D; softa * num_heads
        a_tails +&#x3D; softa * num_tails
        b_heads +&#x3D; softb * num_heads
        b_tails +&#x3D; softb * num_tails

    return [a_heads &#x2F; (a_heads + a_tails), b_heads &#x2F; (b_heads + b_tails)]

def em(observations, thetas, tol&#x3D;1e-4, iterations&#x3D;100):
    &quot;&quot;&quot;
    模拟抛掷硬币实验并使用EM算法估计硬币A与硬币B正面朝上的概率。
    :param observations: 抛掷硬币的实验结果记录，类型为list。
    :param thetas: 硬币A与硬币B正面朝上的概率的初始值，类型为list，如[0.2, 0.7]代表硬币A正面朝上的概率为0.2，硬币B正面朝上的概率为0.7。
    :param tol: 差异容忍度，即当EM算法估计出来的参数theta不怎么变化时，可以提前挑出循环。例如容忍度为1e-4，则表示若这次迭代的估计结果与上一次迭代的估计结果之间的L1距离小于1e-4则跳出循环。为了正确的评测，请不要修改该值。
    :param iterations: EM算法的最大迭代次数。为了正确的评测，请不要修改该值。
    :return: 将估计出来的硬币A和硬币B正面朝上的概率组成list或者ndarray返回。如[0.4, 0.6]表示你认为硬币A正面朝上的概率为0.4，硬币B正面朝上的概率为0.6。
    &quot;&quot;&quot;

    #********* Begin *********#
    new_thetas&#x3D;0
    for i in range(iterations):
        new_thetas&#x3D;em_single(thetas,observations)
        if(np.sum(np.abs(np.array(new_thetas)-np.array(thetas)))&lt;tol):
            break
        thetas&#x3D;new_thetas

    return new_thetas

    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p><img title="" src="/images/ml/330931b0-7aa1-49a4-9175-648ba48b488f.png" alt="330931b0-7aa1-49a4-9175-648ba48b488f" style="zoom:100%;"><img src="/images/ml/4c790a9f-bc87-4ae3-b8d2-0e71053ba5fe.png" title="" alt="4c790a9f-bc87-4ae3-b8d2-0e71053ba5fe" style="zoom:100%;"><img src="/images/ml/2511f0bd-2098-4af2-8995-1ad788f84bf2.png" title="" alt="2511f0bd-2098-4af2-8995-1ad788f84bf2" style="zoom:100%;"><img src="/images/ml/f58e9601-924d-4456-bfd0-82c71595e1aa.png" alt="f58e9601-924d-4456-bfd0-82c71595e1aa"></p>
<p><code>if y_true[i] == y_true[j] and y_pred[i] == y_pre #求与操作用and，&amp;是位运算</code></p>
<p><code>a+=1</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def calc(y_true, y_pred):
    a &#x3D; b &#x3D; c &#x3D; d &#x3D; 0
    for i in range(len(y_true)):
        for j in range(i + 1, len(y_pred)):
            if y_true[i] &#x3D;&#x3D; y_true[j] and y_pred[i] &#x3D;&#x3D; y_pred[j]:
                a +&#x3D; 1
            elif y_true[i] &#x3D;&#x3D; y_true[j] and y_pred[i] !&#x3D; y_pred[j]:
                b +&#x3D; 1
            elif y_true[i] !&#x3D; y_true[j] and y_pred[i] &#x3D;&#x3D; y_pred[j]:
                c +&#x3D; 1
            else:
                d +&#x3D; 1
    return [a, b, c, d]

def calc_JC(y_true, y_pred):
    ex &#x3D; calc(y_true, y_pred)
    return ex[0] &#x2F; (ex[0] + ex[1] + ex[2]) if (ex[0] + ex[1] + ex[2]) !&#x3D; 0 else 0.0

def calc_FM(y_true, y_pred):
    ex &#x3D; calc(y_true, y_pred)
    if (ex[0] + ex[1]) &#x3D;&#x3D; 0 or (ex[0] + ex[2]) &#x3D;&#x3D; 0:
        return 0.0
    return np.sqrt((ex[0] &#x2F; (ex[0] + ex[1])) * (ex[0] &#x2F; (ex[0] + ex[2])))

def calc_Rand(y_true, y_pred):
    m &#x3D; len(y_true)
    ex &#x3D; calc(y_true, y_pred)
    return 2 * (ex[0] + ex[3]) &#x2F; (m * (m - 1)) if m &gt; 1 else 0.0
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>DB 指数越小就越就意味着簇内距离越小同时簇间距离越大，也就是说DB 指数越小越好</p>
<p>Dunn 指数越大意味着簇内距离越小同时簇间距离越大，也就是说 Dunn 指数 越大越好</p>
<p><img title="" src="/images/ml/29169c68-916f-4aca-b12f-9cf208277638.png" alt="29169c68-916f-4aca-b12f-9cf208277638" style="zoom:100%;"><img title="" src="/images/ml/a27f2156-e19b-448d-a1a4-35ff3967c1e0.png" alt="a27f2156-e19b-448d-a1a4-35ff3967c1e0" style="zoom:100%;"></p>
<p><code>np.array,arr.tolist</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np
a &#x3D; [1, 2, 3]
arr &#x3D; np.array(a)

arr &#x3D; np.array([1, 2, 3])
a &#x3D; arr.tolist()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>centroids = np.array([np.mean(feature[pred == label], axis=0) for label in labels])</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def calc_DBI(feature, pred):
    labels &#x3D; np.unique(pred)
    k &#x3D; len(labels)
    centroids &#x3D; np.array([np.mean(feature[pred &#x3D;&#x3D; label], axis&#x3D;0) for label in labels])

    # 计算每个簇的类内平均距离（紧密度）
    s &#x3D; []
    for label in labels:
        cluster_points &#x3D; feature[pred &#x3D;&#x3D; label]
        centroid &#x3D; np.mean(cluster_points, axis&#x3D;0)
        distances &#x3D; np.sqrt(np.sum((cluster_points - centroid) ** 2, axis&#x3D;1))
        s.append(np.mean(distances))
    s &#x3D; np.array(s)

    # 计算簇质心间的距离矩阵
    M &#x3D; np.zeros((k, k))
    for i in range(k):
        for j in range(k):
            if i !&#x3D; j:
                M[i][j] &#x3D; np.linalg.norm(centroids[i] - centroids[j])

    # DBI 计算
    dbi &#x3D; 0
    for i in range(k):
        max_r &#x3D; -np.inf
        for j in range(k):
            if i !&#x3D; j:
                r &#x3D; (s[i] + s[j]) &#x2F; M[i][j]
                if r &gt; max_r:
                    max_r &#x3D; r
        dbi +&#x3D; max_r
    return dbi &#x2F; k


    for label in labels:
        feat&#x3D;feature[pred&#x3D;&#x3D;label]
        center&#x3D;np.mean(feat,axis&#x3D;0)
        dist&#x3D;[]
        for i in feat:
            dist.append(np.linalg.norm(i-center))
        s.append(np.mean(dist))
    #s[i]为簇内平均距离
    maxsum&#x3D;0
    for i in range(k):
        max1&#x3D;0
        for j in range(k):
            if(i!&#x3D;j):
                max1&#x3D;max((s[i]+s[j])&#x2F;np.linalg.norm(centroids[i]-centroids[j]),max1)
        maxsum+&#x3D;max1
    return maxsum&#x2F;k



    #********* End *********#


def calc_DI(feature, pred):
    labels &#x3D; np.unique(pred)
    k &#x3D; len(labels)

    # 计算每个簇的最大内部距离
    max_intra &#x3D; 0
    for label in labels:
        points &#x3D; feature[pred &#x3D;&#x3D; label]
        for i in range(len(points)):
            for j in range(i + 1, len(points)):
                dist &#x3D; np.linalg.norm(points[i] - points[j])
                if dist &gt; max_intra:
                    max_intra &#x3D; dist

    # 计算簇间最小距离
    min_inter &#x3D; np.inf
    for i in range(k):
        for j in range(i + 1, k):
            points_i &#x3D; feature[pred &#x3D;&#x3D; labels[i]]
            points_j &#x3D; feature[pred &#x3D;&#x3D; labels[j]]
            for p in points_i:
                for q in points_j:
                    dist &#x3D; np.linalg.norm(p - q)
                    if dist &lt; min_inter:
                        min_inter &#x3D; dist

    return min_inter &#x2F; max_intra if max_intra !&#x3D; 0 else 0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>维数灾难通常是指对于已知样本数目，存在一个特征数目的最大值，当实际使用的特征数目超过这个最大值时，机器学习算法的性能不是得到改善，而是退化（过拟合）。</p>
<ul>
<li><p>降低机器学习算法的时间复杂度；</p>
</li>
<li><p>节省了提取不必要特征的开销；</p>
</li>
<li><p>缓解因为维数灾难所造成的过拟合现象。</p>
</li>
</ul>
<h3 id="PCA主成分分析"><a href="#PCA主成分分析" class="headerlink" title="PCA主成分分析"></a>PCA主成分分析</h3><p> PCA 是将数据从原来的坐标系转换到新的坐标系，新的坐标系的选择是由数据本身决定的。</p>
<p>第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且方差最大的方向。然后该过程一直重复，重复次数为原始数据中的特征数量。最后会发现大部分方差都包含在最前面几个新坐标轴中，因此可以忽略剩下的坐标轴，从而达到降维的目的。</p>
<p> PCA 在降维时，需要指定将维度降至多少维，假设降至 k 维，则 PCA 的算法流程如下：</p>
<p>demean；<br>计算数据的协方差矩阵；<br>计算协方差矩阵的特征值与特征向量；<br>按照特征值，将特征向量从大到小进行排序；<br>选取前 k 个特征向量作为转换矩阵；<br>demean 后的数据与转换矩阵做矩阵乘法获得降维后的数据</p>
<p><code>matrix=np.cov(data.T) #cov函数期望行代表特征</code></p>
<p><code>val, vec = np.linalg.eigh(mat) #计算特征值和特征向量</code></p>
<p><code>indices = np.argsort(val)[::-1]  # 倒序排序，val越大越重要</code></p>
<p><code>vec_res = vec_sort[:,:k]</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def pca(data, k):
    &#39;&#39;&#39;
    对data进行PCA，并将结果返回
    :param data: 数据集，类型为ndarray
    :param k: 想要降成几维，类型为int
    :return: 降维后的数据，类型为ndarray
    &#39;&#39;&#39;

    #********* Begin *********#
    # 均值归一化
    me &#x3D; np.mean(data, axis&#x3D;0)
    data &#x3D; data - me

    # 计算协方差矩阵    
    # after_demean的行数为样本个数，列数为特征个数
    # 由于cov函数的输入希望是行代表特征，列代表数据的矩阵，所以要转置
    mat &#x3D; np.cov(data.T)

    # 计算特征值和特征向量
    val, vec &#x3D; np.linalg.eigh(mat)

    # 按特征值从大到小排序，选择前k个特征向量
    indices &#x3D; np.argsort(val)[::-1]  # 倒序排序，val越大越重要
    vec_sort &#x3D; vec[:, indices]       # 按排序后的索引选择特征向量

    # 选择前k个主成分
    vec_res &#x3D; vec_sort[:, [i for i in range(k)]]        # 取前k列

    # 返回降维后的数据
    return data.dot(vec_res)
    #********* End *********#

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="MDS多维缩放"><a href="#MDS多维缩放" class="headerlink" title="MDS多维缩放"></a>MDS多维缩放</h3><p>MDS 算法认为，在数据样本中，每个样本的每个特征值并不是数据间关系的必要特征，真正的基础特征是每个点与数据集中其他点的距离。</p>
<p><img src="/images/ml/52b1aa56-a1b4-4912-9f37-958227f2295d.png" title="" alt="52b1aa56-a1b4-4912-9f37-958227f2295d" style="zoom:100%;"></p>
<p><code>H = np.eye(n_samples) - np.ones((n_samples, n_samples)) / n_samples #单位阵减去1/n全一矩阵</code></p>
<p><code>B = -0.5 * np.dot(H, np.dot(distance_matrix ** 2, H))</code></p>
<p><code>eigvals_k = np.diag(np.sqrt(eigvals_sorted[:k])) #取前 k 个排序好的特征值，先开平方，然后把它们放到一个对角矩阵中</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

def euclidean_distance(x, y):
    &quot;&quot;&quot;计算欧几里得距离&quot;&quot;&quot;
    return np.sqrt(np.sum((x - y) ** 2))

def compute_distance_matrix(data):
    &quot;&quot;&quot;计算距离矩阵&quot;&quot;&quot;
    n_samples &#x3D; data.shape[0]
    distance_matrix &#x3D; np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(n_samples):
           distance_matrix[i, j] &#x3D; euclidean_distance(data[i], data[j])
    return distance_matrix

def mds(data, k):
    &quot;&quot;&quot;
    手动实现MDS降维
    :param data: 原始数据 (样本，特征) 形式的 numpy 数组
    :param k: 目标降维的维度，默认2
    :return: 降维后的数据
    &quot;&quot;&quot;
    # 1. 计算距离矩阵
    distance_matrix &#x3D; compute_distance_matrix(data)

    # 2. 中心化距离矩阵
    n_samples &#x3D; distance_matrix.shape[0]
    H &#x3D; np.eye(n_samples) - np.ones((n_samples, n_samples)) &#x2F; n_samples
    B &#x3D; -0.5 * np.dot(H, np.dot(distance_matrix ** 2, H))

    # 3. 特征值分解
    eigvals, eigvecs &#x3D; np.linalg.eigh(B)

    # 4. 按照特征值从大到小排序
    sorted_indices &#x3D; np.argsort(eigvals)[::-1]  # 降序排列
    eigvals_sorted &#x3D; eigvals[sorted_indices]
    eigvecs_sorted &#x3D; eigvecs[:, sorted_indices]

    # 5. 选择前k个特征值和特征向量
    eigvals_k &#x3D; np.diag(np.sqrt(eigvals_sorted[:k]))
    eigvecs_k &#x3D; eigvecs_sorted[:, :k]

    # 6. 计算低维坐标
    low_dim_data &#x3D; np.dot(eigvecs_k, eigvals_k)

    return low_dim_data  # 返回转置后的低维数据



# -*- coding: utf-8 -*-
from sklearn.manifold import MDS

def mds(data,d):
    &#39;&#39;&#39;
    input:data(ndarray):待降维数据
          d(int):降维后数据维度
    output:Z(ndarray):降维后数据
    &#39;&#39;&#39;
    #********* Begin *********#
    model&#x3D;MDS(n_components&#x3D;d)
#n_components ：即我们进行 MDS 降维时降到的维数。在降维时需要输入这个参数。
    Z&#x3D;model.fit_transform(data)

    #********* End *********#
    return Z

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Isomap等度量映射"><a href="#Isomap等度量映射" class="headerlink" title="Isomap等度量映射"></a>Isomap等度量映射</h3><p>Isomap 算法可以看作是 “在局部保持邻居关系的基础上，再用 MDS 降维”</p>
<p><img src="/images/ml/12a51984-99cd-49db-8d4d-223eb5b394bc.png" alt="12a51984-99cd-49db-8d4d-223eb5b394bc"></p>
<p><code>Z = V @ L #特征向量组成的矩阵在左边，乘特征值构成的的对角阵</code></p>
<p><code>model = Isomap(n_components=d,n_neighbors=k) 取k个相邻数据，降到d维</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -*- coding: utf-8 -*-
import numpy as np

def isomap(data, d, k, Max&#x3D;1e9):
    &#39;&#39;&#39;
    input:
        data(ndarray): 待降维数据，shape&#x3D;(n_samples, n_features)
        d(int): 降维后数据维数
        k(int): 最近的k个样本
        Max(float): 表示无穷大
    output:
        Z(ndarray): 降维后的数据，shape&#x3D;(n_samples, d)
    &#39;&#39;&#39;
    n &#x3D; data.shape[0]

    # Step 1: 计算欧氏距离矩阵
    dist_matrix &#x3D; np.zeros((n, n))
    for i in range(n):
        for j in range(i + 1, n):
            dist &#x3D; np.linalg.norm(data[i] - data[j])
            dist_matrix[i][j] &#x3D; dist_matrix[j][i] &#x3D; dist

    # Step 2: 构建 k-近邻图（非邻接的点设为 Max）
    graph &#x3D; np.full((n, n), Max)
    for i in range(n):
        neighbors &#x3D; np.argsort(dist_matrix[i])[1:k+1]  # 取前k个最近邻（排除自身）
        for j in neighbors:
            graph[i][j] &#x3D; dist_matrix[i][j]
            graph[j][i] &#x3D; dist_matrix[i][j]  # 保持对称

    # Step 3: 使用 Floyd-Warshall 算法求最短路径（近似测地线）
    for k_mid in range(n):
        for i in range(n):
            for j in range(n):
                if graph[i][j] &gt; graph[i][k_mid] + graph[k_mid][j]:
                    graph[i][j] &#x3D; graph[i][k_mid] + graph[k_mid][j]

    # Step 4: 多维尺度分析 (MDS)
    D &#x3D; graph
    D_squared &#x3D; D ** 2
    H &#x3D; np.eye(n) - np.ones((n, n)) &#x2F; n
    B &#x3D; -0.5 * H @ D_squared @ H

    # Step 5: 特征值分解
    eigvals, eigvecs &#x3D; np.linalg.eigh(B)
    idx &#x3D; np.argsort(eigvals)[::-1]
    eigvals &#x3D; eigvals[idx]
    eigvecs &#x3D; eigvecs[:, idx]

    # Step 6: 计算嵌入坐标 Z
    L &#x3D; np.diag(np.sqrt(eigvals[:d]))
    V &#x3D; eigvecs[:, :d]
    Z &#x3D; V @ L

    return Z


# -*- coding: utf-8 -*-
from sklearn.manifold import Isomap

def isomap(data,d,k):
    &#39;&#39;&#39;
    input:data(ndarray):待降维数据
          d(int):降维后数据维度
          k(int):最近的k个样本
    output:Z(ndarray):降维后数据
    &#39;&#39;&#39;
    #********* Begin *********#
    model &#x3D; Isomap(n_components&#x3D;d,n_neighbors&#x3D;k)
    Z&#x3D;model.fit_transform(data)

    #********* End *********#
    return Z
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="LLE局部线性嵌入"><a href="#LLE局部线性嵌入" class="headerlink" title="LLE局部线性嵌入"></a>LLE局部线性嵌入</h3><p><img src="/images/ml/8fe236dc-3502-4bd8-9969-bbe2f0fd0db8.png" title="" alt="8fe236dc-3502-4bd8-9969-bbe2f0fd0db8" style="zoom:100%;"></p>
<p><code>Z = data[N_i] - data[i]  # shape: (k, m)</code></p>
<p> <code>C = Z.dot(Z.T)  # 计算邻居之间差异的协方差矩阵 C，形状是 (k, k)</code></p>
<p><code>C += np.eye(k) * 1e-3  # 为了数值稳定，给协方差矩阵 C 加上一个小的正则项（0.001）</code></p>
<p><code>w = np.linalg.solve(C, ones)  # 解线性方程组，得到每个邻居的权重</code></p>
<p><code>W[i, N_i[j]] = w[j]  # 将第 j 个邻居的权重填入相应位置</code></p>
<p><code>M = (I - W).T.dot(I - W)  # 计算矩阵 M，形式为 (I - W)^T * (I - W)</code></p>
<p> <code>Z = eigvecs[:, idx[1:d+1]]  # 选取最小的 d 个特征值对应的特征向量（跳过第一个全1特征向量）</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># encoding&#x3D;utf8 
import numpy as np

# 计算数据集的欧氏距离矩阵
def calc_dist(data):
    n &#x3D; data.shape[0]  # 获取样本数目
    dist_matrix &#x3D; np.zeros((n, n))  # 初始化一个 n x n 的距离矩阵
    for i in range(n):
        for j in range(n):
            # 计算第 i 个样本与第 j 个样本的欧氏距离
            dist_matrix[i, j] &#x3D; np.linalg.norm(data[i] - data[j])  # np.linalg.norm 计算欧氏距离
    return dist_matrix

# 找到每个样本的 k 个最近邻
def find_neighbors(dist_matrix, k):
    n &#x3D; dist_matrix.shape[0]  # 获取样本数目
    neighbors &#x3D; np.zeros((n, k), dtype&#x3D;int)  # 初始化一个 n x k 的邻居矩阵
    for i in range(n):
        # 对于每个样本，按距离从小到大排序，找到前 k 个邻居（排除自己）
        sorted_indices &#x3D; np.argsort(dist_matrix[i])  # 排序，返回排序后的索引
        neighbors[i] &#x3D; sorted_indices[1:k+1]  # 排除自己（即第一个是自己，跳过）
    return neighbors

# 局部线性嵌入（LLE）算法
def lle(data, d, k):
    n, m &#x3D; data.shape  # n: 样本数目, m: 原始特征维度
    dist_matrix &#x3D; calc_dist(data)  # 计算数据点之间的距离矩阵
    neighbors &#x3D; find_neighbors(dist_matrix, k)  # 找到每个样本的 k 个邻居

    # 初始化权重矩阵 W，存储每个样本到其邻居的权重
    W &#x3D; np.zeros((n, n))

    # 对于每个样本，计算其到邻居的权重
    for i in range(n):
        # 获取第 i 个样本的 k 个邻居的索引
        N_i &#x3D; neighbors[i]
        # 构建邻居矩阵 Z：每列是一个邻居与 x_i 的差
        Z &#x3D; data[N_i] - data[i]  # Z 的形状是 (k, m)，即每行是一个邻居的差向量
        C &#x3D; Z.dot(Z.T)  # 计算邻居之间差异的协方差矩阵 C，形状是 (k, k)
        C +&#x3D; np.eye(k) * 1e-3  # 为了数值稳定，给协方差矩阵 C 加上一个小的正则项（0.001）
#在计算协方差矩阵 C &#x3D; Z.dot(Z.T) 时，可能会遇到矩阵 C 变得奇异（即行列式为零），特别是当邻居之间的差异非常小或者完全相同时。这种情况下，矩阵 C 可能不可逆，导致无法进行后续的线性求解。

        # 解线性方程 Cw &#x3D; 1，得到权重 w
        ones &#x3D; np.ones(k)  # 创建一个大小为 k 的全1向量
        w &#x3D; np.linalg.solve(C, ones)  # 解线性方程组，得到每个邻居的权重
        w &#x3D; w &#x2F; np.sum(w)  # 归一化权重，使得它们的和为 1

        # 将计算得到的权重 w 填入第 i 行的权重矩阵 W 中
        for j in range(k):
            W[i, N_i[j]] &#x3D; w[j]  # 将第 j 个邻居的权重填入相应位置

    # 构造矩阵 M &#x3D; (I - W)^T * (I - W)
    I &#x3D; np.eye(n)  # n x n 的单位矩阵
    M &#x3D; (I - W).T.dot(I - W)  # 计算矩阵 M，形式为 (I - W)^T * (I - W)

    # 计算 M 的特征值和特征向量
    eigvals, eigvecs &#x3D; np.linalg.eigh(M)  # eigvals: 特征值, eigvecs: 特征向量

    # 选择最小的 d+1 个特征值对应的特征向量（第一个特征值是 0 对应的全1向量，跳过）
    idx &#x3D; np.argsort(eigvals)  # 获取特征值的排序索引
    Z &#x3D; eigvecs[:, idx[1:d+1]]  # 选取最小的 d 个特征值对应的特征向量（跳过第一个全1特征向量）

    return Z  # 返回降维后的数据



# -*- coding: utf-8 -*-
from sklearn.manifold import LocallyLinearEmbedding

def lle(data,d,k):
    &#39;&#39;&#39;
    input:data(ndarray):待降维数据
          d(int):降维后数据维度
          k(int):邻域内样本数
    output:Z(ndarray):降维后数据
    &#39;&#39;&#39;
    #********* Begin *********#
    model&#x3D;LocallyLinearEmbedding(n_components&#x3D;d,n_neighbors&#x3D;k)
    Z&#x3D;model.fit_transform(data)

    #********* End *********#
    return Z
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Chen Zhou</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://whuchenzhou.github.io/2025/10/08/ML/">https://whuchenzhou.github.io/2025/10/08/ML/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Chen Zhou</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Python/">
                                    <span class="chip bg-color">Python</span>
                                </a>
                            
                                <a href="/tags/%E7%AE%97%E6%B3%95/">
                                    <span class="chip bg-color">算法</span>
                                </a>
                            
                                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">机器学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/10/11/Multi-View-3D-Point-Tracking/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/1.jpg" class="responsive-img" alt="Multi-View 3D Point Tracking">
                        
                        <span class="card-title">Multi-View 3D Point Tracking</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/2025ICCV/" class="post-category">
                                    2025ICCV
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%A4%9A%E8%A7%86%E8%A7%92%E5%BB%BA%E6%A8%A1/">
                        <span class="chip bg-color">多视角建模</span>
                    </a>
                    
                    <a href="/tags/3D%E7%82%B9%E8%BF%BD%E8%B8%AA/">
                        <span class="chip bg-color">3D点追踪</span>
                    </a>
                    
                    <a href="/tags/%E7%82%B9%E4%BA%91/">
                        <span class="chip bg-color">点云</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/10/08/Image-as-IMU/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="Image as IMU">
                        
                        <span class="card-title">Image as IMU</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/2025ICCV/" class="post-category">
                                    2025ICCV
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E8%BF%90%E5%8A%A8%E6%A8%A1%E7%B3%8A/">
                        <span class="chip bg-color">运动模糊</span>
                    </a>
                    
                    <a href="/tags/%E8%BF%90%E5%8A%A8%E4%BC%B0%E8%AE%A1/">
                        <span class="chip bg-color">运动估计</span>
                    </a>
                    
                    <a href="/tags/CV/">
                        <span class="chip bg-color">CV</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2025</span>
            
            <a href="/about" target="_blank">Chen Zhou</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/WhuChenzhou" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:whuchenzhou@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1157979681" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1157979681" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/sakura.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
