<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>西瓜书中涉及的机器学习算法</title>
      <link href="/2025/10/08/ML/"/>
      <url>/2025/10/08/ML/</url>
      
        <content type="html"><![CDATA[<p>这是笔者在准备nju推免机试时准备的自用资料，但是估计从今年（2025）过后不会再有这类型的机器学习算法题目了，可以作为学习西瓜书的一些参考（应该有不少谬误及typo）</p><hr><h2 id="常见损失函数"><a href="#常见损失函数" class="headerlink" title="常见损失函数"></a>常见损失函数</h2><h3 id="1-Mean-Squared-Error-MSE"><a href="#1-Mean-Squared-Error-MSE" class="headerlink" title="1. Mean Squared Error (MSE)"></a>1. Mean Squared Error (MSE)</h3><p>均方误差 (MSE) 用于回归任务，衡量预测值与真实值之间的平均平方差。</p><h4 id="数学公式："><a href="#数学公式：" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (y_{\text{true}}^i - y_{\text{pred}}^i)^2</script><ul><li>$y_{\text{true}}^i $是第 ( i ) 个样本的真实值。</li><li>$ y_{\text{pred}}^i$ 是第 ( i ) 个样本的预测值。</li><li>( N ) 是样本总数。</li></ul><h3 id="2-Mean-Absolute-Error-MAE"><a href="#2-Mean-Absolute-Error-MAE" class="headerlink" title="2. Mean Absolute Error (MAE)"></a>2. Mean Absolute Error (MAE)</h3><p>平均绝对误差 (MAE) 用于回归任务，衡量预测值与真实值之间的平均绝对差。</p><h4 id="数学公式：-1"><a href="#数学公式：-1" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{MAE}} = \frac{1}{N} \sum_{i=1}^{N} |y_{\text{true}}^i - y_{\text{pred}}^i|</script><ul><li>$ y_{\text{true}}^i $ 是第 ( i ) 个样本的真实值。</li><li>$ y_{\text{pred}}^i $ 是第 ( i ) 个样本的预测值。</li></ul><hr><h3 id="3-Binary-Cross-Entropy"><a href="#3-Binary-Cross-Entropy" class="headerlink" title="3. Binary Cross-Entropy"></a>3. Binary Cross-Entropy</h3><p>二分类交叉熵用于二分类任务，衡量模型预测的概率与真实标签之间的差异。</p><h4 id="数学公式：-2"><a href="#数学公式：-2" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{binary}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_{\text{true}}^i \log(y_{\text{pred}}^i) + (1 - y_{\text{true}}^i) \log(1 - y_{\text{pred}}^i) \right]</script><ul><li>( $y_{\text{true}}^i \in \{0, 1\}$ ) 是第 ( i ) 个样本的真实标签。</li><li>( $y_{\text{pred}}^i $) 是第 ( i ) 个样本的预测概率。</li></ul><hr><h3 id="4-Categorical-Cross-Entropy"><a href="#4-Categorical-Cross-Entropy" class="headerlink" title="4. Categorical Cross-Entropy"></a>4. Categorical Cross-Entropy</h3><p>多分类交叉熵用于多分类任务，衡量模型预测的概率分布与真实标签的分布之间的差异。</p><h4 id="数学公式：-3"><a href="#数学公式：-3" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{categorical}} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{\text{true},c}^i \log(y_{\text{pred},c}^i)</script><ul><li>( $y_{\text{true},c}^i \in \{0, 1\}$ ) 是第 ( i ) 个样本的真实标签的 one-hot 编码。</li><li>( $y_{\text{pred},c}^i $) 是第 ( i ) 个样本属于类别 ( c ) 的预测概率。</li><li>( C ) 是类别的总数。</li></ul><hr><h3 id="5-Hinge-Loss-SVM-Loss"><a href="#5-Hinge-Loss-SVM-Loss" class="headerlink" title="5. Hinge Loss (SVM Loss)"></a>5. Hinge Loss (SVM Loss)</h3><p>铰链损失（SVM损失）通常用于支持向量机（SVM），它最大化分类边界。</p><h4 id="数学公式：-4"><a href="#数学公式：-4" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{hinge}} = \frac{1}{N} \sum_{i=1}^{N} \max(0, 1 - y_{\text{true}}^i y_{\text{pred}}^i)</script><ul><li>( $y_{\text{true}}^i$ ) 是第 ( i ) 个样本的真实标签（取值为 ±1)）。</li><li>( $y_{\text{pred}}^i $) 是第 ( i ) 个样本的预测值。</li></ul><hr><h3 id="6-Huber-Loss"><a href="#6-Huber-Loss" class="headerlink" title="6. Huber Loss"></a>6. Huber Loss</h3><p>Huber 损失对回归任务中的异常值进行了处理，当误差较小时使用平方误差，误差较大时使用线性误差。</p><h4 id="数学公式：-5"><a href="#数学公式：-5" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{huber}} = \frac{1}{N} \sum_{i=1}^{N} \left[ \begin{array}{ll}0.5 \cdot \left| y_{\text{true}}^i - y_{\text{pred}}^i \right|^2 & \text{if } \left| y_{\text{true}}^i - y_{\text{pred}}^i \right| \leq \delta \\\delta \cdot \left( \left| y_{\text{true}}^i - y_{\text{pred}}^i \right| - 0.5 \cdot \delta \right) & \text{if } \left| y_{\text{true}}^i - y_{\text{pred}}^i \right| > \delta\end{array} \right.</script><ul><li>( \delta ) 是一个超参数，控制“平滑”与“惩罚”之间的平衡。</li><li>( y_{\text{true}}^i ) 是第 ( i ) 个样本的真实值。</li><li>( y_{\text{pred}}^i ) 是第 ( i ) 个样本的预测值。</li></ul><hr><h3 id="7-L2-Regularization-Loss"><a href="#7-L2-Regularization-Loss" class="headerlink" title="7. L2 Regularization Loss"></a>7. L2 Regularization Loss</h3><p>L2 正则化损失常用于防止过拟合，鼓励模型参数的平方和较小。</p><h4 id="数学公式：-6"><a href="#数学公式：-6" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{L2}} = \lambda_{\text{reg}} \sum_{j=1}^{M} w_j^2</script><ul><li>( w_j ) 是模型参数的第 ( j ) 个权重。</li><li>( \lambda_{\text{reg}} ) 是正则化系数。</li><li>( M ) 是参数的数量。</li><li><ul><li>惩罚的是参数的<strong>平方</strong>，所以对大的参数惩罚非常重（比如 w=10，惩罚是100；w=2，惩罚是4）。</li><li>它会让所有参数都<strong>向0收缩</strong>，但通常不会让任何参数<strong>精确为0</strong>。</li><li>结果是：<strong>所有特征都保留，但权重变小了</strong>，模型更平滑。</li><li>处理特征相关性强</li></ul></li></ul><hr><h3 id="8-L1-Regularization-Loss"><a href="#8-L1-Regularization-Loss" class="headerlink" title="8. L1 Regularization Loss"></a>8. L1 Regularization Loss</h3><p>L1 正则化损失常用于促进稀疏性，鼓励模型参数为零。</p><h4 id="数学公式：-7"><a href="#数学公式：-7" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">L_{\text{L1}} = \lambda_{\text{reg}} \sum_{j=1}^{M} |w_j|</script><ul><li>( w_j ) 是模型参数的第 ( j ) 个权重。</li><li>( \lambda_{\text{reg}} ) 是正则化系数。</li><li>( M ) 是参数的数量。</li><li><ul><li>惩罚的是参数的<strong>绝对值</strong>，对大参数和小参数的惩罚是线性的。</li><li>由于其几何特性（L1的等高线是菱形），优化过程中更容易“撞到角点”，而角点对应某些参数为0。</li><li>结果是：<strong>会把不重要的特征的权重直接压缩为0</strong>，实现<strong>特征选择</strong></li><li>促进参数权重的稀疏性</li></ul></li></ul><hr><h3 id="9-Kullback-Leibler-Divergence-KL-Divergence"><a href="#9-Kullback-Leibler-Divergence-KL-Divergence" class="headerlink" title="9. Kullback-Leibler Divergence (KL Divergence)"></a>9. Kullback-Leibler Divergence (KL Divergence)</h3><p>KL 散度衡量两个概率分布之间的差异，常用于概率模型和生成模型。</p><h4 id="数学公式：-8"><a href="#数学公式：-8" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">D_{\text{KL}}(p || q) = \sum_{i=1}^{N} p_i \log \left( \frac{p_i}{q_i} \right)</script><ul><li>( p_i ) 是真实分布的第 ( i ) 个概率值。</li><li>( q_i ) 是预测分布的第 ( i ) 个概率值。</li></ul><hr><h3 id="10-Softmax-Function"><a href="#10-Softmax-Function" class="headerlink" title="10. Softmax Function"></a>10. Softmax Function</h3><p>Softmax 函数将一个向量转换为概率分布，广泛应用于多分类问题。</p><h4 id="数学公式：-9"><a href="#数学公式：-9" class="headerlink" title="数学公式："></a>数学公式：</h4><script type="math/tex; mode=display">\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}}</script><ul><li>( x_i ) 是输入向量 ( x ) 中的第 ( i ) 个元素。</li><li>( C ) 是类别的总数。</li></ul><hr><h3 id="11-Softmax-Loss"><a href="#11-Softmax-Loss" class="headerlink" title="11. Softmax Loss"></a>11. Softmax Loss</h3><p>Softmax 损失（与交叉熵损失类似）常用于多分类问题中，结合 Softmax 函数计算损失。</p><h3 id="数学公式：-10"><a href="#数学公式：-10" class="headerlink" title="数学公式："></a>数学公式：</h3><script type="math/tex; mode=display">L_{\text{softmax}} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{\text{true},c}^i \log(y_{\text{pred},c}^i)</script><ul><li>( $y_{\text{true},c}^i \in \{0, 1\} $) 是第 ( i ) 个样本的真实标签的 one-hot 编码。</li><li>( $y_{\text{pred},c}^i $) 是第 ( i ) 个样本属于类别 ( c ) 的预测概率。</li></ul><hr><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np# 1. Mean Squared Error (MSE)def mean_squared_error(y_true, y_pred):    return np.mean((y_true - y_pred) ** 2)# 2. Mean Absolute Error (MAE)def mean_absolute_error(y_true, y_pred):    return np.mean(np.abs(y_true - y_pred))# 3. Binary Cross-Entropydef binary_crossentropy(y_true, y_pred):    epsilon &#x3D; 1e-15  # Avoid log(0)    y_pred &#x3D; np.clip(y_pred, epsilon, 1 - epsilon)  # Prevent y_pred from being 0 or 1    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))&#x2F;&#x2F;把 y_pred 的所有值限制在 [epsilon, 1 - epsilon] 范围内# 4. Categorical Cross-Entropydef categorical_crossentropy(y_true, y_pred):    #numpy.clip(a, a_min, a_max, out&#x3D;None)    epsilon &#x3D; 1e-15  # 避免对数计算时出现log(0)    y_pred &#x3D; np.clip(y_pred, epsilon, 1 - epsilon)  # 防止y_pred为0或1    #y_true 必须是 one-hot 编码（独热编码） 的形式    return -np.mean(np.sum(y_true * np.log(y_pred), axis&#x3D;1))    #axis&#x3D;1出来时竖着，&#x3D;0出来时横着# 5. Hinge Loss (SVM Loss)def hinge_loss(y_true, y_pred):    return np.mean(np.maximum(0, 1 - y_true * y_pred))# 6. Huber Lossdef huber_loss(y_true, y_pred, delta&#x3D;1.0):    error &#x3D; np.abs(y_true - y_pred)    is_small_error &#x3D; error &lt;&#x3D; delta    small_error_loss &#x3D; 0.5 * error**2    large_error_loss &#x3D; delta * (error - 0.5 * delta)    return np.mean(np.where(is_small_error, small_error_loss, large_error_loss))# 7. L2 Regularization Lossdef l2_regularization_loss(weights, lambda_reg&#x3D;0.01):    return lambda_reg * np.sum(weights**2)# 8. L1 Regularization Lossdef l1_regularization_loss(weights, lambda_reg&#x3D;0.01):    return lambda_reg * np.sum(np.abs(weights))# 9. Kullback-Leibler Divergence (KL Divergence)def kl_divergence(p, q):    epsilon &#x3D; 1e-15  # Prevent log(0)    p &#x3D; np.clip(p, epsilon, 1 - epsilon)    q &#x3D; np.clip(q, epsilon, 1 - epsilon)    return np.sum(p * np.log(p &#x2F; q))# 10. Softmax Functiondef softmax(x):    exp_x &#x3D; np.exp(x - np.max(x, axis&#x3D;-1, keepdims&#x3D;True))  # Prevent overflow    return exp_x &#x2F; np.sum(exp_x, axis&#x3D;-1, keepdims&#x3D;True)# 11. Softmax Lossdef softmax_loss(y_true, y_pred):    epsilon &#x3D; 1e-15    y_pred &#x3D; np.clip(y_pred, epsilon, 1 - epsilon)  # Prevent log(0)    return -np.mean(np.sum(y_true * np.log(y_pred), axis&#x3D;1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><p>如果数据中存在缺失值，可以选择填充它们，通常用均值或中位数来填充数值型数据。如果是类别数据，也可以选择填充众数。</p><p><code>mean_value = np.nanmean(column)#忽略nan值</code></p><p><code>column[np.isnan(column)] = mean_value # 填充nan缺失值为mean_value</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef fill_missing_values(data):    &#39;&#39;&#39;    使用每列的均值填充缺失值    :param data: 数据集，类型为ndarray    :return: 填充后的数据集    &#39;&#39;&#39;    # 遍历每一列，填充缺失值    for i in range(data.shape[1]):        column &#x3D; data[:, i]        # 计算列的均值（忽略nan）        mean_value &#x3D; np.nanmean(column)        # 填充缺失值        column[np.isnan(column)] &#x3D; mean_value    return data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p>对特征进行标准化，使其均值为0，方差为1，避免特征尺度对模型产生不良影响。标准化可以使用 <code>(x - mean) / std</code> 的方式来实现。</p><p><code>means=np.mean(data,axis=0) #取每一个特征的均值</code></p><p><code>std=np.std(data,axis=0) #取每一个特征的标准差</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def standardize_data(data):    &#39;&#39;&#39;    标准化数据：每个特征减去均值，再除以标准差    :param data: 数据集，类型为ndarray    :return: 标准化后的数据集    &#39;&#39;&#39;    # 计算每列的均值和标准差    means &#x3D; np.mean(data, axis&#x3D;0)    stds &#x3D; np.std(data, axis&#x3D;0)    # 标准化每一列    standardized_data &#x3D; (data - means) &#x2F; stds    return standardized_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p><img src="/images/ml/72967e50-080e-477c-91cc-73731e5a22db.png" alt="72967e50-080e-477c-91cc-73731e5a22db"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">min_vals &#x3D; np.min(data, axis&#x3D;0)max_vals &#x3D; np.max(data, axis&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">def normalize_data(data):    &#39;&#39;&#39;    归一化数据：将每个特征缩放到[0, 1]范围内    :param data: 数据集，类型为ndarray    :return: 归一化后的数据集    &#39;&#39;&#39;    # 计算每列的最小值和最大值    min_vals &#x3D; np.min(data, axis&#x3D;0)    max_vals &#x3D; np.max(data, axis&#x3D;0)    # 归一化每一列    normalized_data &#x3D; (data - min_vals) &#x2F; (max_vals - min_vals)    return normalized_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Z-score"><a href="#Z-score" class="headerlink" title="Z-score"></a>Z-score</h3><p>可以使用 Z-Score 方法来去除异常值。通常认为 Z-Score 大于 3 或小于 -3 的样本是异常值。</p><p><code>filtered_data = data[np.all(np.abs(z_scores) &lt; threshold, axis=1)]</code></p><ul><li><code>axis=1</code>：沿着列方向判断（即：对每个样本的所有特征判断）</li><li><code>np.all</code>：要求<strong>所有特征都满足条件</strong>（即：所有特征的 |Z| &lt; threshold）</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">def remove_outliers(data, threshold&#x3D;3):    &#39;&#39;&#39;    使用 Z-Score 去除异常值    :param data: 数据集，类型为ndarray    :param threshold: Z-Score 阈值，默认是3    :return: 去除异常值后的数据集    &#39;&#39;&#39;    # 计算每列的均值和标准差    means &#x3D; np.mean(data, axis&#x3D;0)    stds &#x3D; np.std(data, axis&#x3D;0)    # 计算 Z-Score    z_scores &#x3D; (data - means) &#x2F; stds    # 过滤 Z-Score 小于阈值的数据    filtered_data &#x3D; data[np.all(np.abs(z_scores) &lt; threshold, axis&#x3D;1)]    return filtered_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -&gt; np.ndarray:    # Your code here, make sure to round    m,n&#x3D;X.shape    theta&#x3D;np.zeros((n,1))    y &#x3D; y.reshape(-1, 1)  # (m,) -&gt; (m, 1)    for i in range(iterations):        # 计算预测误差        error &#x3D; X @ theta - y  # (m, 1)        # 计算梯度: (1&#x2F;m) * X^T @ error        gradient &#x3D; (1&#x2F;m) * X.T @ error        # 更新参数        theta -&#x3D; alpha * gradient    return np.round(theta,4)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Python 尝试广播 <code>(3,1)</code> 和 <code>(3,)</code>，但<strong>广播规则不匹配</strong>，<code>y</code> reshape 成 <code>(m, 1)</code></p><h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p><img title="" src="/images/ml/de3a4ee6-0c36-421d-b365-d688dd1c4542.png" alt="de3a4ee6-0c36-421d-b365-d688dd1c4542" style="zoom:100%;"><img title="" src="/images/ml/94d0039d-c97e-4a61-ad89-375994782858.png" alt="94d0039d-c97e-4a61-ad89-375994782858" style="zoom:100%;"></p><p><img title="" src="/images/ml/6caef8a4-87bd-4b74-ae1c-97b14174b7f9.png" alt="6caef8a4-87bd-4b74-ae1c-97b14174b7f9" style="zoom:100%;"><img title="" src="/images/ml/93d50e4f-e0a3-4658-af4d-49e041d061d2.png" alt="93d50e4f-e0a3-4658-af4d-49e041d061d2" style="zoom:100%;"></p><p>MSE(mean squard error) MAE(mean absolute error) RMSE(root mse) R^2(R-squard)</p><p><code>np.mean((y_predict - y_test) ** 2) #mean自带了求和功能</code></p><p><code>X = np.hstack((np.ones((train_data.shape[0], 1)), train_data))#hstack添加一个偏置项b，每个样本的bias特征值为1</code></p><p><code>np.linalg.inv(X.T.dot(X)).dot(X.T).dot(train_label)</code></p><p><code>numpy.var(array, axis) #var为方差variance，std为标准差</code></p><p><code>numpy.vstack(([1,2,3],[4,5,6])) 行顺序堆叠</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np# MSEdef mse_score(y_predict, y_test):    return np.mean((y_predict - y_test) ** 2)# R2def r2_score(y_predict, y_test):    &#39;&#39;&#39;    input: y_predict (ndarray): 预测值           y_test (ndarray): 真实值    output: r2 (float): r2值    &#39;&#39;&#39;    t1 &#x3D; np.sum((y_predict - y_test) ** 2)    t2 &#x3D; np.sum((y_test - np.mean(y_test)) ** 2)    return 1 - t1 &#x2F; t2class LinearRegression:    def __init__(self):        &#39;&#39;&#39;初始化线性回归模型&#39;&#39;&#39;        self.theta &#x3D; None    def fit_normal(self, train_data, train_label):        &#39;&#39;&#39;        input: train_data (ndarray): 训练样本               train_label (ndarray): 训练标签        &#39;&#39;&#39;        X &#x3D; np.hstack((np.ones((train_data.shape[0], 1)), train_data))        #创建一个列向量，全为1，长度等于样本数        self.theta &#x3D; np.linalg.inv(X.T.dot(X)).dot(X.T).dot(train_label)        return self.theta    def predict(self, test_data):        &#39;&#39;&#39;        input: test_data (ndarray): 测试样本        &#39;&#39;&#39;        test_data &#x3D; np.hstack((np.ones((test_data.shape[0], 1)), test_data))        return test_data.dot(self.theta)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="逻辑回归Logistic-Regression"><a href="#逻辑回归Logistic-Regression" class="headerlink" title="逻辑回归Logistic Regression"></a>逻辑回归Logistic Regression</h3><p>逻辑回归是通过回归的思想来解决二分类问题的算法，用一个<strong>S型的函数（Sigmoid函数）</strong>，把线性回归的结果压缩到 [0,1] 区间，解释为“属于某个类别的概率”。它内部还是在做<strong>线性回归</strong> wTx+b，只是最后加了一个非线性变换（Sigmoid），用来做<strong>分类概率输出</strong>。可以使用np.exp(-z)</p><p><img title="" src="/images/ml/c08e2d31-b7a3-464d-9912-a0008883dcf1.png" alt="c08e2d31-b7a3-464d-9912-a0008883dcf1" style="zoom:100%;"></p><p>交叉熵损失yi代表真实类别，log(yi_hat)代表预测类别</p><p><img title="" src="/images/ml/22893227-1066-4b56-b28f-bf929f91158e.png" alt="22893227-1066-4b56-b28f-bf929f91158e" style="zoom:100%;"><img title="" src="/images/ml/ec35d332-8f41-40cc-b8b6-cc53fecdc323.png" alt="ec35d332-8f41-40cc-b8b6-cc53fecdc323" style="zoom:100%;"></p><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>沿着“当前点的负梯度方向”走一小步，就能慢慢靠近最小值，在多维空间中，<strong>梯度向量</strong>是函数增长最快的方向。</p><p>大学习率收敛快，迅速接近最优解，但是容易震荡无法收敛，小学习率稳定收敛到精确解（可能局部），但是慢</p><div class="table-container"><table><thead><tr><th><strong>问题</strong></th><th><strong>过拟合</strong></th><th><strong>欠拟合</strong></th></tr></thead><tbody><tr><td><strong>表现</strong></td><td>训练误差低，测试误差高</td><td>训练误差和测试误差都高，模型过于简单或不充分学习数据特征</td></tr><tr><td><strong>学习率过大</strong></td><td>可能导致模型无法稳定收敛，训练过程震荡，导致最终过拟合训练数据</td><td>过大的学习率可能跳过局部最优，导致无法捕捉到数据中的复杂模式，出现欠拟合</td></tr><tr><td><strong>学习率过小</strong></td><td>可能导致训练过程过于缓慢，使得模型误学习训练数据中的噪声，过拟合</td><td>过小的学习率可能导致参数更新不足，学习不到数据的复杂模式，造成欠拟合</td></tr><tr><td><strong>解决方法</strong></td><td>增加正则化，减少模型复杂度，更多数据，适当调整学习率</td><td>增加模型复杂度，训练更多轮次，使用合适的学习率</td></tr></tbody></table></div><pre class="line-numbers language-python" data-language="python"><code class="language-python"># -*- coding: utf-8 -*-import numpy as npimport warningswarnings.filterwarnings(&quot;ignore&quot;)def sigmoid(x):    &#39;&#39;&#39;    sigmoid函数    :param x: 转换前的输入    :return: 转换后的概率    &#39;&#39;&#39;    return 1&#x2F;(1+np.exp(-x))def fit(x,y,eta&#x3D;1e-3,n_iters&#x3D;10000):    &#39;&#39;&#39;    训练逻辑回归模型    :param x: 训练集特征数据，类型为ndarray    :param y: 训练集标签，类型为ndarray    :param eta: 学习率，类型为float    :param n_iters: 训练轮数，类型为int    :return: 模型参数，类型为ndarray    &#39;&#39;&#39;    #   请在此添加实现代码   #    #********** Begin *********#    m,n&#x3D;x.shape    theta&#x3D;np.zeros(n)    for i in range(n_iters):        yy&#x3D;x.dot(theta)        z&#x3D;sigmoid(yy)        grad&#x3D;x.T.dot(z-y)        theta-&#x3D;eta*grad    return theta    #********** End **********#from sklearn.linear_model import LogisticRegressiondef digit_predict(train_image, train_label, test_image):    &#39;&#39;&#39;    实现功能：训练模型并输出预测结果    :param train_sample: 包含多条训练样本的样本集，类型为ndarray,shape为[-1, 8, 8]    :param train_label: 包含多条训练样本标签的标签集，类型为ndarray    :param test_sample: 包含多条测试样本的测试集，类型为ndarry    :return: test_sample对应的预测标签    &#39;&#39;&#39;    #************* Begin ************#    train&#x3D;train_image.reshape(len(train_image),-1)#-1 表示自动计算维度，将除样本数量以外的维度全部拉平成一维。    test&#x3D;test_image.reshape(len(test_image),-1)    model&#x3D;LogisticRegression(max_iter&#x3D;1000,C&#x3D;12)# C：正则化系数的倒数，默认为 1.0 ，越小代表正则化越强；    model.fit(train,train_label)    result&#x3D;model.predict(test)    return result    #************* End **************#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>它是一个<strong>二分类线性模型</strong>，核心思想是：找到一个超平面，将两类数据线性分开。</p><ul><li><p><strong>适用于线性可分数据集</strong>：即存在一个超平面能将两类样本完全分开。</p></li><li><p><strong>不能处理非线性问题</strong>：如异或问题（XOR）感知机无法解决。</p></li></ul><p><img title="" src="/images/ml/f572fd80-7786-4793-85d6-0f4738df2710.png" alt="f572fd80-7786-4793-85d6-0f4738df2710" style="zoom:100%;"><img title="" src="/images/ml/42e50b31-90f0-4b65-a426-5b8b2a1fa845.png" alt="42e50b31-90f0-4b65-a426-5b8b2a1fa845" style="zoom:100%;"></p><p><code>predict = np.where(output &gt; 0, 1, -1)</code></p><p><code>predict=np.sign(output)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8import numpy as np#构建感知机算法class Perceptron(object):    def __init__(self, learning_rate &#x3D; 0.01, max_iter &#x3D; 200):        self.lr&#x3D;learning_rate        self.max_iter&#x3D;max_iter    def fit(self, data, label):        &#39;&#39;&#39;        input:data(ndarray):训练数据特征              label(ndarray):训练数据标签        output:w(ndarray):训练好的权重               b(ndarry):训练好的偏置        &#39;&#39;&#39;        #编写感知机训练方法，w为权重，b为偏置        #********* Begin *********#        self.w&#x3D;np.random.randn(data.shape[1])        self.b&#x3D;0        for i in range(self.max_iter):            for j in range(len(data)):                output&#x3D;self.w.dot(data[j].T)+self.b                predict&#x3D;1 if output&gt;0 else -1                if(label[j]*output&lt;&#x3D;0):                    self.w+&#x3D;self.lr*label[j]*data[j]                    self.b+&#x3D;self.lr*label[j]        #********* End *********#    def predict(self, data):        &#39;&#39;&#39;        input:data(ndarray):测试数据特征        output:predict(ndarray):预测标签        &#39;&#39;&#39;        #********* Begin *********#        output &#x3D; np.dot(data, self.w) + self.b        predict &#x3D; np.where(output &gt; 0, 1, -1)        predict&#x3D;np.sign(output)        #********* End *********#        return predict#encoding&#x3D;utf8import osimport pandas as pdfrom sklearn.linear_model.perceptron import Perceptronif os.path.exists(&#39;.&#x2F;step2&#x2F;result.csv&#39;):    os.remove(&#39;.&#x2F;step2&#x2F;result.csv&#39;)#********* Begin *********#train_data&#x3D;pd.read_csv(&#39;.&#x2F;step2&#x2F;train_data.csv&#39;)train_label&#x3D;pd.read_csv(&#39;.&#x2F;step2&#x2F;train_label.csv&#39;)train_label&#x3D;train_label[&#39;target&#39;]test_data&#x3D;pd.read_csv(&#39;.&#x2F;step2&#x2F;test_data.csv&#39;)model&#x3D;Perceptron(eta0&#x3D;0.1,max_iter&#x3D;500)#eta0：学习率大小，默认为 1.0 ；#max_iter：最大训练轮数。model.fit(train_data,train_label)prediction&#x3D;model.predict(test_data)res&#x3D;pd.DataFrame(prediction,columns&#x3D;[&#39;result&#39;])res.to_csv(&#39;.&#x2F;step2&#x2F;result.csv&#39;,index&#x3D;False)#********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="LDA（线性判别分析）"><a href="#LDA（线性判别分析）" class="headerlink" title="LDA（线性判别分析）"></a>LDA（线性判别分析）</h3><p>LDA 是一种 <strong>有监督降维</strong> 技术，其目标是将数据投影到一个低维空间中，使得不同类别之间尽可能可分。目标是最大化类间散度（类之间的距离），最小化类内散度（每类自己的分散程度）。</p><p><img title="" src="/images/ml/4d717c12-bca5-47eb-ab20-f99094171823.png" alt="4d717c12-bca5-47eb-ab20-f99094171823" style="zoom:100%;"><img title="" src="/images/ml/b53022c6-9481-4c55-95cb-702d108573d5.png" alt="b53022c6-9481-4c55-95cb-702d108573d5" style="zoom:100%;"></p><p><img title="" src="/images/ml/4d5a5350-0b9e-4559-b764-ce155badb894.png" alt="4d5a5350-0b9e-4559-b764-ce155badb894" style="zoom:100%;"><img title="" src="/images/ml/3e366804-e71c-43ad-8782-1b6cc0153e69.png" alt="3e366804-e71c-43ad-8782-1b6cc0153e69" style="zoom:100%;"><img title="" src="/images/ml/470f2a07-a3aa-4edf-8c7c-8dc4c1a54a8d.png" alt="470f2a07-a3aa-4edf-8c7c-8dc4c1a54a8d" style="zoom:100%;"><img title="" src="/images/ml/bfa7df51-524a-441f-96ef-9e0129454a26.png" alt="bfa7df51-524a-441f-96ef-9e0129454a26" style="zoom:100%;"></p><p><code>x0m = np.mean(x0, axis=0)  # 类别 0 的均值向量,求样本的每个特征均值要加axis=0</code></p><p><code>sigma0 = np.cov(x0, rowvar=False)#默认 np.cov 会按行计算特征（即样本是列），但我们通常每一行是一个样本，所以需要加</code>.T<code>或使用rowvar=False</code></p><p><code>mean_diff = (x0m - x1m).reshape(-1, 1)  # 列向量化, shape=(n,) 变成 shape=(n, 1)，列向量</code></p><p><code>return np.linalg.inv(sw).dot(x0m - x1m)</code></p><p><code>x_new = model.fit_transform(x, y)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef lda(X, y):    &#39;&#39;&#39;    线性判别分析（Linear Discriminant Analysis）    参数:        X(ndarray): 样本特征矩阵，形状为 (n_samples, n_features)        y(ndarray): 样本标签向量，取值为 0 或 1（仅支持二分类）    返回:        w(ndarray): 最优投影方向向量，用于将高维数据投影到一维，实现类别分离    &#39;&#39;&#39;    # 将数据按类别分开（这里是二分类，分成类 0 和类 1）    x0 &#x3D; X[y &#x3D;&#x3D; 0]  # 类别 0 的所有样本    x1 &#x3D; X[y &#x3D;&#x3D; 1]  # 类别 1 的所有样本    # 分别计算每一类的均值向量（每个特征的均值）    x0m &#x3D; np.mean(x0, axis&#x3D;0)  # 类别 0 的均值向量    x1m &#x3D; np.mean(x1, axis&#x3D;0)  # 类别 1 的均值向量    # 分别计算每一类的类内协方差矩阵（每一类内部的样本方差+协方差）    # rowvar&#x3D;False 表示按列计算协方差（列是特征，行是样本）    sigma0 &#x3D; np.cov(x0, rowvar&#x3D;False)    sigma1 &#x3D; np.cov(x1, rowvar&#x3D;False)    # 总类内散度矩阵 Sw（两个类别的协方差矩阵之和）    sw &#x3D; sigma0 + sigma1    # 类间散度矩阵 Sb &#x3D; (μ1 - μ0)(μ1 - μ0)^T，衡量类别中心的分离程度    mean_diff &#x3D; (x0m - x1m).reshape(-1, 1)  # 列向量化    sb &#x3D; mean_diff @ mean_diff.T  # 外积构成类间散度矩阵    # 计算广义特征值问题 Sw^-1 * Sb 的特征值与特征向量    eigvals, eigvecs &#x3D; np.linalg.eig(np.linalg.inv(sw).dot(sb))    # 将特征值按从大到小排序，找出最大特征值对应的方向    idx &#x3D; np.argsort(eigvals)[::-1]  # 逆序排列索引    w &#x3D; eigvecs[:, idx[0]]  # 最大特征值对应的特征向量（最佳投影方向）    return w  # 返回用于降维的最优方向向量#encoding&#x3D;utf8 from sklearn.discriminant_analysis import LinearDiscriminantAnalysisdef lda(x,y):    &#39;&#39;&#39;    input:x(ndarray):待处理数据          y(ndarray):待处理数据标签    output:x_new(ndarray):降维后数据    &#39;&#39;&#39;    model &#x3D; LinearDiscriminantAnalysis(n_components&#x3D;2)     x_new &#x3D; model.fit_transform(x, y)    return x_new<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="多分类策略"><a href="#多分类策略" class="headerlink" title="多分类策略"></a>多分类策略</h3><h4 id="OvO"><a href="#OvO" class="headerlink" title="OvO"></a>OvO</h4><p>将 K 类的多分类问题拆分为 $\frac{K(K-1)}{2}$ 个二分类问题，每个分类器只区分两个类别。</p><p>对于任意两个类别 $c_i$ 与 $c_j$（$i &lt; j$）：</p><ol><li><p>从数据集中仅选出这两个类别的样本。</p></li><li><p>用二分类模型训练出分类器 $f_{i,j}$，判断样本属于 $c_i$ 还是 $c_j$。</p></li><li><p>最终得到 $\frac{K(K-1)}{2}$ 个分类器。</p></li><li><ul><li><p>对测试样本 $x$，输入所有分类器 $f_{i,j}$。</p></li><li><p>每个分类器对样本进行投票（即判断更像哪个类别）。</p></li><li><p>汇总投票，得票最多的类别为最终预测类别。</p></li></ul></li></ol><p><code>self.classes_ = np.unique(y)</code></p><p><code>self.models[(cls1, cls2)] = model</code> map</p><p><code>votes = [defaultdict(int) for _ in range(len(X))]</code></p><p><code>for (cls1, cls2), model in self.models.items()</code></p><p><code>`enumerate(preds)</code> 的作用是：<strong>在遍历预测结果 <code>preds</code> 的同时，提供每个预测结果的索引</strong>。它返回的是 <code>(索引, 值)</code> 对。`</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">from collections import defaultdictfrom sklearn.linear_model import LogisticRegressionclass OvOClassifier:    def __init__(self, base_model&#x3D;None):        self.base_model &#x3D; base_model if base_model else LogisticRegression()        self.models &#x3D; &#123;&#125;        self.classes_ &#x3D; None    def fit(self, X, y):        self.classes_ &#x3D; np.unique(y)        self.models &#x3D; &#123;&#125;        for i in range(len(self.classes_)):            for j in range(i + 1, len(self.classes_)):                cls1, cls2 &#x3D; self.classes_[i], self.classes_[j]# 找到所有属于 cls1 或 cls2 的样本X_pair &#x3D; []y_binary &#x3D; []for xi, yi in zip(X, y):    if yi &#x3D;&#x3D; cls1 or yi &#x3D;&#x3D; cls2:        X_pair.append(xi)        y_binary.append(1 if yi &#x3D;&#x3D; cls1 else 0)X_pair &#x3D; np.array(X_pair)y_binary &#x3D; np.array(                model &#x3D; LogisticRegression()                model.fit(X_pair, y_binary)                self.models[(cls1, cls2)] &#x3D; model#是一个 字典，key 是一个类别对的 元组，value 是对应的 训练好的模型对象    def predict(self, X):        votes &#x3D; [defaultdict(int) for _ in range(len(X))]#创建了一个列表 votes，这个列表有 len(X) 个元素，每个元素是一个 defaultdict(int)        for (cls1, cls2), model in self.models.items():            preds &#x3D; model.predict(X)            for i, pred in enumerate(preds):                voted_class &#x3D; cls1 if pred &#x3D;&#x3D; 1 else cls2                votes[i][voted_class] +&#x3D; 1        final_preds &#x3D; []        final_preds &#x3D; [max(vote, key&#x3D;vote.get) for vote in votes]#vote 是一个 dict（通常是像 &#123;&#39;A&#39;: 2, &#39;B&#39;: 3&#125; 这样的结构），键是类别，值是票数；#vote.get 用作 key 函数，表示对每个键使用 vote.get(键) 作为排序依据；#max(...) 找出拥有最多票数的类别。        return np.array(final_preds)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="OvR"><a href="#OvR" class="headerlink" title="OvR"></a>OvR</h4><p>将一个 K 类的多分类问题拆解为 K 个二分类问题。每个分类器的任务是识别某一个类别（“正类”）与其余所有类别（“负类”）之间的区别。</p><p>假设标签集合为 $C = {c_1, c_2, …, c_K}$</p><p>对于每一个类别 $c_i$：</p><ol><li><p>创建一个新的二分类训练集：</p><ul><li><p>样本属于 $c_i$ 的，标签为 1（正类）</p></li><li><p>样本不属于 $c_i$ 的，标签为 0（负类）</p></li></ul></li><li><p>用一个二分类器（如 Logistic Regression、SVM）训练该数据子集。</p></li><li><p>共训练 K 个分类器，分别记为 $f_1, f_2, …, f_K$。</p></li><li><ul><li><p>对于一个测试样本 $x$，将其输入所有分类器 $f_1(x), …, f_K(x)$。</p></li><li><p>每个分类器输出一个置信度/得分。</p></li><li><p>选择得分最高的那个分类器所代表的类别作为预测结果。</p></li></ul></li></ol><div class="table-container"><table><thead><tr><th>特性</th><th>OvR (One-vs-Rest)</th><th>OvO (One-vs-One)</th></tr></thead><tbody><tr><td>分类器数量</td><td>K</td><td>K(K-1)/2</td></tr><tr><td>每个分类器训练样本</td><td>全部样本（正类 vs 其他）</td><td>两类样本</td></tr><tr><td>预测机制</td><td>最大置信度</td><td>多数投票</td></tr><tr><td>实现复杂度</td><td>较低</td><td>较高</td></tr><tr><td>优点</td><td>简洁、高效</td><td>更适合类别数少、边界复杂的任务</td></tr><tr><td>缺点</td><td>易受类别不均衡影响</td><td>分类器数量多、计算成本高</td></tr></tbody></table></div><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np# 逻辑回归类class tiny_logistic_regression(object):    def __init__(self):        # W 是系数，b 是截距        self.coef_ &#x3D; None        self.intercept_ &#x3D; None        self._theta &#x3D; None  # 存储所有的 W 和 b        self.label_map &#x3D; &#123;&#125;  # 0&#x2F;1 到标签的映射    def _sigmoid(self, x):        return 1. &#x2F; (1. + np.exp(-x))  # Sigmoid 激活函数    # 训练模型    def fit(self, train_datas, train_labels, learning_rate&#x3D;1e-4, n_iters&#x3D;1e3):        # 计算损失函数        def J(theta, X_b, y):            y_hat &#x3D; self._sigmoid(X_b.dot(theta))            try:                return -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) &#x2F; len(y)            except:                return float(&#39;inf&#39;)  # 避免计算出错        # 损失函数对 theta 的梯度        def dJ(theta, X_b, y):            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) &#x2F; len(y)        # 批量梯度下降        def gradient_descent(X_b, y, initial_theta, learning_rate, n_iters&#x3D;1e2, epsilon&#x3D;1e-6):            theta &#x3D; initial_theta            cur_iter &#x3D; 0            while cur_iter &lt; n_iters:                gradient &#x3D; dJ(theta, X_b, y)                last_theta &#x3D; theta                theta &#x3D; theta - learning_rate * gradient  # 更新 theta                # 判断是否满足收敛条件                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:                    break                cur_iter +&#x3D; 1            return theta        # 将训练数据加上截距项（x0 &#x3D; 1）        X_b &#x3D; np.hstack([np.ones((len(train_datas), 1)), train_datas])        initial_theta &#x3D; np.zeros(X_b.shape[1])  # 初始化theta为零        self._theta &#x3D; gradient_descent(X_b, train_labels, initial_theta, learning_rate, n_iters)        self.intercept_ &#x3D; self._theta[0]        self.coef_ &#x3D; self._theta[1:]        return self    # 预测样本属于正类的概率    def predict_proba(self, X):        X_b &#x3D; np.hstack([np.ones((len(X), 1)), X])        return self._sigmoid(X_b.dot(self._theta))    # 预测：如果属于正类的概率 &gt;&#x3D; 0.5，则预测为1，否则为0    def predict(self, X):        proba &#x3D; self.predict_proba(X)        return np.array(proba &gt;&#x3D; 0.5, dtype&#x3D;&#39;int&#39;)# One-vs-Rest (OvR) 分类器class OvR(object):    def __init__(self):        self.models &#x3D; []  # 用于保存每个类别的二分类器        self.real_label &#x3D; []  # 用于保存正类标签    def fit(self, train_datas, train_labels):        &#39;&#39;&#39;        OvR的训练阶段，将模型保存到self.models中        :param train_datas: 训练集数据，类型为ndarray        :param train_labels: 训练集标签，类型为ndarray，shape为(-1,)        :return: None        &#39;&#39;&#39;        self.classes &#x3D; np.unique(train_labels)  # 获取所有类别标签        for cls in self.classes:            # 将当前类别作为正类，其他类别作为负类            y_binary &#x3D; (train_labels &#x3D;&#x3D; cls).astype(int)            model &#x3D; tiny_logistic_regression()            model.fit(train_datas, y_binary)  # 使用二分类器训练模型            self.models.append(model)  # 将训练好的模型加入模型列表            self.real_label.append(cls)  # 保存当前类别的标签    def predict(self, test_datas):        &#39;&#39;&#39;        OvR的预测阶段        :param test_datas: 测试集数据，类型为ndarray        :return: 预测结果，类型为ndarray        &#39;&#39;&#39;        # 存储每个样本对每个类别的概率        all_probs &#x3D; np.zeros((len(test_datas), len(self.classes)))        for i, model in enumerate(self.models):            # 获取每个模型对每个测试样本属于该类别的概率            probs &#x3D; model.predict_proba(test_datas)            all_probs[:, i] &#x3D; probs  # 存储该类别的概率        # 每个样本选择概率最大的类别        predicted_class_indices &#x3D; np.argmax(all_probs, axis&#x3D;1)        return self.real_label[predicted_class_indices]  # 返回每个样本的最终预测类别<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>np.log2,Counter,len,np.unique</p><p>for label,count in labelcount.items():</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npfrom collections import Counterdef calcInfoGain(feature, label, index):    &#39;&#39;&#39;    计算信息增益    :param feature: 测试用例中的特征矩阵，类型为ndarray    :param label: 测试用例中的标签，类型为ndarray    :param index: 特征列的索引，指示使用第几个特征来计算信息增益。    :return: 信息增益，类型float    &#39;&#39;&#39;    # 计算原始熵    all_count &#x3D; len(label)    # 获取标签的频率    label_count &#x3D; Counter(label)    # 计算标签的熵    all_entropy &#x3D; 0    for label_value, count in label_count.items():        p &#x3D; count &#x2F; all_count        all_entropy -&#x3D; p * np.log2(p)    # 计算按特征划分后的熵    feature_values &#x3D; np.unique(feature[:, index])  # 获取该特征的所有可能值# feature[:, index] 表示选择 feature 矩阵的所有行（: 表示所有行），但是只选择第 index 列（即该列所有样本的特征值）。    weighted_entropy &#x3D; 0    for value in feature_values:        # 根据特征值划分数据        subset_label &#x3D; label[feature[:, index] &#x3D;&#x3D; value]        subset_count &#x3D; len(subset_label)        # 计算子集的熵        subset_entropy &#x3D; 0        subset_label_count &#x3D; Counter(subset_label)        for subset_label_value, count in subset_label_count.items():            p &#x3D; count &#x2F; subset_count            subset_entropy -&#x3D; p * np.log2(p)        # 加权平均子集的熵        weighted_entropy +&#x3D; (subset_count &#x2F; all_count) * subset_entropy    # 信息增益 &#x3D; 总熵 - 加权熵    info_gain &#x3D; all_entropy - weighted_entropy    return info_gain return info_gain<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">def calcInfoGainRatio(feature, label, index):    #********* Begin *********#    all_count&#x3D;len(label)    #D    needE&#x3D;0    all_features &#x3D; np.unique(feature[:, index])  # 获取该特征列的所有唯一值    for f in all_features:        count&#x3D;0        for j in range(len(label)):            if feature[j][index]&#x3D;&#x3D;f:                count+&#x3D;1        #D1        needE-&#x3D;(count&#x2F;all_count)*np.log2(count&#x2F;all_count)    gain&#x3D;calcInfoGain(feature,label,index)    return gain&#x2F;needE    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Gini系数"><a href="#Gini系数" class="headerlink" title="Gini系数"></a>Gini系数</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">def calcGini(feature, label, index):    &#39;&#39;&#39;    &#39;&#39;&#39;    #********* Begin *********#    #先计算特征个数，在用特征里面的小个数计算    allcount&#x3D;len(label)    feats&#x3D;np.unique(feature[:,index])    Gini&#x3D;0    for val in feats:        sub_count&#x3D;0        sublabels&#x3D;[]        subG&#x3D;1        for j in range(allcount):            if feature[j][index]&#x3D;&#x3D;val:                sub_count+&#x3D;1                sublabels.append(label[j])        label_count&#x3D;Counter(sublabels)        for la,co in label_count.items():            subG-&#x3D;(co&#x2F;sub_count)**2        Gini+&#x3D;subG*(sub_count&#x2F;allcount)    return Gini    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>鸢尾花识别</p><p>pd.read_csv,to_csv,DataFrame,</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npfrom sklearn.tree import DecisionTreeClassifierimport pandas as pd#********* Begin *********#model&#x3D;DecisionTreeClassifier()train_label &#x3D; pd.read_csv(&#39;.&#x2F;step7&#x2F;train_label.csv&#39;).valuestrain_data &#x3D; pd.read_csv(&#39;.&#x2F;step7&#x2F;train_data.csv&#39;).valuestest_data &#x3D; pd.read_csv(&#39;.&#x2F;step7&#x2F;test_data.csv&#39;).valuesmodel.fit(train_data,train_label)result&#x3D;model.predict(test_data)resultd&#x3D;pd.DataFrame(result,columns&#x3D;[&#39;prediction&#39;])resultd.to_csv(&#39;.&#x2F;step7&#x2F;predict.csv&#39;,index&#x3D;False)#********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h2><p>全概率公式</p><p><img src="/images/ml/9bbceb6a-0ae0-449f-9af5-cb33b8ef2ffd.png" title="" alt="9bbceb6a-0ae0-449f-9af5-cb33b8ef2ffd" style="zoom:100%;"></p><p>贝叶斯公式（条件概率乘法定理/全概率公式）</p><p><img src="/images/ml/cb2d0212-cba3-430c-8d92-d2eb0ca1ffe9.png" title="" alt="cb2d0212-cba3-430c-8d92-d2eb0ca1ffe9" style="zoom:100%;"><img src="/images/ml/b772390c-44da-4753-ba81-a7004d95f0a1.png" title="" alt="b772390c-44da-4753-ba81-a7004d95f0a1" style="zoom:100%;"></p><h3 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h3><p><code>label_val, label_count = np.unique(label, return_counts=True)</code></p><p><code>self.label_prob = &#123;label_val[i]: label_count[i] / all_count for i in range(len(label_val))&#125;</code></p><p><code>feat_lines = feature[label == val]</code></p><p><code>for a, b in zip(feat_val, feat_count):</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npclass NaiveBayesClassifier(object):    def __init__(self):        self.label_prob &#x3D; &#123;&#125;  # 每种类别在数据中出现的概率        self.condition_prob &#x3D; &#123;&#125;  # 条件概率 P(x|y)    def fit(self, feature, label):        &#39;&#39;&#39;        对模型进行训练，计算各种概率        :param feature: 训练数据集的特征，ndarray        :param label: 训练数据集的标签，ndarray        :return: 无返回        &#39;&#39;&#39;        # 计算每个标签的概率 P(y)        label_val, label_count &#x3D; np.unique(label, return_counts&#x3D;True)        all_count &#x3D; len(label)        self.label_prob &#x3D; &#123;label_val[i]: label_count[i] &#x2F; all_count for i in range(len(label_val))&#125;        # 计算每个标签下特征的条件概率 P(x|y)        feat_num &#x3D; feature.shape[1]  # 特征数量        self.condition_prob &#x3D; &#123;&#125;        for val in label_val:            feat_lines &#x3D; feature[label &#x3D;&#x3D; val]  # 当前标签下的特征数据            feature_prob &#x3D; &#123;&#125;            for i in range(feat_num):                feat_val, feat_count &#x3D; np.unique(feat_lines[:, i], return_counts&#x3D;True)  # 当前特征列的取值及其频数                feature_prob[i] &#x3D; &#123;&#125;                sub_count &#x3D; len(feat_lines)                for a, b in zip(feat_val, feat_count):                    feature_prob[i][a] &#x3D; b &#x2F; sub_count  # 计算条件概率            self.condition_prob[val] &#x3D; feature_prob  # 保存当前标签的条件概率    def predict(self, feature):        &#39;&#39;&#39;        对数据进行预测，返回预测结果        :param feature: 测试数据集所有特征组成的ndarray        :return: 预测的标签        &#39;&#39;&#39;        predictions &#x3D; []        for sample in feature:            label_post &#x3D; &#123;&#125;            # 计算每个标签的后验概率 P(y|x)            for label_val, prob in self.label_prob.items():                probability &#x3D; np.log2(prob)  # P(y)                # 计算当前标签下的条件概率 P(x|y)                for idx, val in enumerate(sample):                    probability +&#x3D; np.log2(self.condition_prob[label_val][idx].get(val, 1e-10))                label_post[label_val] &#x3D; probability            # 选择后验概率最大的标签作为预测结果            predict &#x3D; max(label_post, key&#x3D;label_post.get)            predictions.append(predict)        return np.array(predictions)  # 返回 numpy 数组<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Laplace平滑"><a href="#Laplace平滑" class="headerlink" title="Laplace平滑"></a>Laplace平滑</h3><p>假设N表示训练数据集总共有多少种类别，Ni表示训练数据集中第i列总共有多少种取值。则训练过程中在算类别的概率时分子加1，分母加N，算条件概率时分子加1，分母加Ni</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npclass NaiveBayesClassifier(object):    def __init__(self):        &#39;&#39;&#39;        &#39;&#39;&#39;        self.label_prob &#x3D; &#123;&#125;  # 存储每个类别的先验概率        self.condition_prob &#x3D; &#123;&#125;  # 存储每个类别下每个特征的条件概率    def fit(self, feature, label):        &#39;&#39;&#39;        对模型进行训练，计算每种类别的先验概率和条件概率        :param feature: 训练数据集所有特征组成的 ndarray        :param label: 训练数据集所有标签组成的 ndarray        :return: 无返回值，通过更新 self.label_prob 和 self.condition_prob 来保存训练结果        &#39;&#39;&#39;        #********* Begin *********#        row_num &#x3D; len(feature)  # 样本的数量        col_num &#x3D; len(feature[0])  # 特征的数量        unique_label_count &#x3D; len(set(label))  # 唯一类别的数量        # 计算每个类别的出现次数        for c in label:            if c in self.label_prob:                self.label_prob[c] +&#x3D; 1            else:                self.label_prob[c] &#x3D; 1        # 计算每个类别的先验概率，并进行拉普拉斯平滑        for key in self.label_prob.keys():            self.label_prob[key] +&#x3D; 1  # 拉普拉斯平滑            self.label_prob[key] &#x2F;&#x3D; (unique_label_count + row_num)  # 计算先验概率            # 构建条件概率字典，初始化每个类别下每个特征值的概率            self.condition_prob[key] &#x3D; &#123;&#125;            for i in range(col_num):                self.condition_prob[key][i] &#x3D; &#123;&#125;                for k in np.unique(feature[:, i], axis&#x3D;0):  # 遍历每个特征的不同取值                    self.condition_prob[key][i][k] &#x3D; 1  # 初始概率设为 1（拉普拉斯平滑）        # 统计训练集中每个类别下特征值的频次        for i in range(len(feature)):            for j in range(len(feature[i])):                # 更新每个特征值的出现频次                self.condition_prob[label[i]][j][feature[i][j]] +&#x3D; 1        # 计算每个类别下每个特征值的条件概率，进行拉普拉斯平滑        for label_key in self.condition_prob.keys():            for k in self.condition_prob[label_key].keys():                total &#x3D; len(self.condition_prob[label_key][k].keys())  # 特征的种类数                for v in self.condition_prob[label_key][k].values():                    total +&#x3D; v  # 总的计数值（包括平滑项）                for kk in self.condition_prob[label_key][k].keys():                    # 计算条件概率                    self.condition_prob[label_key][k][kk] &#x2F;&#x3D; total        #********* End *********#    def predict(self, feature):        &#39;&#39;&#39;        对数据进行预测，返回预测结果        :param feature: 测试数据集所有特征组成的 ndarray        :return: 预测结果的数组，包含每个测试样本的预测类别        &#39;&#39;&#39;        result &#x3D; []  # 存储预测结果        # 对每条测试数据进行预测        for i, f in enumerate(feature):            prob &#x3D; np.zeros(len(self.label_prob.keys()))  # 存储每个类别的概率，初始化为零            ii &#x3D; 0  # 用于索引类别            for label, label_prob in self.label_prob.items():                prob[ii] &#x3D; label_prob  # 初始化为先验概率                for j in range(len(f)):  # 遍历当前样本的所有特征                    # 计算该特征值在该类别下的条件概率                    prob[ii] *&#x3D; self.condition_prob[label][j][f[j]]                ii +&#x3D; 1  # 处理下一个类别            # 选择具有最大概率的类别作为预测结果            result.append(list(self.label_prob.keys())[np.argmax(prob)])        return np.array(result)  # 返回预测结果<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="新闻文本分类"><a href="#新闻文本分类" class="headerlink" title="新闻文本分类"></a>新闻文本分类</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.feature_extraction.text import CountVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import TfidfTransformerimport numpy as npdef news_predict(train_sample, train_label, test_sample):    &#39;&#39;&#39;    训练模型并进行预测，返回预测结果    :param train_sample: 原始训练集中的新闻文本，类型为 ndarray    :param train_label: 训练集中新闻文本对应的主题标签，类型为 ndarray    :param test_sample: 原始测试集中的新闻文本，类型为 ndarray    :return: 预测结果，类型为 ndarray    &#39;&#39;&#39;    #********* Begin *********#    # 实例化CountVectorizer，转换文本为词频矩阵    vec &#x3D; CountVectorizer()    X_train &#x3D; vec.fit_transform(train_sample)  # 先fit拟合训练集，再对训练集进行词频向量化    X_test &#x3D; vec.transform(test_sample)  # 对测试集进行词频向量化    # 实例化TfidfTransformer，转换为TF-IDF矩阵    tfidf &#x3D; TfidfTransformer()    X_train &#x3D; tfidf.fit_transform(X_train)  # 将训练集词频矩阵转换为TF-IDF矩阵    X_test &#x3D; tfidf.transform(X_test)  # 将测试集词频矩阵转换为TF-IDF矩阵    # 实例化MultinomialNB模型，设置平滑参数alpha&#x3D;0.8    model &#x3D; MultinomialNB(alpha&#x3D;0.8)    model.fit(X_train, train_label)  # 使用训练集训练模型    # 预测测试集的标签    result &#x3D; model.predict(X_test)    # 返回预测结果    return np.array(result)    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h3><p><code>计算训练数据的均值和标准差    mean = np.mean(train_data, axis=0)    std = np.std(train_data,axis=0)</code></p><p><code>train_data = (train_data - mean) / std</code></p><p><code>model = LinearSVC(C=10, max_iter=10000)  # 增大C的值并增加最大迭代次数</code></p><p><strong><code>C</code>（正则化参数）</strong>: 控制模型的复杂度，较大的 <code>C</code> 值会让模型更加“严格”地拟合训练数据，但可能导致过拟合。较小的 <code>C</code> 值则允许更多的错误，从而使得模型更为平滑，防止过拟合</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8from sklearn.svm import LinearSVCimport pandas as pdimport numpy as npdef linearsvc_predict(train_data,train_label,test_data):    &#39;&#39;&#39;    input:train_data(ndarray):训练数据          train_label(ndarray):训练标签    output:predict(ndarray):测试集预测标签    &#39;&#39;&#39;    # ********* Begin *********#    # 1. 数据标准化（手动进行标准化：均值0，方差1）    # 计算训练数据的均值和标准差    mean &#x3D; np.mean(train_data, axis&#x3D;0)    std &#x3D; np.std(train_data, axis&#x3D;0)    # 对训练数据和测试数据进行标准化    train_data &#x3D; (train_data - mean) &#x2F; std    test_data &#x3D; (test_data - mean) &#x2F; std    # 2. 创建并训练模型    model &#x3D; LinearSVC(C&#x3D;10, max_iter&#x3D;10000)  # 增大C的值并增加最大迭代次数    model.fit(train_data, train_label)    # 3. 预测测试数据    predict &#x3D; model.predict(test_data)    # ********* End *********#     return predict<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a>非线性SVM</h3><p><img title="" src="/images/ml/526644dc-2c68-464b-9590-50f1c27f5fcc.png" alt="526644dc-2c68-464b-9590-50f1c27f5fcc" style="zoom:100%;"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8from sklearn.svm import SVCdef svc_predict(train_data,train_label,test_data,kernel):    &#39;&#39;&#39;    input:train_data(ndarray):训练数据          train_label(ndarray):训练标签          kernel(str):使用核函数类型:              &#39;linear&#39;:线性核函数              &#39;poly&#39;:多项式核函数              &#39;rbf&#39;:径像核函数&#x2F;高斯核    output:predict(ndarray):测试集预测标签    &#39;&#39;&#39;    #********* Begin *********#     model&#x3D;SVC(kernel&#x3D;kernel)    model.fit(train_data,train_label)    predict&#x3D;model.predict(test_data)    #********* End *********#     return predict<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h3><p>提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。</p><p>历史上，<strong>Kearns 和 Valiant 首先提出了强可学习和弱可学习的概念</strong>指出：在 PAC 学习的框架中，一个概念，</p><p>如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；</p><p>一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。</p><p>非常有趣的是 Schapire 后来证明强可学习与弱可学习是等价的，也就是说，在 PAC 学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p><p>这样一来，问题便成为，在<strong>学习中，如果已经发现了弱学习算法，那么能否将它提升为强学习算法。大家知道，发现弱学习算法通常要比发现强学习算法容易得多</strong>。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。</p><p>与 bagging 不同， boosting 采用的是一个串行训练的方法。首先，它训练出一个弱分类器，然后在此基础上，再训练出一个稍好点的弱分类器，以此类推，不断的训练出多个弱分类器，最终再将这些分类器相结合，这就是 boosting 的基本思想</p><p>可以看出，<strong>子模型之间存在强依赖关系，必须串行生成。</strong> boosting 是利用不同模型的相加，构成一个更好的模型，求取模型一般都采用序列化方法，后面的模型依据前面的模型</p><p>Adaboost算法原理<br>对提升方法来说，有两个问题需要回答：<strong>一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器</strong></p><p>关于第 1 个问题，<strong>AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值</strong></p><p>这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。</p><p>至于第 2 个问题，即弱分类器的组合，<strong>AdaBoost采取加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</strong></p><h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>与 Boosting 这种串行集成学习算法不同， Bagging 是并行式集成学习方法。</p><p>如果使用 Bagging 解决分类问题，就是将多个分类器的结果整合起来进行投票，选取票数最高的结果作为最终结果。如果使用 Bagging 解决回归问题，就将多个回归器的结果加起来然后求平均，将平均值作为最终结果。</p><p>n足够大时，考虑用正态分布来拟合二项分布</p><p><img src="/images/ml/02a997d7-32a2-4a6f-a846-782d1ecd53a2.png" title="" alt="02a997d7-32a2-4a6f-a846-782d1ecd53a2" style="zoom:100%;"></p><p> Bagging 在训练时的特点就是随机有放回采样和并行。</p><p><strong>随机有放回采样:</strong> 假设训练数据集有 m 条样本数据，每次从这 m 条数据中随机取一条数据放入采样集，然后将其返回，让下一次采样有机会仍然能被采样。<strong>然后重复 m 次，就能得到拥有 m 条数据的采样集</strong>，该采样集作为 Bagging 的众多分类器中的一个作为训练数据集。假设有 T 个分类器（随便什么分类器），那么就重复 T 此随机有放回采样，<strong>构建出 T 个采样集分别作为 T 个分类器的训练数据集</strong>。</p><p><strong>并行：</strong> 假设有 10 个分类器，在 Boosting 中，1 号分类器训练完成之后才能开始 2 号分类器的训练，而<strong>在 Bagging 中，分类器可以同时进行训练，当所有分类器训练完成之后，整个 Bagging 的训练过程就结束了</strong>。</p><p><code>samples = np.random.choice(n, n, replace=True)</code>从0到n-1选取n个有放回采样，</p><p><code>prediction=[model.predict(feature) for model in self.models]</code></p><p><code>votes=[prediction[j][i] for j in range(self.n_model)]</code></p><p><code>max(set(votes),key=votes.count)</code>以votes.count为排序依据，对set(votes)进行排序并取最大值</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npfrom sklearn.tree import DecisionTreeClassifierclass BaggingClassifier():    def __init__(self, n_model&#x3D;10):        &#39;&#39;&#39;        初始化函数，设置模型参数        :param n_model: 分类器数量，默认为10个决策树        &#39;&#39;&#39;        # 分类器的数量，默认为10        self.n_model &#x3D; n_model        # 用于保存训练后的模型，存储每个决策树        self.models &#x3D; []    def fit(self, feature, label):        # 获取样本数量        n &#x3D; len(label)        # 训练 n_model 个决策树        for i in range(self.n_model):            # 有放回地随机采样样本索引            samples &#x3D; np.random.choice(n, n, replace&#x3D;True)            # 生成样本的特征和标签子集            sample_f &#x3D; feature[samples]            sample_l &#x3D; label[samples]            # 使用决策树模型进行训练，最大深度为3            model &#x3D; DecisionTreeClassifier(max_depth&#x3D;3)            model.fit(sample_f, sample_l)            # 将训练好的模型添加到模型列表中            self.models.append(model)    def predict(self, feature):        &#39;&#39;&#39;        对测试集数据进行预测，采用投票机制        :param feature: 测试集数据，类型为ndarray，形状为 (n_samples, n_features)        :return: 预测结果，类型为ndarray，形状为 (n_samples,)        &#39;&#39;&#39;        # 存储每个模型的预测结果        prediction&#x3D;[]        for model in self.models:            prediction.append(model.predict(feature))        # prediction&#x3D;[model.predict(feature) for model in self.models]        final_prediction&#x3D;[]        for i in range(len(feature)):            votes&#x3D;[prediction[j][i] for j in range(self.n_model)]            l&#x3D;max(set(votes),key&#x3D;votes.count)            final_prediction.append(l)        return np.array(final_prediction)        #************* End **************#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林是 Bagging 的一种扩展变体，随机森林的训练过程相对与 Bagging 的训练过程的改变有：</p><p><strong>基学习器：</strong> Bagging 的基学习器可以是任意学习器，而<strong>随机森林则是以决策树作为基学习器</strong>。<br><strong>随机属性选择：</strong> 假设原始训练数据集有 10 个特征，从这 10 个特征中<strong>随机选取 k 个特征构成训练数据子集</strong>，然后将这个子集作为训练集扔给决策树去训练。其中 k 的取值一般为 <strong>log2(特征数量)</strong> 。<br>这样的改动通常会使得随机森林具有更加强的泛化性，因为每一棵决策树的训练数据集是随机的，而且训练数据集中的特征也是随机抽取的。如果每一棵决策树模型的差异比较大，那么就很容易<strong>能够解决决策树容易过拟合的问题</strong>。</p><p>随机森林的预测流程与 Bagging 的预测流程基本一致，<strong>如果是回归，就将结果基学习器的预测结果全部加起来算平均</strong>；如果是<strong>分类，就投票，票数最多的结果作为最终结果。但需要注意的是，在预测时所用到的特征必须与训练模型时所用到的特征保持一致</strong>。</p><p><code>s = np.random.choice(l, self.feature_nums, replace=False)</code>随机选择不重复的特征</p><p><code>sub_sample = sample_f[:, s]</code>在s列的所有行数据</p><p><code>votes.append(model.predict([sub_f])[0])  # 获取预测结果</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npfrom collections import Counterfrom sklearn.tree import DecisionTreeClassifierclass RandomForestClassifier():    def __init__(self, n_model&#x3D;10):        &#39;&#39;&#39;        初始化函数        &#39;&#39;&#39;        self.n_model &#x3D; n_model        self.feature_nums &#x3D; None        self.models &#x3D; []        self.col_indexs &#x3D; []    def fit(self, feature, label):        &#39;&#39;&#39;        训练模型        :param feature: 训练集数据，类型为ndarray        :param label: 训练集标签，类型为ndarray        :return: None        &#39;&#39;&#39;        n, l &#x3D; feature.shape        if self.feature_nums is None:            self.feature_nums &#x3D; int(np.log2(l))  # 取整以确保feature_nums是整数        for i in range(self.n_model):            # 有放回采样            sample &#x3D; np.random.choice(n, n, replace&#x3D;True)            sample_f &#x3D; feature[sample]            sample_l &#x3D; label[sample]            # 随机选择特征列            s &#x3D; np.random.choice(l, self.feature_nums, replace&#x3D;False)            self.col_indexs.append(s)            sub_sample &#x3D; sample_f[:, s]            model &#x3D; DecisionTreeClassifier(max_depth&#x3D;3)            model.fit(sub_sample, sample_l)            self.models.append(model)    def predict(self, feature):        &#39;&#39;&#39;        :param feature: 测试集数据，类型为ndarray        :return: 预测结果，类型为ndarray        &#39;&#39;&#39;        predictions &#x3D; []           for j in range(feature.shape[0]):  # 遍历每个测试样本            votes &#x3D; []            for i, model in enumerate(self.models):  # 遍历每棵决策树                col &#x3D; self.col_indexs[i]  # 获取模型训练时使用的特征列                sub_f &#x3D; feature[j, col]  # 获取测试样本的特征                votes.append(model.predict([sub_f])[0])  # 获取预测结果            # 投票机制，选择出现次数最多的结果            predictions.append(max(set(votes), key&#x3D;votes.count))        return np.array(predictions)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="层次聚类距离计算："><a href="#层次聚类距离计算：" class="headerlink" title="层次聚类距离计算："></a>层次聚类距离计算：</h3><p><code>dist = np.linalg.norm(i - j) #直接计算i，j的欧氏距离</code></p><p><code>return np.min(dist) #np.argmin(dist)返回索引</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef calc_min_dist(cluster1, cluster2):    &#39;&#39;&#39;    计算簇间最小距离    :param cluster1:簇1中的样本数据，类型为ndarray    :param cluster2:簇2中的样本数据，类型为ndarray    :return:簇1与簇2之间的最小距离    &#39;&#39;&#39;    min_dist &#x3D; float(&#39;inf&#39;)  # 初始化最小距离为正无穷大    for i in cluster1:        for j in cluster2:            dist &#x3D; np.linalg.norm(i - j)  # 计算欧几里得距离            if dist &lt; min_dist:                min_dist &#x3D; dist    return min_distdef calc_max_dist(cluster1, cluster2):    &#39;&#39;&#39;    计算簇间最大距离    :param cluster1:簇1中的样本数据，类型为ndarray    :param cluster2:簇2中的样本数据，类型为ndarray    :return:簇1与簇2之间的最大距离    &#39;&#39;&#39;    max_dist &#x3D; -float(&#39;inf&#39;)  # 初始化最大距离为负无穷大    for i in cluster1:        for j in cluster2:            dist &#x3D; np.linalg.norm(i - j)  # 计算欧几里得距离            if dist &gt; max_dist:                max_dist &#x3D; dist    return max_distdef calc_avg_dist(cluster1, cluster2):    &#39;&#39;&#39;    计算簇间平均距离    :param cluster1:簇1中的样本数据，类型为ndarray    :param cluster2:簇2中的样本数据，类型为ndarray    :return:簇1与簇2之间的平均距离    &#39;&#39;&#39;    total_dist &#x3D; 0    count1 &#x3D; 0    count2 &#x3D; 0    for i in cluster1:        for j in cluster2:            total_dist +&#x3D; np.linalg.norm(i - j)  # 计算欧几里得距离    return total_dist &#x2F; (len(cluster1)*len(cluster2))  # 返回平均距离<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><h4 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h4><p>曼哈顿距离即每个维度距离求和，欧氏距离即空间直线距离</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def distance(x,y,p&#x3D;2):    &#39;&#39;&#39;    input:x(ndarray):第一个样本的坐标          y(ndarray):第二个样本的坐标          p(int):等于1时为曼哈顿距离，等于2时为欧氏距离    output:distance(float):x到y的距离          &#39;&#39;&#39;     if(p&#x3D;&#x3D;1):          return sum(np.abs(x-y))    else:          return np.sqrt(sum((x-y)**2))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="质心计算"><a href="#质心计算" class="headerlink" title="质心计算"></a>质心计算</h4><p><code>dist=np.power(np.sum(np.abs(x-y)**p),1/p)</code></p><p><code>Cmass=np.mean(data,axis=0);</code></p><p><code>dist_list=sorted(dist)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8import numpy as np#计算样本间距离def distance(x, y, p&#x3D;2):    #********* Begin *********#    # 计算x和y之间差的绝对值的p次方      dist&#x3D;np.power(np.sum(np.abs(x-y)**p),1&#x2F;p)    # 计算最终的距离值     #********* End *********#    return dist#计算质心def cal_Cmass(data):    &#39;&#39;&#39;    input:data(ndarray):数据样本    output:mass(ndarray):数据样本质心    &#39;&#39;&#39;    #********* Begin *********#    # 计算数据样本的质心（即每个维度的平均值）      Cmass&#x3D;np.mean(data,axis&#x3D;0)    #********* End *********#    return Cmass#计算每个样本到质心的距离，并按照从小到大的顺序排列def sorted_list(data,Cmass):    &#39;&#39;&#39;    input:data(ndarray):数据样本          Cmass(ndarray):数据样本质心    output:dis_list(list):排好序的样本到质心距离    &#39;&#39;&#39;    #********* Begin *********#    # 初始化一个空列表，用于存储距离      dist&#x3D;[]    # 遍历数据样本中的每个样本     for sample in data:        dis&#x3D;distance(Cmass,sample)        dist.append(dis)    # 计算当前样本到质心的距离，并添加到列表中      # 对距离列表进行排序      dist_list&#x3D;sorted(dist)    return dist_list    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="k-means-1"><a href="#k-means-1" class="headerlink" title="k-means"></a>k-means</h4><p><code>model = KMeans(n_clusters=k, init=&#39;k-means++&#39;, max_iter=500, tol=1e-4, random_state=42)</code></p><p>K-means 是一种 <strong>无监督学习</strong> 方法，它确实没有使用真实标签（ground truth labels），但仍然会为每个样本分配一个 <strong>聚类标签（cluster index）</strong>，这个标签是算法自动生成的，表示样本属于哪个簇（cluster）</p><ol><li><p><strong>初始化聚类中心</strong></p><ul><li>随机选择 <code>k</code> 个样本作为初始聚类中心 <code>centroids</code>。</li></ul></li><li><p><strong>迭代优化</strong>（直到收敛或达到最大迭代次数）：</p><ul><li><p><strong>步骤 1：分配样本到最近的簇</strong></p><ul><li><p>对每个样本 <code>x ∈ X</code>，计算它与所有 <code>centroids</code> 的距离，并分配到最近的簇。</p></li><li><p>代码实现：<code>create_clusters(centroids, X)</code>。</p></li></ul></li><li><p><strong>步骤 2：更新聚类中心</strong></p><ul><li><p>对每个簇，计算其所有样本的均值，作为新的聚类中心。</p></li><li><p>代码实现：<code>update_centroids(clusters)</code>。</p></li></ul></li><li><p><strong>步骤 3：检查收敛</strong></p><ul><li><p>如果新旧聚类中心的距离变化 <code>&lt;= ε</code>（<code>varepsilon</code>），则停止迭代。</p></li><li><p>否则，继续迭代。</p></li></ul></li></ul></li><li><p><strong>返回聚类标签</strong></p><ul><li>最终，为每个样本分配其最近的聚类中心的索引，作为聚类标签 <code>labels</code>。</li></ul></li></ol><p><code>labels = np.zeros(X.shape[0], dtype=int)</code></p><p><code>return np.argmin(distance)</code></p><p><code>clusters=[[] for _ in range(self.k)]</code></p><p><code>new_centroids[i] = np.mean(clusters[i], axis=0)</code></p><p><code>centroid_shift = np.linalg.norm(new_centroids - centroids)</code></p><p><code>#NumPy 提供的一个线性代数函数，用来计算矩阵的“范数”，在这里是计算 向量的欧几里得距离</code></p><p>sample=np.random.choice(len(X),self.k,replace=False)</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8import numpy as np# 计算一个样本与数据集中所有样本的欧氏距离的平方def euclidean_distance(one_sample, X):    return np.sum((X - one_sample) ** 2, axis&#x3D;1)class Kmeans():    &quot;&quot;&quot;Kmeans聚类算法.    Parameters:    -----------    k: int        聚类的数目.    max_iterations: int        最大迭代次数.     varepsilon: float        判断是否收敛, 如果上一次的所有k个聚类中心与本次的所有k个聚类中心的差都小于varepsilon,         则说明算法已经收敛    &quot;&quot;&quot;    def __init__(self, k&#x3D;2, max_iterations&#x3D;500, varepsilon&#x3D;0.0001):        self.k &#x3D; k        self.max_iterations &#x3D; max_iterations        self.varepsilon &#x3D; varepsilon        np.random.seed(1)    #********* Begin *********#    # 从所有样本中随机选取self.k样本作为初始的聚类中心    def init_random_centroids(self, X):        sample&#x3D;np.random.choice(len(X),self.k,replace&#x3D;False)        return X[sample]    # 返回距离该样本最近的一个中心索引[0, self.k)    def _closest_centroid(self, sample, centroids):        distance&#x3D;euclidean_distance(sample,centroids)        return np.argmin(distance)    # 将所有样本进行归类，归类规则就是将该样本归类到与其最近的中心    def create_clusters(self, centroids, X):        clusters&#x3D;[[] for _ in range(self.k)]        for sample in X:            closest_idx&#x3D;self._closest_centroid(sample,centroids)            clusters[closest_idx].append(sample)        return clusters    # 对中心进行更新    def update_centroids(self, clusters, X):        new_centroids &#x3D; np.zeros((self.k, len(clusters[0][0])))        for i in range(self.k):            # 对每个簇内样本求均值            new_centroids[i] &#x3D; np.mean(clusters[i], axis&#x3D;0)        return new_centroids    # 对整个数据集X进行Kmeans聚类，返回其聚类的标签    def predict(self, X):        # 从所有样本中随机选取self.k样本作为初始的聚类中心        centroids&#x3D;self.init_random_centroids(X)        # 迭代，直到算法收敛(上一次的聚类中心和这一次的聚类中心几乎重合)或者达到最大迭代次数           for i in range(self.max_iterations):            clusters&#x3D;self.create_clusters(centroids,X)            new_centroids&#x3D;self.update_centroids(clusters,X)            centroid_shift &#x3D; np.linalg.norm(new_centroids - centroids)            if centroid_shift &lt;&#x3D; self.varepsilon:                break            # 更新聚类中心            centroids &#x3D; new_centroids            # 如果聚类中心几乎没有变化，说明算法已经收敛，退出迭代        labels &#x3D; np.zeros(X.shape[0], dtype&#x3D;int)        for i, sample in enumerate(X):            labels[i] &#x3D; self._closest_centroid(sample, centroids)        return labels<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><ul><li><p><strong>簇（Cluster）</strong> = <strong>高密度区域</strong>（数据点密集）。</p></li><li><p><strong>噪声（Noise）</strong> = <strong>低密度区域</strong>（数据点稀疏）。</p></li><li><p><strong><code>eps</code> (ε)</strong>：邻域半径，用于判断两个点是否“相邻”。</p></li><li><p><strong><code>min_samples</code></strong>：形成一个簇所需的最小样本数（核心点的邻域至少要有这么多点）。</p></li></ul><div class="table-container"><table><thead><tr><th>类型</th><th>定义</th><th>示例</th></tr></thead><tbody><tr><td><strong>核心点（Core Point）</strong></td><td>在 <code>eps</code> 邻域内至少有 <code>min_samples</code> 个点（包括自己）</td><td>若 <code>min_samples=5</code>，某点周围有 ≥5 个点（含自己），则它是核心点</td></tr><tr><td><strong>边界点（Border Point）</strong></td><td>在某个核心点的 <code>eps</code> 邻域内，但自身不满足核心点条件</td><td>周围点 &lt; <code>min_samples</code>，但属于某个核心点的邻域</td></tr><tr><td><strong>噪声点（Noise Point）</strong></td><td>既不是核心点，也不是边界点</td><td>离群点，不属于任何簇</td></tr></tbody></table></div><ul><li><p>如果点 <code>q</code> 在点 <code>p</code> 的 <code>eps</code> 邻域内，且 <code>p</code> 是核心点，则称 <code>q</code> 从 <code>p</code> <strong>密度直达</strong>。</p></li><li><p>如果存在一个核心点序列 <code>p1, p2, ..., pn</code>，使得 <code>pi+1</code> 从 <code>pi</code> 密度直达，则称 <code>p1</code> 和 <code>pn</code> <strong>密度相连</strong>。</p></li></ul><ol><li><p><strong>随机选择一个未访问的点 <code>p</code></strong>。</p></li><li><p><strong>检查 <code>p</code> 的 <code>eps</code> 邻域</strong>：</p><ul><li><p>如果 <code>p</code> 是核心点（邻域内点数 ≥ <code>min_samples</code>）：</p><ul><li><p>创建一个新簇。</p></li><li><p>递归地找出所有从 <code>p</code> <strong>密度可达</strong>的点，加入该簇。</p></li></ul></li><li><p>如果 <code>p</code> 是噪声点，标记为噪声。</p></li></ul></li><li><p><strong>重复上述过程，直到所有点被访问</strong>。</p></li></ol><ul><li><p><strong>不需要预先指定簇数量（k）</strong>（比 K-means 更灵活）。</p></li><li><p><strong>能发现任意形状的簇</strong>（K-means 只能发现球形簇）。</p></li><li><p><strong>能识别噪声点</strong>（适合处理离群值）。</p></li><li><p><strong>对参数 <code>eps</code> 和 <code>min_samples</code> 敏感</strong>，需要调参。</p><p>| 特性           | DBSCAN                | K-means |<br>| —————— | ——————————- | ———- |<br>| <strong>簇形状</strong>      | 任意形状                  | 球形      |<br>| <strong>噪声处理</strong>     | 能识别噪声                 | 不能      |<br>| <strong>需指定簇数（k）</strong> | 不需要                   | 需要      |<br>| <strong>参数依赖</strong>     | <code>eps</code> 和 <code>min_samples</code> | <code>k</code>     |<br>| <strong>适合场景</strong>     | 非凸簇、噪声多               | 凸簇、数据均匀 |</p><p><code>neighbors += new_neighbors  # 将新邻域的点加入待处理列表</code><br><code>fit_predict()</code> 是一个结合了训练和预测的简化语法，通常在 <strong>聚类</strong> 或 <strong>无监督学习</strong> 中使用，它的作用是：首先对数据进行训练（拟合模型），然后直接对数据进行预测，返回聚类标签或者分类标签。这种方法可以简化代码并提高效率，尤其是在进行无监督学习时</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">#encoding&#x3D;utf8 import numpy as npimport random# 寻找eps邻域内的点def findNeighbor(j, X, eps):    &quot;&quot;&quot;    计算样本X[j]的eps邻域，返回所有在该邻域内的点的索引    input:        j (int): 当前点的索引        X (ndarray): 样本数据        eps (float): 邻域半径    output:        N (list): 邻域内的点的索引    &quot;&quot;&quot;    N &#x3D; []  # 存放邻域内的点    for p in range(X.shape[0]):  # 遍历所有点        # 计算欧式距离        temp &#x3D; np.sqrt(np.sum(np.square(X[j] - X[p])))          if temp &lt;&#x3D; eps:  # 如果点p在eps半径范围内            N.append(p)  # 将该点加入邻域列表    return Ndef expandCluster(X, labels, point, neighbors, cluster_id, eps, min_Pts):    &quot;&quot;&quot;    扩展簇，将密度可达的点加入到当前簇    input:        X (ndarray): 样本数据        labels (list): 当前的聚类标签        point (int): 当前扩展的核心点的索引        neighbors (list): 核心点的邻域        cluster_id (int): 当前簇的ID        eps (float): 邻域半径        min_Pts (int): 最小邻域内点数    &quot;&quot;&quot;    labels[point] &#x3D; cluster_id  # 将当前点标记为当前簇的成员    i &#x3D; 0    while i &lt; len(neighbors):  # 遍历邻域中的每一个点        neighbor_point &#x3D; neighbors[i]        # 如果邻居点是噪声（未分类），将其标记为当前簇的一部分        if labels[neighbor_point] &#x3D;&#x3D; -1:              labels[neighbor_point] &#x3D; cluster_id        # 如果邻居点没有被标记，且它是一个核心点，则继续扩展簇        elif labels[neighbor_point] &#x3D;&#x3D; 0:            labels[neighbor_point] &#x3D; cluster_id            new_neighbors &#x3D; findNeighbor(neighbor_point, X, eps)            if len(new_neighbors) &gt;&#x3D; min_Pts:  # 如果该点是核心点                neighbors +&#x3D; new_neighbors  # 将新邻域的点加入待处理列表        i +&#x3D; 1# DBSCAN算法def dbscan(X, eps, min_Pts):    &#39;&#39;&#39;    输入:        X (ndarray): 样本数据        eps (float): eps邻域半径        min_Pts (int): eps邻域内最少点个数    输出:        labels (list): 聚类结果，每个点的标签（-1表示噪声，其他数字表示簇编号）    &#39;&#39;&#39;    # 样本数量    nums &#x3D; len(X)    labels &#x3D; np.zeros(nums)  # 初始化所有点的标签为0（表示未处理）    cluster_id &#x3D; 0  # 聚类编号初始化为0    # 遍历每个样本点    for i in range(nums):        if labels[i] !&#x3D; 0:  # 如果该点已经被处理过，跳过            continue        else:            # 获取点i的邻域            neighbors &#x3D; findNeighbor(i, X, eps)            if len(neighbors) &lt; min_Pts:  # 如果邻域内的点数小于min_Pts，标记为噪声                labels[i] &#x3D; -1            else:  # 如果是核心点，则扩展簇                cluster_id +&#x3D; 1  # 创建一个新的簇                labels[i] &#x3D; cluster_id  # 将当前点标记为该簇                expandCluster(X, labels, i, neighbors, cluster_id, eps, min_Pts)    return labels  # 返回聚类结果<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="AGNES"><a href="#AGNES" class="headerlink" title="AGNES"></a>AGNES</h3><p>自底向上的不断合并簇直到簇数量满足要求</p><p><code>clusters[cova] += clusters[covb]</code> 合并簇</p><p><code>del clusters[covb]  # 删除第二个簇</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef AGNES(feature, k):    &#39;&#39;&#39;    AGNES聚类并返回聚类结果，量化距离时请使用簇间最大欧氏距离    假设数据集为&#96;[1, 2], [10, 11], [1, 3]]，那么聚类结果可能为&#96;[[1, 2], [1, 3]], [[10, 11]]]    :param feature:数据集，类型为ndarray    :param k:表示想要将数据聚成&#96;k&#96;类，类型为&#96;int&#96;    :return:聚类结果，类型为list    &#39;&#39;&#39;    #********* Begin *********#    clusters&#x3D;[[feature[i]] for i in range(len(feature))]    cluster_nums&#x3D;len(clusters)    while cluster_nums &gt; k:        dist_min &#x3D; 1e9        cova, covb &#x3D; -1, -1        # 寻找距离最小的簇对        for idx1 in range(cluster_nums):            for idx2 in range(idx1 + 1, cluster_nums):                # 计算两个簇之间的最小欧氏距离                dist &#x3D; []                for i in clusters[idx1]:                    for j in clusters[idx2]:                        dist.append(np.linalg.norm(i - j))                curr_max &#x3D; np.min(dist)  # 选择最小欧氏距离                if curr_min &lt; dist_min:                    dist_min &#x3D; curr_max                    cova, covb &#x3D; idx1, idx2        # 合并最小距离的两个簇        clusters[cova] +&#x3D; clusters[covb]        del clusters[covb]  # 删除第二个簇        # 更新簇的数量        cluster_nums -&#x3D; 1    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>单次迭代</p><p><code>np.sum(np.abs(np.array(new_thetas)-np.array(thetas))</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef em_single(init_values, observations):    pa, pb &#x3D; init_values    a_heads &#x3D; 0    a_tails &#x3D; 0    b_heads &#x3D; 0    b_tails &#x3D; 0    for trial in observations:        num_heads &#x3D; sum(trial)        num_tails &#x3D; len(trial) - num_heads        log_ka &#x3D; np.sum([np.log(pa if coin &#x3D;&#x3D; 1 else 1 - pa) for coin in trial])        log_kb &#x3D; np.sum([np.log(pb if coin &#x3D;&#x3D; 1 else 1 - pb) for coin in trial])        # Convert log-probabilities to probabilities safely        max_log &#x3D; max(log_ka, log_kb)        exp_ka &#x3D; np.exp(log_ka - max_log)        exp_kb &#x3D; np.exp(log_kb - max_log)        total &#x3D; exp_ka + exp_kb        softa &#x3D; exp_ka &#x2F; total        softb &#x3D; exp_kb &#x2F; total        a_heads +&#x3D; softa * num_heads        a_tails +&#x3D; softa * num_tails        b_heads +&#x3D; softb * num_heads        b_tails +&#x3D; softb * num_tails    return [a_heads &#x2F; (a_heads + a_tails), b_heads &#x2F; (b_heads + b_tails)]def em(observations, thetas, tol&#x3D;1e-4, iterations&#x3D;100):    &quot;&quot;&quot;    模拟抛掷硬币实验并使用EM算法估计硬币A与硬币B正面朝上的概率。    :param observations: 抛掷硬币的实验结果记录，类型为list。    :param thetas: 硬币A与硬币B正面朝上的概率的初始值，类型为list，如[0.2, 0.7]代表硬币A正面朝上的概率为0.2，硬币B正面朝上的概率为0.7。    :param tol: 差异容忍度，即当EM算法估计出来的参数theta不怎么变化时，可以提前挑出循环。例如容忍度为1e-4，则表示若这次迭代的估计结果与上一次迭代的估计结果之间的L1距离小于1e-4则跳出循环。为了正确的评测，请不要修改该值。    :param iterations: EM算法的最大迭代次数。为了正确的评测，请不要修改该值。    :return: 将估计出来的硬币A和硬币B正面朝上的概率组成list或者ndarray返回。如[0.4, 0.6]表示你认为硬币A正面朝上的概率为0.4，硬币B正面朝上的概率为0.6。    &quot;&quot;&quot;    #********* Begin *********#    new_thetas&#x3D;0    for i in range(iterations):        new_thetas&#x3D;em_single(thetas,observations)        if(np.sum(np.abs(np.array(new_thetas)-np.array(thetas)))&lt;tol):            break        thetas&#x3D;new_thetas    return new_thetas    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p><img title="" src="/images/ml/330931b0-7aa1-49a4-9175-648ba48b488f.png" alt="330931b0-7aa1-49a4-9175-648ba48b488f" style="zoom:100%;"><img src="/images/ml/4c790a9f-bc87-4ae3-b8d2-0e71053ba5fe.png" title="" alt="4c790a9f-bc87-4ae3-b8d2-0e71053ba5fe" style="zoom:100%;"><img src="/images/ml/2511f0bd-2098-4af2-8995-1ad788f84bf2.png" title="" alt="2511f0bd-2098-4af2-8995-1ad788f84bf2" style="zoom:100%;"><img src="/images/ml/f58e9601-924d-4456-bfd0-82c71595e1aa.png" alt="f58e9601-924d-4456-bfd0-82c71595e1aa"></p><p><code>if y_true[i] == y_true[j] and y_pred[i] == y_pre #求与操作用and，&amp;是位运算</code></p><p><code>a+=1</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def calc(y_true, y_pred):    a &#x3D; b &#x3D; c &#x3D; d &#x3D; 0    for i in range(len(y_true)):        for j in range(i + 1, len(y_pred)):            if y_true[i] &#x3D;&#x3D; y_true[j] and y_pred[i] &#x3D;&#x3D; y_pred[j]:                a +&#x3D; 1            elif y_true[i] &#x3D;&#x3D; y_true[j] and y_pred[i] !&#x3D; y_pred[j]:                b +&#x3D; 1            elif y_true[i] !&#x3D; y_true[j] and y_pred[i] &#x3D;&#x3D; y_pred[j]:                c +&#x3D; 1            else:                d +&#x3D; 1    return [a, b, c, d]def calc_JC(y_true, y_pred):    ex &#x3D; calc(y_true, y_pred)    return ex[0] &#x2F; (ex[0] + ex[1] + ex[2]) if (ex[0] + ex[1] + ex[2]) !&#x3D; 0 else 0.0def calc_FM(y_true, y_pred):    ex &#x3D; calc(y_true, y_pred)    if (ex[0] + ex[1]) &#x3D;&#x3D; 0 or (ex[0] + ex[2]) &#x3D;&#x3D; 0:        return 0.0    return np.sqrt((ex[0] &#x2F; (ex[0] + ex[1])) * (ex[0] &#x2F; (ex[0] + ex[2])))def calc_Rand(y_true, y_pred):    m &#x3D; len(y_true)    ex &#x3D; calc(y_true, y_pred)    return 2 * (ex[0] + ex[3]) &#x2F; (m * (m - 1)) if m &gt; 1 else 0.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>DB 指数越小就越就意味着簇内距离越小同时簇间距离越大，也就是说DB 指数越小越好</p><p>Dunn 指数越大意味着簇内距离越小同时簇间距离越大，也就是说 Dunn 指数 越大越好</p><p><img title="" src="/images/ml/29169c68-916f-4aca-b12f-9cf208277638.png" alt="29169c68-916f-4aca-b12f-9cf208277638" style="zoom:100%;"><img title="" src="/images/ml/a27f2156-e19b-448d-a1a4-35ff3967c1e0.png" alt="a27f2156-e19b-448d-a1a4-35ff3967c1e0" style="zoom:100%;"></p><p><code>np.array,arr.tolist</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npa &#x3D; [1, 2, 3]arr &#x3D; np.array(a)arr &#x3D; np.array([1, 2, 3])a &#x3D; arr.tolist()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>centroids = np.array([np.mean(feature[pred == label], axis=0) for label in labels])</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef calc_DBI(feature, pred):    labels &#x3D; np.unique(pred)    k &#x3D; len(labels)    centroids &#x3D; np.array([np.mean(feature[pred &#x3D;&#x3D; label], axis&#x3D;0) for label in labels])    # 计算每个簇的类内平均距离（紧密度）    s &#x3D; []    for label in labels:        cluster_points &#x3D; feature[pred &#x3D;&#x3D; label]        centroid &#x3D; np.mean(cluster_points, axis&#x3D;0)        distances &#x3D; np.sqrt(np.sum((cluster_points - centroid) ** 2, axis&#x3D;1))        s.append(np.mean(distances))    s &#x3D; np.array(s)    # 计算簇质心间的距离矩阵    M &#x3D; np.zeros((k, k))    for i in range(k):        for j in range(k):            if i !&#x3D; j:                M[i][j] &#x3D; np.linalg.norm(centroids[i] - centroids[j])    # DBI 计算    dbi &#x3D; 0    for i in range(k):        max_r &#x3D; -np.inf        for j in range(k):            if i !&#x3D; j:                r &#x3D; (s[i] + s[j]) &#x2F; M[i][j]                if r &gt; max_r:                    max_r &#x3D; r        dbi +&#x3D; max_r    return dbi &#x2F; k    for label in labels:        feat&#x3D;feature[pred&#x3D;&#x3D;label]        center&#x3D;np.mean(feat,axis&#x3D;0)        dist&#x3D;[]        for i in feat:            dist.append(np.linalg.norm(i-center))        s.append(np.mean(dist))    #s[i]为簇内平均距离    maxsum&#x3D;0    for i in range(k):        max1&#x3D;0        for j in range(k):            if(i!&#x3D;j):                max1&#x3D;max((s[i]+s[j])&#x2F;np.linalg.norm(centroids[i]-centroids[j]),max1)        maxsum+&#x3D;max1    return maxsum&#x2F;k    #********* End *********#def calc_DI(feature, pred):    labels &#x3D; np.unique(pred)    k &#x3D; len(labels)    # 计算每个簇的最大内部距离    max_intra &#x3D; 0    for label in labels:        points &#x3D; feature[pred &#x3D;&#x3D; label]        for i in range(len(points)):            for j in range(i + 1, len(points)):                dist &#x3D; np.linalg.norm(points[i] - points[j])                if dist &gt; max_intra:                    max_intra &#x3D; dist    # 计算簇间最小距离    min_inter &#x3D; np.inf    for i in range(k):        for j in range(i + 1, k):            points_i &#x3D; feature[pred &#x3D;&#x3D; labels[i]]            points_j &#x3D; feature[pred &#x3D;&#x3D; labels[j]]            for p in points_i:                for q in points_j:                    dist &#x3D; np.linalg.norm(p - q)                    if dist &lt; min_inter:                        min_inter &#x3D; dist    return min_inter &#x2F; max_intra if max_intra !&#x3D; 0 else 0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>维数灾难通常是指对于已知样本数目，存在一个特征数目的最大值，当实际使用的特征数目超过这个最大值时，机器学习算法的性能不是得到改善，而是退化（过拟合）。</p><ul><li><p>降低机器学习算法的时间复杂度；</p></li><li><p>节省了提取不必要特征的开销；</p></li><li><p>缓解因为维数灾难所造成的过拟合现象。</p></li></ul><h3 id="PCA主成分分析"><a href="#PCA主成分分析" class="headerlink" title="PCA主成分分析"></a>PCA主成分分析</h3><p> PCA 是将数据从原来的坐标系转换到新的坐标系，新的坐标系的选择是由数据本身决定的。</p><p>第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且方差最大的方向。然后该过程一直重复，重复次数为原始数据中的特征数量。最后会发现大部分方差都包含在最前面几个新坐标轴中，因此可以忽略剩下的坐标轴，从而达到降维的目的。</p><p> PCA 在降维时，需要指定将维度降至多少维，假设降至 k 维，则 PCA 的算法流程如下：</p><p>demean；<br>计算数据的协方差矩阵；<br>计算协方差矩阵的特征值与特征向量；<br>按照特征值，将特征向量从大到小进行排序；<br>选取前 k 个特征向量作为转换矩阵；<br>demean 后的数据与转换矩阵做矩阵乘法获得降维后的数据</p><p><code>matrix=np.cov(data.T) #cov函数期望行代表特征</code></p><p><code>val, vec = np.linalg.eigh(mat) #计算特征值和特征向量</code></p><p><code>indices = np.argsort(val)[::-1]  # 倒序排序，val越大越重要</code></p><p><code>vec_res = vec_sort[:,:k]</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef pca(data, k):    &#39;&#39;&#39;    对data进行PCA，并将结果返回    :param data: 数据集，类型为ndarray    :param k: 想要降成几维，类型为int    :return: 降维后的数据，类型为ndarray    &#39;&#39;&#39;    #********* Begin *********#    # 均值归一化    me &#x3D; np.mean(data, axis&#x3D;0)    data &#x3D; data - me    # 计算协方差矩阵        # after_demean的行数为样本个数，列数为特征个数    # 由于cov函数的输入希望是行代表特征，列代表数据的矩阵，所以要转置    mat &#x3D; np.cov(data.T)    # 计算特征值和特征向量    val, vec &#x3D; np.linalg.eigh(mat)    # 按特征值从大到小排序，选择前k个特征向量    indices &#x3D; np.argsort(val)[::-1]  # 倒序排序，val越大越重要    vec_sort &#x3D; vec[:, indices]       # 按排序后的索引选择特征向量    # 选择前k个主成分    vec_res &#x3D; vec_sort[:, [i for i in range(k)]]        # 取前k列    # 返回降维后的数据    return data.dot(vec_res)    #********* End *********#<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="MDS多维缩放"><a href="#MDS多维缩放" class="headerlink" title="MDS多维缩放"></a>MDS多维缩放</h3><p>MDS 算法认为，在数据样本中，每个样本的每个特征值并不是数据间关系的必要特征，真正的基础特征是每个点与数据集中其他点的距离。</p><p><img src="/images/ml/52b1aa56-a1b4-4912-9f37-958227f2295d.png" title="" alt="52b1aa56-a1b4-4912-9f37-958227f2295d" style="zoom:100%;"></p><p><code>H = np.eye(n_samples) - np.ones((n_samples, n_samples)) / n_samples #单位阵减去1/n全一矩阵</code></p><p><code>B = -0.5 * np.dot(H, np.dot(distance_matrix ** 2, H))</code></p><p><code>eigvals_k = np.diag(np.sqrt(eigvals_sorted[:k])) #取前 k 个排序好的特征值，先开平方，然后把它们放到一个对角矩阵中</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npdef euclidean_distance(x, y):    &quot;&quot;&quot;计算欧几里得距离&quot;&quot;&quot;    return np.sqrt(np.sum((x - y) ** 2))def compute_distance_matrix(data):    &quot;&quot;&quot;计算距离矩阵&quot;&quot;&quot;    n_samples &#x3D; data.shape[0]    distance_matrix &#x3D; np.zeros((n_samples, n_samples))    for i in range(n_samples):        for j in range(n_samples):           distance_matrix[i, j] &#x3D; euclidean_distance(data[i], data[j])    return distance_matrixdef mds(data, k):    &quot;&quot;&quot;    手动实现MDS降维    :param data: 原始数据 (样本，特征) 形式的 numpy 数组    :param k: 目标降维的维度，默认2    :return: 降维后的数据    &quot;&quot;&quot;    # 1. 计算距离矩阵    distance_matrix &#x3D; compute_distance_matrix(data)    # 2. 中心化距离矩阵    n_samples &#x3D; distance_matrix.shape[0]    H &#x3D; np.eye(n_samples) - np.ones((n_samples, n_samples)) &#x2F; n_samples    B &#x3D; -0.5 * np.dot(H, np.dot(distance_matrix ** 2, H))    # 3. 特征值分解    eigvals, eigvecs &#x3D; np.linalg.eigh(B)    # 4. 按照特征值从大到小排序    sorted_indices &#x3D; np.argsort(eigvals)[::-1]  # 降序排列    eigvals_sorted &#x3D; eigvals[sorted_indices]    eigvecs_sorted &#x3D; eigvecs[:, sorted_indices]    # 5. 选择前k个特征值和特征向量    eigvals_k &#x3D; np.diag(np.sqrt(eigvals_sorted[:k]))    eigvecs_k &#x3D; eigvecs_sorted[:, :k]    # 6. 计算低维坐标    low_dim_data &#x3D; np.dot(eigvecs_k, eigvals_k)    return low_dim_data  # 返回转置后的低维数据# -*- coding: utf-8 -*-from sklearn.manifold import MDSdef mds(data,d):    &#39;&#39;&#39;    input:data(ndarray):待降维数据          d(int):降维后数据维度    output:Z(ndarray):降维后数据    &#39;&#39;&#39;    #********* Begin *********#    model&#x3D;MDS(n_components&#x3D;d)#n_components ：即我们进行 MDS 降维时降到的维数。在降维时需要输入这个参数。    Z&#x3D;model.fit_transform(data)    #********* End *********#    return Z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Isomap等度量映射"><a href="#Isomap等度量映射" class="headerlink" title="Isomap等度量映射"></a>Isomap等度量映射</h3><p>Isomap 算法可以看作是 “在局部保持邻居关系的基础上，再用 MDS 降维”</p><p><img src="/images/ml/12a51984-99cd-49db-8d4d-223eb5b394bc.png" alt="12a51984-99cd-49db-8d4d-223eb5b394bc"></p><p><code>Z = V @ L #特征向量组成的矩阵在左边，乘特征值构成的的对角阵</code></p><p><code>model = Isomap(n_components=d,n_neighbors=k) 取k个相邻数据，降到d维</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># -*- coding: utf-8 -*-import numpy as npdef isomap(data, d, k, Max&#x3D;1e9):    &#39;&#39;&#39;    input:        data(ndarray): 待降维数据，shape&#x3D;(n_samples, n_features)        d(int): 降维后数据维数        k(int): 最近的k个样本        Max(float): 表示无穷大    output:        Z(ndarray): 降维后的数据，shape&#x3D;(n_samples, d)    &#39;&#39;&#39;    n &#x3D; data.shape[0]    # Step 1: 计算欧氏距离矩阵    dist_matrix &#x3D; np.zeros((n, n))    for i in range(n):        for j in range(i + 1, n):            dist &#x3D; np.linalg.norm(data[i] - data[j])            dist_matrix[i][j] &#x3D; dist_matrix[j][i] &#x3D; dist    # Step 2: 构建 k-近邻图（非邻接的点设为 Max）    graph &#x3D; np.full((n, n), Max)    for i in range(n):        neighbors &#x3D; np.argsort(dist_matrix[i])[1:k+1]  # 取前k个最近邻（排除自身）        for j in neighbors:            graph[i][j] &#x3D; dist_matrix[i][j]            graph[j][i] &#x3D; dist_matrix[i][j]  # 保持对称    # Step 3: 使用 Floyd-Warshall 算法求最短路径（近似测地线）    for k_mid in range(n):        for i in range(n):            for j in range(n):                if graph[i][j] &gt; graph[i][k_mid] + graph[k_mid][j]:                    graph[i][j] &#x3D; graph[i][k_mid] + graph[k_mid][j]    # Step 4: 多维尺度分析 (MDS)    D &#x3D; graph    D_squared &#x3D; D ** 2    H &#x3D; np.eye(n) - np.ones((n, n)) &#x2F; n    B &#x3D; -0.5 * H @ D_squared @ H    # Step 5: 特征值分解    eigvals, eigvecs &#x3D; np.linalg.eigh(B)    idx &#x3D; np.argsort(eigvals)[::-1]    eigvals &#x3D; eigvals[idx]    eigvecs &#x3D; eigvecs[:, idx]    # Step 6: 计算嵌入坐标 Z    L &#x3D; np.diag(np.sqrt(eigvals[:d]))    V &#x3D; eigvecs[:, :d]    Z &#x3D; V @ L    return Z# -*- coding: utf-8 -*-from sklearn.manifold import Isomapdef isomap(data,d,k):    &#39;&#39;&#39;    input:data(ndarray):待降维数据          d(int):降维后数据维度          k(int):最近的k个样本    output:Z(ndarray):降维后数据    &#39;&#39;&#39;    #********* Begin *********#    model &#x3D; Isomap(n_components&#x3D;d,n_neighbors&#x3D;k)    Z&#x3D;model.fit_transform(data)    #********* End *********#    return Z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="LLE局部线性嵌入"><a href="#LLE局部线性嵌入" class="headerlink" title="LLE局部线性嵌入"></a>LLE局部线性嵌入</h3><p><img src="/images/ml/8fe236dc-3502-4bd8-9969-bbe2f0fd0db8.png" title="" alt="8fe236dc-3502-4bd8-9969-bbe2f0fd0db8" style="zoom:100%;"></p><p><code>Z = data[N_i] - data[i]  # shape: (k, m)</code></p><p> <code>C = Z.dot(Z.T)  # 计算邻居之间差异的协方差矩阵 C，形状是 (k, k)</code></p><p><code>C += np.eye(k) * 1e-3  # 为了数值稳定，给协方差矩阵 C 加上一个小的正则项（0.001）</code></p><p><code>w = np.linalg.solve(C, ones)  # 解线性方程组，得到每个邻居的权重</code></p><p><code>W[i, N_i[j]] = w[j]  # 将第 j 个邻居的权重填入相应位置</code></p><p><code>M = (I - W).T.dot(I - W)  # 计算矩阵 M，形式为 (I - W)^T * (I - W)</code></p><p> <code>Z = eigvecs[:, idx[1:d+1]]  # 选取最小的 d 个特征值对应的特征向量（跳过第一个全1特征向量）</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># encoding&#x3D;utf8 import numpy as np# 计算数据集的欧氏距离矩阵def calc_dist(data):    n &#x3D; data.shape[0]  # 获取样本数目    dist_matrix &#x3D; np.zeros((n, n))  # 初始化一个 n x n 的距离矩阵    for i in range(n):        for j in range(n):            # 计算第 i 个样本与第 j 个样本的欧氏距离            dist_matrix[i, j] &#x3D; np.linalg.norm(data[i] - data[j])  # np.linalg.norm 计算欧氏距离    return dist_matrix# 找到每个样本的 k 个最近邻def find_neighbors(dist_matrix, k):    n &#x3D; dist_matrix.shape[0]  # 获取样本数目    neighbors &#x3D; np.zeros((n, k), dtype&#x3D;int)  # 初始化一个 n x k 的邻居矩阵    for i in range(n):        # 对于每个样本，按距离从小到大排序，找到前 k 个邻居（排除自己）        sorted_indices &#x3D; np.argsort(dist_matrix[i])  # 排序，返回排序后的索引        neighbors[i] &#x3D; sorted_indices[1:k+1]  # 排除自己（即第一个是自己，跳过）    return neighbors# 局部线性嵌入（LLE）算法def lle(data, d, k):    n, m &#x3D; data.shape  # n: 样本数目, m: 原始特征维度    dist_matrix &#x3D; calc_dist(data)  # 计算数据点之间的距离矩阵    neighbors &#x3D; find_neighbors(dist_matrix, k)  # 找到每个样本的 k 个邻居    # 初始化权重矩阵 W，存储每个样本到其邻居的权重    W &#x3D; np.zeros((n, n))    # 对于每个样本，计算其到邻居的权重    for i in range(n):        # 获取第 i 个样本的 k 个邻居的索引        N_i &#x3D; neighbors[i]        # 构建邻居矩阵 Z：每列是一个邻居与 x_i 的差        Z &#x3D; data[N_i] - data[i]  # Z 的形状是 (k, m)，即每行是一个邻居的差向量        C &#x3D; Z.dot(Z.T)  # 计算邻居之间差异的协方差矩阵 C，形状是 (k, k)        C +&#x3D; np.eye(k) * 1e-3  # 为了数值稳定，给协方差矩阵 C 加上一个小的正则项（0.001）#在计算协方差矩阵 C &#x3D; Z.dot(Z.T) 时，可能会遇到矩阵 C 变得奇异（即行列式为零），特别是当邻居之间的差异非常小或者完全相同时。这种情况下，矩阵 C 可能不可逆，导致无法进行后续的线性求解。        # 解线性方程 Cw &#x3D; 1，得到权重 w        ones &#x3D; np.ones(k)  # 创建一个大小为 k 的全1向量        w &#x3D; np.linalg.solve(C, ones)  # 解线性方程组，得到每个邻居的权重        w &#x3D; w &#x2F; np.sum(w)  # 归一化权重，使得它们的和为 1        # 将计算得到的权重 w 填入第 i 行的权重矩阵 W 中        for j in range(k):            W[i, N_i[j]] &#x3D; w[j]  # 将第 j 个邻居的权重填入相应位置    # 构造矩阵 M &#x3D; (I - W)^T * (I - W)    I &#x3D; np.eye(n)  # n x n 的单位矩阵    M &#x3D; (I - W).T.dot(I - W)  # 计算矩阵 M，形式为 (I - W)^T * (I - W)    # 计算 M 的特征值和特征向量    eigvals, eigvecs &#x3D; np.linalg.eigh(M)  # eigvals: 特征值, eigvecs: 特征向量    # 选择最小的 d+1 个特征值对应的特征向量（第一个特征值是 0 对应的全1向量，跳过）    idx &#x3D; np.argsort(eigvals)  # 获取特征值的排序索引    Z &#x3D; eigvecs[:, idx[1:d+1]]  # 选取最小的 d 个特征值对应的特征向量（跳过第一个全1特征向量）    return Z  # 返回降维后的数据# -*- coding: utf-8 -*-from sklearn.manifold import LocallyLinearEmbeddingdef lle(data,d,k):    &#39;&#39;&#39;    input:data(ndarray):待降维数据          d(int):降维后数据维度          k(int):邻域内样本数    output:Z(ndarray):降维后数据    &#39;&#39;&#39;    #********* Begin *********#    model&#x3D;LocallyLinearEmbedding(n_components&#x3D;d,n_neighbors&#x3D;k)    Z&#x3D;model.fit_transform(data)    #********* End *********#    return Z<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image as IMU</title>
      <link href="/2025/10/08/Image-as-IMU/"/>
      <url>/2025/10/08/Image-as-IMU/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" async  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><p>这篇文章提出了从单张运动模糊图像中估计相机速度的方法，将模糊作为运动的线索而非噪声，此处的运动模糊是曝光时间内多个“虚拟图像”的叠加，其模糊轨迹可视为“虚拟光流”，蕴含了相机运动信息</p><hr><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><p>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</p><p>文章来自2025ICCV，作者是Jerred Chen 和Ronald Clark，arxiv地址为</p><p><a href="https://arxiv.org/abs/2503.17358">[2503.17358] Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</a></p><h2 id="传统方法的问题"><a href="#传统方法的问题" class="headerlink" title="传统方法的问题"></a>传统方法的问题</h2><p>在机器人和VR/AR应用中，相机快速移动会导致严重的<strong>运动模糊</strong>（motion blur）。这种模糊会破坏传统视觉里程计（VO）和运动恢复结构（SfM）方法所依赖的图像特征匹配，导致这些方法失效</p><p><strong>现有解决方案的缺陷</strong>：</p><ul><li><strong>丢弃模糊帧</strong>：简单粗暴，但是会丢失宝贵的运动信息。</li><li><strong>融合IMU</strong>：虽然有效，但引入了额外的硬件成本、复杂的传感器同步问题以及IMU固有的漂移误差（drift）</li></ul><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><ul><li><p><strong>图像形成过程</strong>：在曝光时间 τ 内，相机不断移动，每个像素累积了来自不同场景点的光线。公式(1)给出了模糊图像 $I_B$​ 的积分定义。</p><script type="math/tex; mode=display">I_B = g \int_{\tau_0}^{\tau} I_\nu(t) \, dt</script></li><li><p><strong>离散化近似</strong>：在实践中，将曝光时间离散化为 N 个瞬间，每个瞬间对应一个“虚拟图像”$I_{\nu_i}$ ​。模糊图像是这些虚拟图像的平均。</p><script type="math/tex; mode=display">I_B \approx g \frac{1}{N} \sum_{i=1}^{N} I_{\nu_i}</script></li><li><p><strong>核心直觉</strong>：一张模糊图像可以被视为由多个“虚拟图像”叠加而成。这些虚拟图像之间的像素对应关系，就编码了相机的运动信息。</p></li><li><p><strong>虚拟对应关系</strong>：假设场景是刚性的，那么从第一个虚拟图像到最后一个虚拟图像   $I_{\nu_1}, \ldots, I_{\nu_N}$  每个像素的位移就是一个“虚拟光流”。这个光流场直接关联到相机在曝光期间的相对运动。</p></li><li><p><strong>结合深度</strong>：如果再知道场景的深度 D，就可以将2D的像素运动（光流）与3D的相机运动联系起来，从而求解出完整的6DoF相对位姿。</p></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Flow-and-Depth-Prediction-光流与深度预测"><a href="#Flow-and-Depth-Prediction-光流与深度预测" class="headerlink" title="Flow and Depth Prediction (光流与深度预测)"></a><strong>Flow and Depth Prediction (光流与深度预测)</strong></h3><ul><li><p><strong>网络架构</strong>：采用 <strong>SegNeXt</strong> 作为共享编码器，两个独立的解码器分别输出光流 F 和深度 D。</p></li><li><p><strong>光流定义</strong>：其中 p1​ 是第一个虚拟图像中的像素坐标，p2′​ 是同一3D点投影到最后一个虚拟图像中的坐标。</p><script type="math/tex; mode=display">F = p'_2 - p_1</script></li><li><p><strong>损失函数</strong>：</p></li></ul><script type="math/tex; mode=display">L_1 = \lambda_F \|F - h_f(\hat{F}_{fw}, \hat{F}_{bw})\| + \lambda_D \|D - \hat{D}\|</script><ul><li><p>对光流和深度分别施加L1损失。</p></li><li><p>引入<strong>重定向函数</strong> hf​解决训练歧义。因为无法确定哪个虚拟图像是“开始”，所以标签 F 会被设定为与预测方向 F 更接近的那个真实方向（正向 $F_{fw}$或反向$F_{bw}$确保训练稳定。</p><script type="math/tex; mode=display">h_f(\hat{F}_{fw}, \hat{F}_{bw}; F) =\begin{cases}\hat{F}_{fw}, & \text{if } \langle \hat{F}_{fw}, F \rangle > \langle \hat{F}_{bw}, F \rangle \\\hat{F}_{bw}, & \text{otherwise}\end{cases}</script></li></ul><h3 id="Differentiable-Velocity-Computation-可微分速度计算"><a href="#Differentiable-Velocity-Computation-可微分速度计算" class="headerlink" title="Differentiable Velocity Computation (可微分速度计算)"></a><strong>Differentiable Velocity Computation (可微分速度计算)</strong></h3><ul><li><strong>理论依据</strong>：采用Trucco and Verri的运动场方程，该方程建立了像素运动 (Fx​,Fy​)、深度 d、焦距 f 和相机6DoF运动 (θx​,θy​,θz​,tx​,ty​,tz​) 之间的数学关系。</li></ul><script type="math/tex; mode=display">  F_x = \frac{t_z p_x - t_x f}{d} - \theta_y f + \theta_z p_y + \frac{\theta_x p_x p_y}{f} - \frac{\theta_y p_x^2}{f}</script><script type="math/tex; mode=display">  F_y = \frac{t_z p_y - t_y f}{d} + \theta_x f - \theta_z p_x - \frac{\theta_y p_x p_y}{f} + \frac{\theta_x p_y^2}{f}</script><ul><li><p><strong>线性系统</strong>：将运动场方程改写为标准的线性形式 Ax=b（公式8和9）。</p><ul><li>A 矩阵由像素坐标、深度和焦距构成。</li><li>x 是待求解的6个运动参数。</li><li>b 是预测的光流向量。</li></ul></li></ul><script type="math/tex; mode=display">A =\begin{bmatrix} -\frac{f}{d} & 0 & \frac{p_x}{d} & \frac{p_x p_y}{f} & -\frac{p_x^2 + f^2}{f} & p_y \\ 0 & -\frac{f}{d} & \frac{p_y}{d} & \frac{p_y^2 + f^2}{f} & -\frac{p_x p_y}{f} & -p_x\end{bmatrix}</script><script type="math/tex; mode=display">x =\begin{bmatrix}t_x \\ t_y \\ t_z \\ \theta_x \\ \theta_y \\ \theta_z\end{bmatrix}, \quadb =\begin{bmatrix}F_x \\ F_y\end{bmatrix}</script><ul><li><strong>最小二乘求解</strong>：对于图像中的所有像素，系统是超定的。使用最小二乘法求解最优解</li></ul><script type="math/tex; mode=display">  x = (A^\top A)^{-1} A^\top b</script><ul><li><p><strong>速度计算</strong>：将求得的相对位姿变化除以曝光时间 τ，即可得到瞬时速度 (ω,v)。</p></li><li><p><strong>端到端训练</strong>：由于最小二乘求解是可微分的，可以将最终的速度预测误差（公式11中的 L2​）反向传播，实现从输入图像到最终速度的端到端优化。</p></li></ul><script type="math/tex; mode=display">L_2 = \lambda_R \| R - h_p(\hat{R}) \|^2      + \lambda_t \| t - h_p(\hat{t}) \|^2      + L_1</script><h3 id="Direction-Disambiguation-方向歧义性解决"><a href="#Direction-Disambiguation-方向歧义性解决" class="headerlink" title="Direction Disambiguation (方向歧义性解决)"></a><strong>Direction Disambiguation (方向歧义性解决)</strong></h3><ul><li><p><strong>问题根源</strong>：运动模糊是时间对称的，无法区分正向和反向运动。</p></li><li><p><strong>解决方案</strong>：</p><ol><li><p>给定连续帧$I_{i-1}$, $I_i$, $I_{i+1}$，其中 $I_i$ 是模糊图像。</p></li><li><p>将预测的完整光流 $F_ i$ 按时间比例缩放，得到短时间光流 $F_{i’}$,然后用 $F_{i’}$扭曲$l_{i}$得到$l_{i’}$</p><script type="math/tex; mode=display">F'_i = \Delta t_f \, F_i, \quadI'_i = \Phi(I_i; F'_i)</script></li><li><p>计算两种可能方向（正向和反向）下的总光度误差 $e_{fw}$​ 和 $e_{bw}$(i-1，i，i+1)。</p><script type="math/tex; mode=display">P(I_1, I_2) = \frac{1}{HW} \sum_{u=0}^{H-1} \sum_{v=0}^{W-1} \big| I_1(u,v) - I_2(u,v) \big| \tag {帧光度误差}</script><script type="math/tex; mode=display">e_{fw} = P(I_{i+1}, I'_i, fw) + P(I_{i-1}, I'_i, bw)</script><script type="math/tex; mode=display">e_{bw} = P(I_{i+1}, I'_i, bw) + P(I_{i-1}, I'_i, fw)</script></li><li><p>选择误差更小的方向作为最终的运动方向。</p><script type="math/tex; mode=display">\omega, v =\begin{cases}\omega_{fw}, v_{fw}, & \text{if } e_{fw} < e_{bw} \\\omega_{bw}, v_{bw}, & \text{otherwise}\end{cases}</script></li></ol></li></ul><h2 id="数据集构建"><a href="#数据集构建" class="headerlink" title="数据集构建"></a>数据集构建</h2><h3 id="合成数据集"><a href="#合成数据集" class="headerlink" title="合成数据集"></a>合成数据集</h3><p><strong>模糊合成流程</strong>：</p><ol><li>选取一个真实图像 $I_{i}$ 。</li><li>使用<strong>RIFE</strong>插值模型，在 $I_{i}$ 和后续的 N 个真实帧之间生成多个虚拟帧，形成一个序列$I$。</li><li>将序列 $I$转换到线性空间，取平均，再转回sRGB，得到模糊图像 $I_B$​。</li></ol><p><strong>真值获取</strong>：</p><ul><li><p><strong>深度</strong>：使用PromptDA模型，结合第一个虚拟图像 $I_{\nu_1}$​ 和ARKit稀疏深度，生成稠密深度图 D。</p></li><li><p><strong>光流</strong>：利用深度图 D，将 $I_{\nu_1}$​ 中的所有像素反投影到3D空间，再投影到 $I_{\nu_N}$，计算位移得到真值光流 F。</p></li><li><p><strong>规模</strong>：约12万训练样本，1.2万验证样本</p></li></ul><h3 id="真实数据集"><a href="#真实数据集" class="headerlink" title="真实数据集"></a>真实数据集</h3><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>为了评估模型的真实性能，作者在4个未见过的真实世界视频序列上进行了测试。评估的核心是相机的<strong>瞬时速度</strong>，因此需要精确的真值。</p><p>对于<strong>角速度</strong>（ω）的真值，直接采用了设备内置<strong>陀螺仪</strong>（gyroscope）的测量数据，这是最直接可靠的物理测量。对于<strong>线速度</strong>（v）的真值，由于没有直接的传感器，作者通过对<strong>ARKit</strong>系统估计出的相机位姿进行<strong>中心有限差分</strong>（centered finite-difference）计算来近似得到。</p><p>评价指标采用了各轴向的<strong>均方根误差</strong>（RMSE），单位分别为弧度每秒（rad/s）和米每秒（m/s）。作为对比的基线方法包括COLMAP、MASt3R和DROID-SLAM。由于这些方法本质上是估计多帧间的相对位姿而非瞬时速度，因此同样通过对它们输出的位姿序列进行有限差分来近似其速度估计，从而与本文方法进行公平比较。</p><h2 id="一些注意"><a href="#一些注意" class="headerlink" title="一些注意"></a>一些注意</h2><p>这篇论文虽然说“from a single motion-blurred image”，但其完整的方法流程（包括方向歧义解决）实际上<strong>隐含地假设了存在一个连续的视频流或至少能获取邻近帧</strong>。</p><p>例如系统正在实时接收视频帧。当检测到某一帧 ${I}_{i}$ 模糊时，它可以利用刚刚过去的 ${I}_{i-1}$ 和即将到来的 ${I}_{i+1}$来解决方向问题。</p>]]></content>
      
      
      <categories>
          
          <category> 2025ICCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运动模糊 </tag>
            
            <tag> 运动估计 </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some python grammar</title>
      <link href="/2025/10/06/Some-python-grammar/"/>
      <url>/2025/10/06/Some-python-grammar/</url>
      
        <content type="html"><![CDATA[<p>这是一篇从C++算法代码的角度思考，如何速成python语法的文章，笔者由于之前需要通过一项python的算法测试，故编写了这篇文章，现在希望可以帮助大家</p><h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 题目描述:# 第一行是一个整数 n，表示接下来有 n 行数据。# 接下来的 n 行，每行一个整数。n &#x3D; int(input())            # 先读取数据量 nfor i in range(n):          # 循环 n 次    data &#x3D; int(input())     # 每次读取一个整数    # 在这里处理 data...    print(data * 2)         # 示例：输出两倍# 读取一行中的两个整数 (用空格分隔)line &#x3D; input()              # 读取整行，例如 &quot;5 10&quot;parts &#x3D; line.split()        # 按空白字符分割，得到列表 [&#39;5&#39;, &#39;10&#39;]a &#x3D; int(parts[0])           # 转换第一个元素b &#x3D; int(parts[1])           # 转换第二个元素# 更简洁的写法 (常用！)a, b &#x3D; map(int, input().split())# 解释: #   input().split() -&gt; [&#39;5&#39;, &#39;10&#39;] (字符串列表)#   map(int, ...) -&gt; 将int函数应用到列表每个元素，得到一个map对象#   a, b &#x3D; ... -&gt; 序列解包，把map对象的前两个值分别赋给a和b# 读取一行k个整数k &#x3D; 3x, y, z &#x3D; map(int, input().split())  # 如果知道数量# 或者，如果数量不确定，存入列表numbers &#x3D; list(map(int, input().split())) # 得到 [5, 10, 3, 8] 这样的列表# 题目描述:# 第一行包含两个整数 n 和 m。# 接下来 n 行，每行 m 个整数，表示一个 n×m 的矩阵。# 读取第一行n, m &#x3D; map(int, input().split())# 初始化一个空列表来存储矩阵matrix &#x3D; []# 读取 n 行for i in range(n):    # 读取一行，并分割成 m 个整数，存入列表    row &#x3D; list(map(int, input().split()))    matrix.append(row)  # 将这一行添加到矩阵中# 现在 matrix 是一个二维列表，可以像 C++ 二维数组一样访问# 例如，matrix[0][0] 是左上角的元素# matrix[i][j] 是第 i+1 行第 j+1 列的元素# 示例：计算所有元素的和total &#x3D; 0for i in range(n):    for j in range(m):        total +&#x3D; matrix[i][j]print(total)print(1, 2, 3)           # 输出: 1 2 3 (默认空格分隔，换行结束)print(1, 2, 3, sep&#x3D;&#39;-&#39;)  # 输出: 1-2-3print(1, end&#x3D;&#39; &#39;)        # 输出: 1 (不换行，以空格结束)print(2)                 # 输出: 2 (换行)# 最终效果: 1 2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Trick"><a href="#Trick" class="headerlink" title="Trick"></a>Trick</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">mid&#x3D;(l+r)&#x2F;&#x2F;2 #python不声明类型，&#x2F;可能出现浮点数，故&#x2F;&#x2F;为整数除法TreeNode(nums[mid])#__init__为构造函数，直接调用即可# 1. 导入必要的模块（如果需要）import sysfrom collections import deque# 4. 主函数：处理输入、调用算法、输出结果def main():    # 读取输入（根据题目要求）    try:        line &#x3D; input().strip()        # 假设输入是一行整数，如: [-10,-3,0,5,9]        # 去掉括号，分割，转整数        nums &#x3D; list(map(int, line.strip(&#39;[]&#39;).split(&#39;,&#39;))) if line.strip(&#39;[]&#39;) else []    except:        print(&quot;Invalid input&quot;)        return    # 创建 Solution 实例并调用方法    sol &#x3D; Solution()    root &#x3D; sol.sortedArrayToBST(nums)    # 输出结果（例如：层序遍历输出）    result &#x3D; level_order(root)   print(result)import sysdata &#x3D; []for line in sys.stdin:    data.append(list(map(int, line.split())))t &#x3D; int(input())  # 测试用例数for _ in range(t):    n &#x3D; int(input())    nums &#x3D; list(map(int, input().split()))    # 处理每组数据self.res &#x3D; []  # 实例变量，在方法内初始化，可以在其他方法调用#你将 ans 列表的引用添加到了 res 中。由于 ans 是一个可变对象，并且在整个递归过程中被反复修改（append 和 pop），#最终 res 中的所有元素都会指向同一个 ans 对象，其值为空（因为最后都 pop() 了），或者状态混乱res.append(ans[:])  # 使用切片复制列表res.append(ans.copy())        left&#x3D;bisect.bisect_left(nums,target)        right&#x3D;bisect.bisect_right(nums,target)#类似lower_bound和upper_bound    indices &#x3D; np.random.permutation(X.shape[0])np.random.permutation(100)：生成一个从 0 到 99 的整数的随机排列（打乱顺序）one_hot[np.arange(x.size), x] &#x3D; 1num,counts&#x3D;np.unique(y,return_counts&#x3D;True)    # 生成索引,注意不是arrange    indices &#x3D; np.arange(n_samples)    # 是否打乱    if shuffle:        np.random.shuffle(indices)            sampled_indices &#x3D; np.random.choice(indices, size&#x3D;n_samples, replace&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>train_indices = np.concatenate(folds[:i] + folds[i+1:])  # 其余合并为训练集#类似extend</code></pre><ul><li><code>input()</code>：读取一行输入，返回字符串。</li><li><code>split()</code>：按空白字符（空格、制表符、换行）分割字符串。</li><li><code>map(func, iterable)</code>：对列表中的每个元素应用函数（如 <code>int</code>）。</li><li><code>list()</code>：将 <code>map</code> 对象转为列表</li><li><code>n = int(input())  假如第一行的数字代表数据组个数</code></li><li>```pyhton<br>folds = np.array_split(indices, 5)<br>folds[0] = array([0, 1])      # 第0折：2个样本<br>folds[1] = array([2, 3])      # 第1折：2个样本<br>folds[2] = array([4, 5])      # 第2折：2个样本<br>folds[3] = array([6, 7])      # 第3折：2个样本<br>folds[4] = array([8, 9])      # 第4折：2个样本<pre class="line-numbers language-none"><code class="language-none">## DICT&#96;&#96;&#96;pythond &#x3D; &#123;&#125;d[&#39;key1&#39;] &#x3D; 100           # 方式1：直接赋值（推荐）del d[&#39;key1&#39;]             # 删除键，不存在会报错 KeyErrord.pop(&#39;key2&#39;)             # 删除并返回值，不存在可设默认值 d.pop(&#39;key2&#39;, None)d[&#39;key1&#39;] &#x3D; 999           # 直接赋值修改d.update(&#123;&#39;key1&#39;: 888&#125;)   # 批量更新value &#x3D; d[&#39;key1&#39;]         # 获取值，key 不存在会报错value &#x3D; d.get(&#39;key1&#39;, -1) # 获取值，不存在返回默认值（推荐）&#39;key1&#39; in d               # 检查 key 是否存在，返回 True&#x2F;Falsed.keys()    # 所有键d.values()  # 所有值d.items()   # 所有键值对 (key, value)# 按 key 排序sorted_by_key &#x3D; dict(sorted(d.items(), key&#x3D;lambda x: x[0]))# 按 value 排序（从小到大）sorted_by_value &#x3D; dict(sorted(d.items(), key&#x3D;lambda x: x[1]))sorted_mp&#x3D;sorted(mp.items(),key&#x3D;lambda x:x[1],reverse&#x3D;True)#不转换为dict，因为dict不支持切片，直接用sorted可以转为list# 从大到小排序sorted_by_value_desc &#x3D; dict(sorted(d.items(), key&#x3D;lambda x: x[1], reverse&#x3D;True))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id="SET"><a href="#SET" class="headerlink" title="SET"></a>SET</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">s &#x3D; set()           # 正确：创建空集合s &#x3D; &#123;1, 2, 3&#125;s.add(4)                  # 添加单个元素s.update([5, 6])          # 添加多个元素（可迭代对象）s.remove(4)               # 删除元素，不存在会报错s.discard(4)              # 删除元素，不存在也不报错s.pop()                   # 随机删除并返回一个元素（无序！）s.clear()                 #清空3 in s                    # 检查元素是否存在，O(1)s.issubset(other_set)     # 子集判断s.issuperset(other_set)   # 超集判断len(s)                    # 元素个数sorted_list &#x3D; sorted(s)           # 升序sorted_list_desc &#x3D; sorted(s, reverse&#x3D;True)  # 降序nums.sort(key&#x3D;lambda x: (x[0], -x[1]))#第一个升序，第二个降序<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="table-container"><table><thead><tr><th>部分</th><th>说明</th></tr></thead><tbody><tr><td><code>nums.sort()</code></td><td>对列表 <code>nums</code> <strong>原地排序</strong></td></tr><tr><td><code>key=</code></td><td>指定一个函数，告诉 Python <strong>按什么规则排序</strong></td></tr><tr><td><code>lambda x: ...</code></td><td>匿名函数，输入是列表中的每个元素 <code>x</code>，输出是一个“排序键”</td></tr><tr><td><code>(x[0], -x[1])</code></td><td>返回一个元组，作为排序的依据</td></tr></tbody></table></div><h2 id="LIST"><a href="#LIST" class="headerlink" title="LIST"></a>LIST</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">dp &#x3D; [0] * (k + 1)#用list模拟数组stack &#x3D; []stack.append(1)stack.append(2)lst.remove(5)top &#x3D; stack.pop()         # 弹出最后一个元素，LIFOstack[-1] &#x3D; 99            # 修改栈顶元素top &#x3D; stack[-1]           # 查看栈顶（不删除）3 in stack                # 检查元素是否存在（O(n)）stack.sort()              # 原地排序（升序）stack.sort(reverse&#x3D;True)  # 原地降序sorted_stack &#x3D; sorted(stack)  # 返回新列表，不改变原栈lst &#x3D; [1, 2, 3, 2, 4]# 查找索引try:    index &#x3D; lst.index(2)    # 返回第一个 2 的索引：1    lst.pop(index)          # 删除该位置except ValueError:    print(&quot;元素不存在&quot;)lst &#x3D; [1, 2, 3, 4]# 删除最后一个lst.pop()        # 返回 4# 删除指定索引lst.pop(0)       # 删除第一个，返回 1# 或者用 del（不返回值）del lst[1]       # 删除索引 1 的元素nums[k:] &#x3D; nums[k:][::-1]#倒序（reverse）#使用nums[k:].reverse()会创建新数组，不改变原list，并且返回Nonesorted_nums &#x3D; sorted(nums, key&#x3D;lambda x: (x[0], -x[1]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="QUEUE"><a href="#QUEUE" class="headerlink" title="QUEUE"></a>QUEUE</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">from collections import dequeq &#x3D; deque()q.append(1)               # 从右边入队q.appendleft(0)           # 从左边入队（双端队列特性）front &#x3D; q.popleft()       # 从左边出队，FIFO，O(1)# q.pop()                 # 从右边出队，变成栈q[0] &#x3D; 99                 # 修改队首（支持索引访问）q[-1] &#x3D; 88                # 修改队尾orted_list &#x3D; sorted(q)   # 转为排序列表<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="table-container"><table><thead><tr><th>操作</th><th><code>dict</code></th><th><code>set</code></th><th><code>list</code> (栈)</th><th><code>deque</code> (队列)</th></tr></thead><tbody><tr><td><strong>增</strong></td><td><code>d[k]=v</code>, <code>d.update()</code></td><td><code>s.add()</code>, <code>s.update()</code></td><td><code>lst.append()</code></td><td><code>q.append()</code>, <code>q.appendleft()</code></td></tr><tr><td><strong>删</strong></td><td><code>del d[k]</code>, <code>d.pop(k)</code></td><td><code>s.remove()</code>, <code>s.discard()</code></td><td><code>lst.pop()</code></td><td><code>q.popleft()</code></td></tr><tr><td><strong>改</strong></td><td><code>d[k] = new_v</code></td><td>先删后增</td><td><code>lst[-1] = new_val</code></td><td><code>q[0] = new_val</code></td></tr><tr><td><strong>查</strong></td><td><code>k in d</code>, <code>d.get(k)</code></td><td><code>x in s</code></td><td><code>lst[-1]</code>, <code>x in lst</code></td><td><code>q[0]</code>, <code>x in q</code></td></tr><tr><td><strong>排序</strong></td><td><code>dict(sorted(d.items()))</code></td><td><code>sorted(s)</code></td><td><code>lst.sort()</code></td><td><code>sorted(q)</code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>方法</th><th>行为</th><th>示例</th></tr></thead><tbody><tr><td><code>append(iterable)</code></td><td>把整个对象当作一个元素添加</td><td><code>q.append([1,2])</code> → <code>deque([[1,2]])</code></td></tr><tr><td><code>extend(iterable)</code></td><td>把可迭代对象的<strong>每个元素</strong>逐个添加</td><td><code>q.extend([1,2])</code> → <code>deque([1, 2])</code></td></tr></tbody></table></div><h2 id="FOR"><a href="#FOR" class="headerlink" title="FOR"></a>FOR</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">for i, num in enumerate(nums):  # 更简洁地遍历索引和for i, char in enumerate(s):s &#x3D; &quot;abc&quot;for char in s:    print(char)  # a, b, c# 带索引遍历for i, char in enumerate(s):    print(f&quot;&#123;i&#125;: &#123;char&#125;&quot;)# 保留索引不在 &#96;indices_to_remove&#96; 中的元素result &#x3D; [value for index, value in enumerate(data) if index not in indices_to_remove]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2025/10/06/hello-world/"/>
      <url>/2025/10/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new &quot;My New Post&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
